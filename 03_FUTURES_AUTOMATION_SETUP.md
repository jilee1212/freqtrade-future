# ğŸ¤– 03_FUTURES_AUTOMATION_SETUP.md

**Binance USDT Perpetual Futures ì „ìš© Freqtrade AI ì‹œìŠ¤í…œì˜ ì™„ì „ ìë™í™” ì„¤ì • ì „ë¬¸ê°€ê¸‰ ê°€ì´ë“œ**

<div align="center">

[![Automation](https://img.shields.io/badge/Automation-100%25-green?style=for-the-badge&logo=docker)](https://docs.freqtrade.io/)
[![Kubernetes](https://img.shields.io/badge/Kubernetes-Ready-blue?style=for-the-badge&logo=kubernetes)](https://kubernetes.io/)
[![CI/CD](https://img.shields.io/badge/CI%2FCD-GitHub%20Actions-orange?style=for-the-badge&logo=githubactions)](https://github.com/features/actions)
[![Monitoring](https://img.shields.io/badge/Monitoring-24%2F7-red?style=for-the-badge&logo=grafana)](https://grafana.com/)

**ğŸ¯ ëª©í‘œ: 24ì‹œê°„ ë¬´ì¤‘ë‹¨ ë¬´ì¸ ìš´ì˜ ê°€ëŠ¥í•œ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ì™„ì „ ìë™í™” ì‹œìŠ¤í…œ êµ¬ì¶•**

</div>

---

## ğŸ“‹ **ëª©ì°¨**

1. [ğŸ—ï¸ ì™„ì „ ìë™í™” ì•„í‚¤í…ì²˜](#ï¸-ì™„ì „-ìë™í™”-ì•„í‚¤í…ì²˜) (40ë¶„ ì½ê¸°)
2. [ğŸ³ Docker ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜](#-docker-ì»¨í…Œì´ë„ˆ-ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜) (45ë¶„)
3. [ğŸ”„ CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì¶•](#-cicd-íŒŒì´í”„ë¼ì¸-êµ¬ì¶•) (50ë¶„)
4. [ğŸ¯ ë©€í‹° ì „ëµ ê´€ë¦¬ ì‹œìŠ¤í…œ](#-ë©€í‹°-ì „ëµ-ê´€ë¦¬-ì‹œìŠ¤í…œ) (55ë¶„)
5. [ğŸ“Š ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸](#-ì‹¤ì‹œê°„-ë°ì´í„°-íŒŒì´í”„ë¼ì¸) (60ë¶„)
6. [ğŸ“¢ í†µí•© ì•Œë¦¼ ì‹œìŠ¤í…œ](#-í†µí•©-ì•Œë¦¼-ì‹œìŠ¤í…œ) (65ë¶„)
7. [ğŸ”„ ìë™ ì¥ì•  ë³µêµ¬ ì‹œìŠ¤í…œ](#-ìë™-ì¥ì• -ë³µêµ¬-ì‹œìŠ¤í…œ) (70ë¶„)
8. [âš¡ ì„±ê³¼ ìµœì í™” ìë™í™”](#-ì„±ê³¼-ìµœì í™”-ìë™í™”) (75ë¶„)
9. [ğŸ” ë³´ì•ˆ ë° ì»´í”Œë¼ì´ì–¸ìŠ¤](#-ë³´ì•ˆ-ë°-ì»´í”Œë¼ì´ì–¸ìŠ¤) (80ë¶„)
10. [ğŸš€ ìš´ì˜ ë° ìœ ì§€ë³´ìˆ˜ ìë™í™”](#-ìš´ì˜-ë°-ìœ ì§€ë³´ìˆ˜-ìë™í™”) (85ë¶„)

---

## ğŸ—ï¸ **ì™„ì „ ìë™í™” ì•„í‚¤í…ì²˜**

### ğŸ“ **ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ê¸°ë°˜ ì„¤ê³„**

```mermaid
graph TB
    subgraph "Load Balancer Layer"
        LB[HAProxy/Nginx]
    end
    
    subgraph "API Gateway"
        AG[Kong/Ambassador]
    end
    
    subgraph "Core Services"
        FT1[Freqtrade Instance 1]
        FT2[Freqtrade Instance 2]
        FT3[Freqtrade Instance N]
    end
    
    subgraph "Data Layer"
        TS[TimescaleDB]
        RD[Redis Cluster]
        ES[Elasticsearch]
    end
    
    subgraph "Message Queue"
        RMQ[RabbitMQ Cluster]
        KF[Apache Kafka]
    end
    
    subgraph "Monitoring Stack"
        PR[Prometheus]
        GF[Grafana]
        JG[Jaeger]
        ELK[ELK Stack]
    end
    
    subgraph "External APIs"
        BA[Binance API]
        TG[Telegram API]
        SL[Slack API]
    end
    
    LB --> AG
    AG --> FT1
    AG --> FT2
    AG --> FT3
    
    FT1 --> TS
    FT2 --> TS
    FT3 --> TS
    
    FT1 --> RD
    FT2 --> RD
    FT3 --> RD
    
    FT1 --> RMQ
    FT2 --> RMQ
    FT3 --> RMQ
    
    RMQ --> KF
    
    FT1 --> BA
    FT2 --> BA
    FT3 --> BA
    
    style FT1 fill:#27ae60
    style FT2 fill:#27ae60
    style FT3 fill:#27ae60
    style TS fill:#3498db
    style RD fill:#e74c3c
    style BA fill:#f9d71c
```

### ğŸ¯ **ì´ë²¤íŠ¸ ë“œë¦¬ë¸ ì•„í‚¤í…ì²˜**

```python
# automation/event_driven_core.py
import asyncio
import json
from typing import Dict, List, Callable, Any
from dataclasses import dataclass, asdict
from enum import Enum
import aioredis
import aiokafka

class EventType(Enum):
    """ì´ë²¤íŠ¸ íƒ€ì… ì •ì˜"""
    TRADE_SIGNAL = "trade_signal"
    POSITION_UPDATE = "position_update"
    RISK_ALERT = "risk_alert"
    SYSTEM_STATUS = "system_status"
    MARKET_DATA_UPDATE = "market_data_update"
    STRATEGY_PERFORMANCE = "strategy_performance"
    FUNDING_RATE_UPDATE = "funding_rate_update"
    LIQUIDATION_WARNING = "liquidation_warning"

@dataclass
class Event:
    """ì´ë²¤íŠ¸ ë°ì´í„° í´ë˜ìŠ¤"""
    event_id: str
    event_type: EventType
    source: str
    timestamp: float
    data: Dict[str, Any]
    metadata: Dict[str, Any] = None

class EventBus:
    """ì¤‘ì•™ ì´ë²¤íŠ¸ ë²„ìŠ¤"""
    
    def __init__(self, redis_url: str, kafka_bootstrap_servers: str):
        self.redis_url = redis_url
        self.kafka_servers = kafka_bootstrap_servers
        self.subscribers: Dict[EventType, List[Callable]] = {}
        self.redis_pool = None
        self.kafka_producer = None
        self.kafka_consumer = None
        
    async def initialize(self):
        """ì´ë²¤íŠ¸ ë²„ìŠ¤ ì´ˆê¸°í™”"""
        # Redis ì—°ê²°
        self.redis_pool = aioredis.ConnectionPool.from_url(self.redis_url)
        
        # Kafka Producer ì´ˆê¸°í™”
        self.kafka_producer = aiokafka.AIOKafkaProducer(
            bootstrap_servers=self.kafka_servers,
            value_serializer=lambda x: json.dumps(asdict(x) if isinstance(x, Event) else x).encode()
        )
        await self.kafka_producer.start()
        
        # Kafka Consumer ì´ˆê¸°í™”
        self.kafka_consumer = aiokafka.AIOKafkaConsumer(
            'freqtrade-events',
            bootstrap_servers=self.kafka_servers,
            value_deserializer=lambda m: json.loads(m.decode())
        )
        await self.kafka_consumer.start()
    
    async def publish(self, event: Event):
        """ì´ë²¤íŠ¸ ë°œí–‰"""
        # Kafkaë¡œ ì´ë²¤íŠ¸ ì „ì†¡
        await self.kafka_producer.send('freqtrade-events', event)
        
        # Redisë¡œ ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ì „ì†¡ (ì¦‰ì‹œ ì²˜ë¦¬ìš©)
        redis = aioredis.Redis(connection_pool=self.redis_pool)
        await redis.publish('realtime-events', json.dumps(asdict(event)))
    
    def subscribe(self, event_type: EventType, handler: Callable):
        """ì´ë²¤íŠ¸ êµ¬ë…"""
        if event_type not in self.subscribers:
            self.subscribers[event_type] = []
        self.subscribers[event_type].append(handler)
    
    async def process_events(self):
        """ì´ë²¤íŠ¸ ì²˜ë¦¬ ë£¨í”„"""
        async for msg in self.kafka_consumer:
            event_data = msg.value
            event = Event(**event_data)
            
            # í•´ë‹¹ ì´ë²¤íŠ¸ íƒ€ì… êµ¬ë…ìë“¤ì—ê²Œ ì „ë‹¬
            if event.event_type in self.subscribers:
                for handler in self.subscribers[event.event_type]:
                    try:
                    # ë¡œì»¬ ì´ë¯¸ì§€ ì •ë³´
                    local_result = subprocess.run(
                        ['docker', 'image', 'inspect', image],
                        capture_output=True, text=True
                    )
                    
                    if local_result.returncode == 0:
                        local_info = json.loads(local_result.stdout)[0]
                        local_digest = local_info.get('RepoDigests', [''])[0]
                        
                        # ì›ê²© ì´ë¯¸ì§€ ì •ë³´
                        remote_result = subprocess.run(
                            ['docker', 'manifest', 'inspect', image],
                            capture_output=True, text=True
                        )
                        
                        if remote_result.returncode == 0:
                            remote_info = json.loads(remote_result.stdout)
                            remote_digest = remote_info.get('config', {}).get('digest', '')
                            
                            # ë‹¤ì´ì œìŠ¤íŠ¸ ë¹„êµ
                            if local_digest != remote_digest:
                                update = UpdatePackage(
                                    name=image,
                                    current_version=local_digest[:12] if local_digest else "unknown",
                                    new_version=remote_digest[:12] if remote_digest else "latest",
                                    update_type=UpdateType.DOCKER,
                                    auto_install=True,
                                    requires_restart=True
                                )
                                updates.append(update)
                                
                except Exception as e:
                    self.logger.warning(f"Failed to check updates for image {image}: {e}")
                    continue
            
            return updates
            
        except Exception as e:
            self.logger.error(f"Docker update check failed: {e}")
            return []
    
    async def _check_freqtrade_updates(self) -> List[UpdatePackage]:
        """Freqtrade ì—…ë°ì´íŠ¸ í™•ì¸"""
        try:
            # GitHub APIë¥¼ í†µí•œ ìµœì‹  ë¦´ë¦¬ì¦ˆ í™•ì¸
            import aiohttp
            
            async with aiohttp.ClientSession() as session:
                async with session.get('https://api.github.com/repos/freqtrade/freqtrade/releases/latest') as resp:
                    if resp.status == 200:
                        release_info = await resp.json()
                        latest_version = release_info['tag_name']
                        
                        # í˜„ì¬ ë²„ì „ í™•ì¸
                        result = subprocess.run(
                            ['freqtrade', '--version'],
                            capture_output=True, text=True
                        )
                        
                        if result.returncode == 0:
                            current_version = result.stdout.strip().split()[-1]
                            
                            if current_version != latest_version:
                                update = UpdatePackage(
                                    name="freqtrade",
                                    current_version=current_version,
                                    new_version=latest_version,
                                    update_type=UpdateType.APPLICATION,
                                    auto_install=False,  # ìˆ˜ë™ ìŠ¹ì¸ í•„ìš”
                                    requires_restart=True
                                )
                                return [update]
            
            return []
            
        except Exception as e:
            self.logger.error(f"Freqtrade update check failed: {e}")
            return []
    
    async def _check_strategy_updates(self) -> List[UpdatePackage]:
        """ì „ëµ ì—…ë°ì´íŠ¸ í™•ì¸"""
        # Git ì €ì¥ì†Œì—ì„œ ì „ëµ ì—…ë°ì´íŠ¸ í™•ì¸
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì „ëµ ì €ì¥ì†Œì˜ ë³€ê²½ì‚¬í•­ í™•ì¸
        return []
    
    def _is_critical_package(self, package_name: str) -> bool:
        """ì¤‘ìš” íŒ¨í‚¤ì§€ ì—¬ë¶€ í™•ì¸"""
        critical_packages = [
            'linux-image', 'kernel', 'systemd', 'openssh-server',
            'docker', 'postgresql', 'nginx'
        ]
        
        return any(critical in package_name for critical in critical_packages)
    
    async def schedule_maintenance_window(self, start_time: datetime, duration_hours: int, 
                                        update_types: List[UpdateType]):
        """ìœ ì§€ë³´ìˆ˜ ìœˆë„ìš° ìŠ¤ì¼€ì¤„ë§"""
        
        window = {
            'start_time': start_time,
            'end_time': start_time + timedelta(hours=duration_hours),
            'update_types': update_types,
            'status': 'scheduled'
        }
        
        self.maintenance_windows.append(window)
        
        # ìŠ¤ì¼€ì¤„ëœ ì—…ë°ì´íŠ¸ íƒœìŠ¤í¬ ìƒì„±
        delay = (start_time - datetime.now()).total_seconds()
        if delay > 0:
            asyncio.create_task(self._scheduled_maintenance(window, delay))
        
        self.logger.info(f"Maintenance window scheduled: {start_time} - {window['end_time']}")
    
    async def _scheduled_maintenance(self, window: Dict, delay: float):
        """ìŠ¤ì¼€ì¤„ëœ ìœ ì§€ë³´ìˆ˜ ì‹¤í–‰"""
        await asyncio.sleep(delay)
        
        try:
            window['status'] = 'in_progress'
            
            # ìœ ì§€ë³´ìˆ˜ ì‹œì‘ ì•Œë¦¼
            await self._send_maintenance_notification("started", window)
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ì—…ë°ì´íŠ¸ í™•ì¸
            available_updates = await self.check_available_updates()
            
            # ìœ ì§€ë³´ìˆ˜ ìœˆë„ìš°ì— í¬í•¨ëœ ì—…ë°ì´íŠ¸ íƒ€ì…ë§Œ ì²˜ë¦¬
            updates_to_install = []
            for update_type in window['update_types']:
                updates_to_install.extend(available_updates.get(update_type, []))
            
            # ë°±ì—… ìƒì„±
            backup_success = await self.backup_manager.execute_backup("pre_update_backup")
            if not backup_success:
                self.logger.error("Pre-update backup failed")
                window['status'] = 'failed'
                return
            
            # ì—…ë°ì´íŠ¸ ì„¤ì¹˜
            success_count = 0
            total_count = len(updates_to_install)
            
            for update in updates_to_install:
                success = await self._install_update(update)
                if success:
                    success_count += 1
            
            # ê²°ê³¼ ì²˜ë¦¬
            if success_count == total_count:
                window['status'] = 'completed'
                await self._send_maintenance_notification("completed", window, {
                    'updates_installed': success_count,
                    'total_updates': total_count
                })
            else:
                window['status'] = 'partial'
                await self._send_maintenance_notification("partial", window, {
                    'updates_installed': success_count,
                    'total_updates': total_count
                })
            
        except Exception as e:
            window['status'] = 'failed'
            self.logger.error(f"Scheduled maintenance failed: {e}")
            await self._send_maintenance_notification("failed", window, {'error': str(e)})
    
    async def _install_update(self, update: UpdatePackage) -> bool:
        """ì—…ë°ì´íŠ¸ ì„¤ì¹˜"""
        
        try:
            self.logger.info(f"Installing update: {update.name} {update.current_version} -> {update.new_version}")
            
            if update.update_type == UpdateType.SYSTEM:
                return await self._install_system_update(update)
            elif update.update_type == UpdateType.DOCKER:
                return await self._install_docker_update(update)
            elif update.update_type == UpdateType.APPLICATION:
                return await self._install_application_update(update)
            elif update.update_type == UpdateType.SECURITY:
                return await self._install_security_update(update)
            
            return False
            
        except Exception as e:
            self.logger.error(f"Update installation failed for {update.name}: {e}")
            return False
    
    async def _install_system_update(self, update: UpdatePackage) -> bool:
        """ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ì„¤ì¹˜"""
        try:
            cmd = ['apt', 'install', '-y', f"{update.name}={update.new_version}"]
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            return result.returncode == 0
            
        except Exception as e:
            self.logger.error(f"System update failed: {e}")
            return False
    
    async def _install_docker_update(self, update: UpdatePackage) -> bool:
        """Docker ì´ë¯¸ì§€ ì—…ë°ì´íŠ¸"""
        try:
            # ìƒˆ ì´ë¯¸ì§€ í’€
            pull_result = subprocess.run(
                ['docker', 'pull', update.name],
                capture_output=True, text=True
            )
            
            if pull_result.returncode != 0:
                return False
            
            # ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘ (Docker Compose ì‚¬ìš©)
            restart_result = subprocess.run(
                ['docker', 'compose', 'up', '-d', '--force-recreate'],
                cwd='/opt/freqtrade-futures',
                capture_output=True, text=True
            )
            
            return restart_result.returncode == 0
            
        except Exception as e:
            self.logger.error(f"Docker update failed: {e}")
            return False
    
    async def _install_application_update(self, update: UpdatePackage) -> bool:
        """ì• í”Œë¦¬ì¼€ì´ì…˜ ì—…ë°ì´íŠ¸"""
        if update.name == "freqtrade":
            try:
                # Docker ì´ë¯¸ì§€ ì—…ë°ì´íŠ¸ë¡œ ì²˜ë¦¬
                docker_update = UpdatePackage(
                    name="freqtradeorg/freqtrade:stable",
                    current_version=update.current_version,
                    new_version=update.new_version,
                    update_type=UpdateType.DOCKER,
                    requires_restart=True
                )
                
                return await self._install_docker_update(docker_update)
                
            except Exception as e:
                self.logger.error(f"Freqtrade update failed: {e}")
                return False
        
        return False
    
    async def _install_security_update(self, update: UpdatePackage) -> bool:
        """ë³´ì•ˆ ì—…ë°ì´íŠ¸ ì„¤ì¹˜"""
        try:
            # unattended-upgrades ì‚¬ìš©
            result = subprocess.run(
                ['unattended-upgrade'],
                capture_output=True, text=True
            )
            
            return result.returncode == 0
            
        except Exception as e:
            self.logger.error(f"Security update failed: {e}")
            return False
    
    async def _send_maintenance_notification(self, status: str, window: Dict, details: Dict = None):
        """ìœ ì§€ë³´ìˆ˜ ì•Œë¦¼ ì „ì†¡"""
        
        if status == "started":
            title = "ğŸ”§ ìœ ì§€ë³´ìˆ˜ ì‹œì‘"
            message = f"ì‹œì‘ ì‹œê°„: {window['start_time'].strftime('%Y-%m-%d %H:%M:%S')}\n"
            message += f"ì˜ˆìƒ ì¢…ë£Œ: {window['end_time'].strftime('%Y-%m-%d %H:%M:%S')}\n"
            message += f"ì—…ë°ì´íŠ¸ íƒ€ì…: {', '.join([t.value for t in window['update_types']])}"
            priority = NotificationPriority.MEDIUM
            
        elif status == "completed":
            title = "âœ… ìœ ì§€ë³´ìˆ˜ ì™„ë£Œ"
            message = f"ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            if details:
                message += f"ì„¤ì¹˜ëœ ì—…ë°ì´íŠ¸: {details['updates_installed']}/{details['total_updates']}"
            priority = NotificationPriority.LOW
            
        elif status == "partial":
            title = "âš ï¸ ìœ ì§€ë³´ìˆ˜ ë¶€ë¶„ ì™„ë£Œ"
            message = f"ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            if details:
                message += f"ì„¤ì¹˜ëœ ì—…ë°ì´íŠ¸: {details['updates_installed']}/{details['total_updates']}\n"
                message += "ì¼ë¶€ ì—…ë°ì´íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            priority = NotificationPriority.MEDIUM
            
        elif status == "failed":
            title = "âŒ ìœ ì§€ë³´ìˆ˜ ì‹¤íŒ¨"
            message = f"ì‹¤íŒ¨ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            if details and 'error' in details:
                message += f"ì˜¤ë¥˜: {details['error']}"
            priority = NotificationPriority.HIGH
        
        notification = Notification(
            title=title,
            message=message,
            priority=priority,
            tags=['maintenance', status]
        )
        
        await self.notification_system.route_notification(notification)

### ğŸ“Š **ìš©ëŸ‰ ê³„íš ë° ìŠ¤ì¼€ì¼ë§**

```python
# automation/capacity_planning.py
import asyncio
import psutil
import numpy as np
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
import logging
from dataclasses import dataclass
from enum import Enum
import json

class ResourceType(Enum):
    """ë¦¬ì†ŒìŠ¤ íƒ€ì…"""
    CPU = "cpu"
    MEMORY = "memory"
    DISK = "disk"
    NETWORK = "network"
    DATABASE_CONNECTIONS = "db_connections"

class ScalingDirection(Enum):
    """ìŠ¤ì¼€ì¼ë§ ë°©í–¥"""
    UP = "up"
    DOWN = "down"
    MAINTAIN = "maintain"

@dataclass
class ResourceMetric:
    """ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­"""
    timestamp: datetime
    resource_type: ResourceType
    value: float
    unit: str
    threshold_warning: float = 80.0
    threshold_critical: float = 90.0

@dataclass
class ScalingRecommendation:
    """ìŠ¤ì¼€ì¼ë§ ê¶Œì¥ì‚¬í•­"""
    resource_type: ResourceType
    direction: ScalingDirection
    current_value: float
    recommended_value: float
    confidence: float
    reasoning: str
    estimated_cost_impact: float

class CapacityPlanningManager:
    """ìš©ëŸ‰ ê³„íš ê´€ë¦¬ì"""
    
    def __init__(self, notification_system):
        self.notification_system = notification_system
        self.metrics_history: Dict[ResourceType, List[ResourceMetric]] = {
            resource_type: [] for resource_type in ResourceType
        }
        self.scaling_policies = {}
        self.prediction_models = {}
        
        # ë©”íŠ¸ë¦­ ë³´ì¡´ ê¸°ê°„ (ì¼)
        self.retention_days = 90
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì‹œì‘
        asyncio.create_task(self._monitoring_loop())
    
    async def _monitoring_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while True:
            try:
                # ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                await self._collect_metrics()
                
                # ìš©ëŸ‰ ë¶„ì„
                await self._analyze_capacity()
                
                # ì˜ˆì¸¡ ëª¨ë¸ ì—…ë°ì´íŠ¸
                await self._update_prediction_models()
                
                # ìŠ¤ì¼€ì¼ë§ ê¶Œì¥ì‚¬í•­ ìƒì„±
                recommendations = await self._generate_scaling_recommendations()
                
                if recommendations:
                    await self._send_capacity_alerts(recommendations)
                
                # 5ë¶„ë§ˆë‹¤ ì‹¤í–‰
                await asyncio.sleep(300)
                
            except Exception as e:
                self.logger.error(f"Capacity monitoring error: {e}")
                await asyncio.sleep(60)
    
    async def _collect_metrics(self):
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        timestamp = datetime.now()
        
        # CPU ì‚¬ìš©ë¥ 
        cpu_percent = psutil.cpu_percent(interval=1)
        cpu_metric = ResourceMetric(
            timestamp=timestamp,
            resource_type=ResourceType.CPU,
            value=cpu_percent,
            unit="percent"
        )
        self.metrics_history[ResourceType.CPU].append(cpu_metric)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
        memory = psutil.virtual_memory()
        memory_metric = ResourceMetric(
            timestamp=timestamp,
            resource_type=ResourceType.MEMORY,
            value=memory.percent,
            unit="percent"
        )
        self.metrics_history[ResourceType.MEMORY].append(memory_metric)
        
        # ë””ìŠ¤í¬ ì‚¬ìš©ë¥ 
        disk = psutil.disk_usage('/')
        disk_percent = (disk.used / disk.total) * 100
        disk_metric = ResourceMetric(
            timestamp=timestamp,
            resource_type=ResourceType.DISK,
            value=disk_percent,
            unit="percent"
        )
        self.metrics_history[ResourceType.DISK].append(disk_metric)
        
        # ë„¤íŠ¸ì›Œí¬ I/O
        network = psutil.net_io_counters()
        network_mbps = (network.bytes_sent + network.bytes_recv) / (1024 * 1024)  # MB/s
        network_metric = ResourceMetric(
            timestamp=timestamp,
            resource_type=ResourceType.NETWORK,
            value=network_mbps,
            unit="mbps"
        )
        self.metrics_history[ResourceType.NETWORK].append(network_metric)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìˆ˜ (PostgreSQL)
        try:
            db_connections = await self._get_database_connections()
            db_metric = ResourceMetric(
                timestamp=timestamp,
                resource_type=ResourceType.DATABASE_CONNECTIONS,
                value=db_connections,
                unit="connections"
            )
            self.metrics_history[ResourceType.DATABASE_CONNECTIONS].append(db_metric)
        except Exception as e:
            self.logger.warning(f"Failed to collect DB metrics: {e}")
        
        # ì˜¤ë˜ëœ ë©”íŠ¸ë¦­ ì •ë¦¬
        await self._cleanup_old_metrics()
    
    async def _get_database_connections(self) -> float:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìˆ˜ ì¡°íšŒ"""
        try:
            import asyncpg
            
            conn = await asyncpg.connect(
                host='localhost',
                port=5432,
                user='freqtrade',
                password=os.getenv('DB_PASSWORD'),
                database='freqtrade'
            )
            
            result = await conn.fetchval(
                "SELECT count(*) FROM pg_stat_activity WHERE state = 'active'"
            )
            
            await conn.close()
            return float(result)
            
        except Exception as e:
            self.logger.error(f"Database connection check failed: {e}")
            return 0.0
    
    async def _cleanup_old_metrics(self):
        """ì˜¤ë˜ëœ ë©”íŠ¸ë¦­ ì •ë¦¬"""
        cutoff_time = datetime.now() - timedelta(days=self.retention_days)
        
        for resource_type in ResourceType:
            metrics = self.metrics_history[resource_type]
            self.metrics_history[resource_type] = [
                m for m in metrics if m.timestamp > cutoff_time
            ]
    
    async def _analyze_capacity(self):
        """ìš©ëŸ‰ ë¶„ì„"""
        for resource_type in ResourceType:
            metrics = self.metrics_history[resource_type]
            
            if len(metrics) < 10:  # ìµœì†Œ ë°ì´í„° í¬ì¸íŠ¸ í•„ìš”
                continue
            
            # ìµœê·¼ 1ì‹œê°„ í‰ê· 
            recent_metrics = [m for m in metrics 
                            if m.timestamp > datetime.now() - timedelta(hours=1)]
            
            if recent_metrics:
                avg_usage = np.mean([m.value for m in recent_metrics])
                max_usage = np.max([m.value for m in recent_metrics])
                
                # ì„ê³„ê°’ í™•ì¸
                if max_usage > recent_metrics[0].threshold_critical:
                    await self._send_critical_alert(resource_type, max_usage)
                elif avg_usage > recent_metrics[0].threshold_warning:
                    await self._send_warning_alert(resource_type, avg_usage)
    
    async def _update_prediction_models(self):
        """ì˜ˆì¸¡ ëª¨ë¸ ì—…ë°ì´íŠ¸"""
        for resource_type in ResourceType:
            metrics = self.metrics_history[resource_type]
            
            if len(metrics) < 100:  # ìµœì†Œ ë°ì´í„° í¬ì¸íŠ¸ í•„ìš”
                continue
            
            try:
                # ì‹œê³„ì—´ ë°ì´í„° ì¤€ë¹„
                timestamps = np.array([m.timestamp.timestamp() for m in metrics])
                values = np.array([m.value for m in metrics])
                
                # ì„ í˜• íšŒê·€ë¥¼ í†µí•œ íŠ¸ë Œë“œ ë¶„ì„
                coefficients = np.polyfit(timestamps, values, 1)
                
                # 24ì‹œê°„ í›„ ì˜ˆì¸¡ê°’
                future_timestamp = datetime.now().timestamp() + (24 * 3600)
                predicted_value = np.polyval(coefficients, future_timestamp)
                
                self.prediction_models[resource_type] = {
                    'coefficients': coefficients.tolist(),
                    'predicted_24h': predicted_value,
                    'trend': 'increasing' if coefficients[0] > 0 else 'decreasing',
                    'updated_at': datetime.now()
                }
                
            except Exception as e:
                self.logger.error(f"Prediction model update failed for {resource_type.value}: {e}")
    
    async def _generate_scaling_recommendations(self) -> List[ScalingRecommendation]:
        """ìŠ¤ì¼€ì¼ë§ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        for resource_type, model in self.prediction_models.items():
            try:
                predicted_value = model['predicted_24h']
                trend = model['trend']
                
                # í˜„ì¬ ê°’
                recent_metrics = self.metrics_history[resource_type][-10:]
                if not recent_metrics:
                    continue
                
                current_avg = np.mean([m.value for m in recent_metrics])
                
                # ìŠ¤ì¼€ì¼ë§ ê²°ì • ë¡œì§
                recommendation = await self._determine_scaling_action(
                    resource_type, current_avg, predicted_value, trend
                )
                
                if recommendation:
                    recommendations.append(recommendation)
                    
            except Exception as e:
                self.logger.error(f"Scaling recommendation generation failed for {resource_type.value}: {e}")
        
        return recommendations
    
    async def _determine_scaling_action(self, resource_type: ResourceType, 
                                      current_value: float, predicted_value: float, 
                                      trend: str) -> Optional[ScalingRecommendation]:
        """ìŠ¤ì¼€ì¼ë§ ì•¡ì…˜ ê²°ì •"""
        
        # ë¦¬ì†ŒìŠ¤ë³„ ì„ê³„ê°’
        thresholds = {
            ResourceType.CPU: {'warning': 70, 'critical': 85},
            ResourceType.MEMORY: {'warning': 75, 'critical': 90},
            ResourceType.DISK: {'warning': 80, 'critical': 95},
            ResourceType.NETWORK: {'warning': 100, 'critical': 500},  # Mbps
            ResourceType.DATABASE_CONNECTIONS: {'warning': 80, 'critical': 95}
        }
        
        threshold = thresholds.get(resource_type, {'warning': 80, 'critical': 90})
        
        # ìŠ¤ì¼€ì¼ ì—… ê¶Œì¥
        if predicted_value > threshold['critical'] or current_value > threshold['warning']:
            if resource_type in [ResourceType.CPU, ResourceType.MEMORY]:
                return ScalingRecommendation(
                    resource_type=resource_type,
                    direction=ScalingDirection.UP,
                    current_value=current_value,
                    recommended_value=current_value * 1.5,  # 50% ì¦ê°€
                    confidence=0.8,
                    reasoning=f"Current: {current_value:.1f}%, Predicted 24h: {predicted_value:.1f}%",
                    estimated_cost_impact=50.0  # USD
                )
            elif resource_type == ResourceType.DISK:
                return ScalingRecommendation(
                    resource_type=resource_type,
                    direction=ScalingDirection.UP,
                    current_value=current_value,
                    recommended_value=100.0,  # ì¶”ê°€ ìŠ¤í† ë¦¬ì§€
                    confidence=0.9,
                    reasoning=f"Disk usage critical: {current_value:.1f}%",
                    estimated_cost_impact=20.0  # USD
                )
        
        # ìŠ¤ì¼€ì¼ ë‹¤ìš´ ê¶Œì¥ (ë¦¬ì†ŒìŠ¤ ì ˆì•½)
        elif current_value < 30 and predicted_value < 40 and trend == 'decreasing':
            if resource_type in [ResourceType.CPU, ResourceType.MEMORY]:
                return ScalingRecommendation(
                    resource_type=resource_type,
                    direction=ScalingDirection.DOWN,
                    current_value=current_value,
                    recommended_value=current_value * 0.8,  # 20% ê°ì†Œ
                    confidence=0.6,
                    reasoning=f"Low usage: {current_value:.1f}%, trend: {trend}",
                    estimated_cost_impact=-25.0  # ë¹„ìš© ì ˆì•½
                )
        
        return None
    
    async def _send_capacity_alerts(self, recommendations: List[ScalingRecommendation]):
        """ìš©ëŸ‰ ì•Œë¦¼ ì „ì†¡"""
        
        if not recommendations:
            return
        
        title = "ğŸ“Š ìš©ëŸ‰ ê³„íš ê¶Œì¥ì‚¬í•­"
        message = "ë‹¤ìŒ ë¦¬ì†ŒìŠ¤ì— ëŒ€í•œ ìŠ¤ì¼€ì¼ë§ì„ ê²€í† í•´ì£¼ì„¸ìš”:\n\n"
        
        for rec in recommendations:
            direction_emoji = "â¬†ï¸" if rec.direction == ScalingDirection.UP else "â¬‡ï¸"
            cost_text = f"+${rec.estimated_cost_impact:.0f}" if rec.estimated_cost_impact > 0 else f"${rec.estimated_cost_impact:.0f}"
            
            message += f"{direction_emoji} {rec.resource_type.value.upper()}\n"
            message += f"   í˜„ì¬: {rec.current_value:.1f}\n"
            message += f"   ê¶Œì¥: {rec.recommended_value:.1f}\n"
            message += f"   ì´ìœ : {rec.reasoning}\n"
            message += f"   ë¹„ìš© ì˜í–¥: {cost_text}/ì›”\n"
            message += f"   ì‹ ë¢°ë„: {rec.confidence*100:.0f}%\n\n"
        
        notification = Notification(
            title=title,
            message=message,
            priority=NotificationPriority.MEDIUM,
            tags=['capacity', 'scaling']
        )
        
        await self.notification_system.route_notification(notification)
    
    async def _send_critical_alert(self, resource_type: ResourceType, value: float):
        """ì„ê³„ ì•Œë¦¼ ì „ì†¡"""
        title = f"ğŸš¨ ë¦¬ì†ŒìŠ¤ ì„ê³„ ìƒíƒœ: {resource_type.value.upper()}"
        message = f"í˜„ì¬ ì‚¬ìš©ë¥ : {value:.1f}%\nì¦‰ì‹œ ì¡°ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        
        notification = Notification(
            title=title,
            message=message,
            priority=NotificationPriority.CRITICAL,
            tags=['resource', 'critical', resource_type.value]
        )
        
        await self.notification_system.route_notification(notification)
    
    async def _send_warning_alert(self, resource_type: ResourceType, value: float):
        """ê²½ê³  ì•Œë¦¼ ì „ì†¡"""
        title = f"âš ï¸ ë¦¬ì†ŒìŠ¤ ê²½ê³ : {resource_type.value.upper()}"
        message = f"í‰ê·  ì‚¬ìš©ë¥ : {value:.1f}%\nëª¨ë‹ˆí„°ë§ì„ ê°•í™”í•´ì£¼ì„¸ìš”."
        
        notification = Notification(
            title=title,
            message=message,
            priority=NotificationPriority.MEDIUM,
            tags=['resource', 'warning', resource_type.value]
        )
        
        await self.notification_system.route_notification(notification)
    
    async def get_capacity_report(self, days: int = 7) -> Dict[str, Any]:
        """ìš©ëŸ‰ ë³´ê³ ì„œ ìƒì„±"""
        
        start_time = datetime.now() - timedelta(days=days)
        report = {
            'period': f'Last {days} days',
            'start_time': start_time.isoformat(),
            'end_time': datetime.now().isoformat(),
            'resources': {},
            'predictions': {},
            'recommendations': []
        }
        
        for resource_type in ResourceType:
            metrics = [m for m in self.metrics_history[resource_type] 
                      if m.timestamp > start_time]
            
            if metrics:
                values = [m.value for m in metrics]
                report['resources'][resource_type.value] = {
                    'average': np.mean(values),
                    'maximum': np.max(values),
                    'minimum': np.min(values),
                    'current': values[-1] if values else 0,
                    'data_points': len(values)
                }
            
            # ì˜ˆì¸¡ ì •ë³´ ì¶”ê°€
            if resource_type in self.prediction_models:
                model = self.prediction_models[resource_type]
                report['predictions'][resource_type.value] = {
                    'predicted_24h': model['predicted_24h'],
                    'trend': model['trend'],
                    'updated_at': model['updated_at'].isoformat()
                }
        
        # ê¶Œì¥ì‚¬í•­ ì¶”ê°€
        recommendations = await self._generate_scaling_recommendations()
        report['recommendations'] = [
            {
                'resource': rec.resource_type.value,
                'direction': rec.direction.value,
                'current': rec.current_value,
                'recommended': rec.recommended_value,
                'confidence': rec.confidence,
                'reasoning': rec.reasoning,
                'cost_impact': rec.estimated_cost_impact
            }
            for rec in recommendations
        ]
        
        return report

### ğŸ¯ **ì„±ëŠ¥ íŠœë‹ ìë™í™”**

```python
# automation/performance_tuning.py
import asyncio
import psutil
import time
import subprocess
import os
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
import logging
from dataclasses import dataclass
from enum import Enum
import json

class TuningCategory(Enum):
    """íŠœë‹ ì¹´í…Œê³ ë¦¬"""
    SYSTEM = "system"
    DATABASE = "database"
    APPLICATION = "application"
    NETWORK = "network"
    STORAGE = "storage"

class TuningAction(Enum):
    """íŠœë‹ ì•¡ì…˜"""
    PARAMETER_CHANGE = "parameter_change"
    CONFIGURATION_UPDATE = "configuration_update"
    RESOURCE_REALLOCATION = "resource_reallocation"
    CACHING_OPTIMIZATION = "caching_optimization"

@dataclass
class PerformanceMetric:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    name: str
    value: float
    unit: str
    target_value: Optional[float] = None
    improvement_potential: float = 0.0

@dataclass
class TuningRecommendation:
    """íŠœë‹ ê¶Œì¥ì‚¬í•­"""
    category: TuningCategory
    action: TuningAction
    description: str
    current_config: Dict[str, Any]
    recommended_config: Dict[str, Any]
    expected_improvement: float
    risk_level: str  # low, medium, high
    implementation_effort: str  # low, medium, high

class PerformanceTuningManager:
    """ì„±ëŠ¥ íŠœë‹ ê´€ë¦¬ì"""
    
    def __init__(self, notification_system):
        self.notification_system = notification_system
        self.performance_baselines = {}
        self.tuning_history = []
        self.active_experiments = {}
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    async def analyze_performance(self) -> Dict[str, List[PerformanceMetric]]:
        """ì„±ëŠ¥ ë¶„ì„"""
        
        analysis_results = {
            'system': await self._analyze_system_performance(),
            'database': await self._analyze_database_performance(),
            'application': await self._analyze_application_performance(),
            'network': await self._analyze_network_performance(),
            'storage': await self._analyze_storage_performance()
        }
        
        return analysis_results
    
    async def _analyze_system_performance(self) -> List[PerformanceMetric]:
        """ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¶„ì„"""
        metrics = []
        
        # CPU ì„±ëŠ¥
        cpu_freq = psutil.cpu_freq()
        if cpu_freq:
            cpu_metric = PerformanceMetric(
                name="cpu_frequency",
                value=cpu_freq.current,
                unit="MHz",
                target_value=cpu_freq.max * 0.8,
                improvement_potential=max(0, (cpu_freq.max * 0.8 - cpu_freq.current) / cpu_freq.max)
            )
            metrics.append(cpu_metric)
        
        # ë©”ëª¨ë¦¬ ì„±ëŠ¥
        memory = psutil.virtual_memory()
        swap = psutil.swap_memory()
        
        memory_efficiency = (memory.available / memory.total) * 100
        metrics.append(PerformanceMetric(
            name="memory_efficiency",
            value=memory_efficiency,
            unit="percent",
            target_value=70.0,
            improvement_potential=max(0, (70.0 - memory_efficiency) / 70.0)
        ))
        
        # ìŠ¤ì™‘ ì‚¬ìš©ë¥ 
        swap_usage = swap.percent
        metrics.append(PerformanceMetric(
            name="swap_usage",
            value=swap_usage,
            unit="percent",
            target_value=5.0,
            improvement_potential=max(0, (swap_usage - 5.0) / 95.0)
        ))
        
        return metrics
    
    async def _analyze_database_performance(self) -> List[PerformanceMetric]:
        """ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¶„ì„"""
        metrics = []
        
        try:
            import asyncpg
            
            conn = await asyncpg.connect(
                host='localhost',
                port=5432,
                user='freqtrade',
                password=os.getenv('DB_PASSWORD'),
                database='freqtrade'
            )
            
            # í™œì„± ì—°ê²° ìˆ˜
            active_connections = await conn.fetchval(
                "SELECT count(*) FROM pg_stat_activity WHERE state = 'active'"
            )
            
            metrics.append(PerformanceMetric(
                name="active_connections",
                value=float(active_connections),
                unit="connections",
                target_value=50.0
            ))
            
            # ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸°
            db_size = await conn.fetchval(
                "SELECT pg_database_size('freqtrade')"
            )
            
            metrics.append(PerformanceMetric(
                name="database_size",
                value=float(db_size) / (1024 * 1024),  # MB
                unit="MB"
            ))
            
            # ìºì‹œ íˆíŠ¸ ë¹„ìœ¨
            cache_hit_ratio = await conn.fetchval("""
                SELECT round(
                    sum(blks_hit) * 100.0 / (sum(blks_hit) + sum(blks_read)), 2
                ) FROM pg_stat_database WHERE datname = 'freqtrade'
            """)
            
            if cache_hit_ratio:
                metrics.append(PerformanceMetric(
                    name="cache_hit_ratio",
                    value=float(cache_hit_ratio),
                    unit="percent",
                    target_value=95.0,
                    improvement_potential=max(0, (95.0 - float(cache_hit_ratio)) / 95.0)
                ))
            
            await conn.close()
            
        except Exception as e:
            self.logger.error(f"Database performance analysis failed: {e}")
        
        return metrics
    
    async def _analyze_application_performance(self) -> List[PerformanceMetric]:
        """ì• í”Œë¦¬ì¼€ì´ì…˜ ì„±ëŠ¥ ë¶„ì„"""
        metrics = []
        
        try:
            # Freqtrade API ì‘ë‹µ ì‹œê°„ ì¸¡ì •
            import aiohttp
            
            start_time = time.time()
            async with aiohttp.ClientSession() as session:
                async with session.get('http://localhost:8080/api/v1/ping') as resp:
                    if resp.status == 200:
                        response_time = (time.time() - start_time) * 1000  # ms
                        
                        metrics.append(PerformanceMetric(
                            name="api_response_time",
                            value=response_time,
                            unit="ms",
                            target_value=100.0,
                            improvement_potential=max(0, (response_time - 100.0) / 1000.0)
                        ))
            
            # Docker ì»¨í…Œì´ë„ˆ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
            docker_stats = await self._get_docker_stats()
            for container_name, stats in docker_stats.items():
                metrics.append(PerformanceMetric(
                    name=f"{container_name}_cpu_usage",
                    value=stats.get('cpu_percent', 0),
                    unit="percent"
                ))
                
                metrics.append(PerformanceMetric(
                    name=f"{container_name}_memory_usage",
                    value=stats.get('memory_percent', 0),
                    unit="percent"
                ))
            
        except Exception as e:
            self.logger.error(f"Application performance analysis failed: {e}")
        
        return metrics
    
    async def _get_docker_stats(self) -> Dict[str, Dict]:
        """Docker ì»¨í…Œì´ë„ˆ í†µê³„ ì¡°íšŒ"""
        try:
            result = subprocess.run(
                ['docker', 'stats', '--no-stream', '--format', 'json'],
                capture_output=True, text=True
            )
            
            if result.returncode == 0:
                stats = {}
                for line in result.stdout.strip().split('\n'):
                    if line:
                        data = json.loads(line)
                        stats[data['Name']] = {
                            'cpu_percent': float(data['CPUPerc'].rstrip('%')),
                            'memory_percent': float(data['MemPerc'].rstrip('%'))
                        }
                return stats
            
        except Exception as e:
            self.logger.error(f"Docker stats collection failed: {e}")
        
        return {}
    
    async def _analyze_network_performance(self) -> List[PerformanceMetric]:
        """ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥ ë¶„ì„"""
        metrics = []
        
        try:
            # ë„¤íŠ¸ì›Œí¬ í†µê³„
            net_io = psutil.net_io_counters()
            
            # ì²˜ë¦¬ëŸ‰ (ë°”ì´íŠ¸/ì´ˆ)
            if hasattr(self, '_last_net_io') and hasattr(self, '_last_net_time'):
                time_delta = time.time() - self._last_net_time
                bytes_sent_per_sec = (net_io.bytes_sent - self._last_net_io.bytes_sent) / time_delta
                bytes_recv_per_sec = (net_io.bytes_recv - self._last_net_io.bytes_recv) / time_delta
                
                metrics.append(PerformanceMetric(
                    name="network_throughput_sent",
                    value=bytes_sent_per_sec / (1024 * 1024),  # MB/s
                    unit="MB/s"
                ))
                
                metrics.append(PerformanceMetric(
                    name="network_throughput_recv",
                    value=bytes_recv_per_sec / (1024 * 1024),  # MB/s
                    unit="MB/s"
                ))
            
            self._last_net_io = net_io
            self._last_net_time = time.time()
            
            # ë„¤íŠ¸ì›Œí¬ ì§€ì—°ì‹œê°„ (ping)
            ping_result = subprocess.run(
                ['ping', '-c', '3', '8.8.8.8'],
                capture_output=True, text=True
            )
            
            if ping_result.returncode == 0:
                # ping ê²°ê³¼ì—ì„œ í‰ê·  ì§€ì—°ì‹œê°„ ì¶”ì¶œ
                lines = ping_result.stdout.split('\n')
                for line in lines:
                    if 'avg' in line:
                        avg_latency = float(line.split('/')[-3])
                        metrics.append(PerformanceMetric(
                            name="network_latency",
                            value=avg_latency,
                            unit="ms",
                            target_value=50.0,
                            improvement_potential=max(0, (avg_latency - 50.0) / 200.0)
                        ))
                        break
            
        except Exception as e:
            self.logger.error(f"Network performance analysis failed: {e}")
        
        return metrics
    
    async def _analyze_storage_performance(self) -> List[PerformanceMetric]:
        """ìŠ¤í† ë¦¬ì§€ ì„±ëŠ¥ ë¶„ì„"""
        metrics = []
        
        try:
            # ë””ìŠ¤í¬ I/O
            disk_io = psutil.disk_io_counters()
            
            if hasattr(self, '_last_disk_io') and hasattr(self, '_last_disk_time'):
                time_delta = time.time() - self._last_disk_time
                
                read_speed = (disk_io.read_bytes - self._last_disk_io.read_bytes) / time_delta
                write_speed = (disk_io.write_bytes - self._last_disk_io.write_bytes) / time_delta
                
                metrics.append(PerformanceMetric(
                    name="disk_read_speed",
                    value=read_speed / (1024 * 1024),  # MB/s
                    unit="MB/s"
                ))
                
                metrics.append(PerformanceMetric(
                    name="disk_write_speed",
                    value=write_speed / (1024 * 1024),  # MB/s
                    unit="MB/s"
                ))
            
            self._last_disk_io = disk_io
            self._last_disk_time = time.time()
            
            # ë””ìŠ¤í¬ ì‚¬ìš©ë¥ 
            disk_usage = psutil.disk_usage('/')
            usage_percent = (disk_usage.used / disk_usage.total) * 100
            
            metrics.append(PerformanceMetric(
                name="disk_usage",
                value=usage_percent,
                unit="percent",
                target_value=80.0,
                improvement_potential=max(0, (usage_percent - 80.0) / 20.0)
            ))
            
        except Exception as e:
            self.logger.error(f"Storage performance analysis failed: {e}")
        
        return metrics
    
    async def generate_tuning_recommendations(self) -> List[TuningRecommendation]:
        """íŠœë‹ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        # ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰
        performance_data = await self.analyze_performance()
        
        recommendations = []
        
        # ì‹œìŠ¤í…œ íŠœë‹ ê¶Œì¥ì‚¬í•­
        system_recommendations = await self._generate_system_tuning(performance_data['system'])
        recommendations.extend(system_recommendations)
        
        # ë°ì´í„°ë² ì´ìŠ¤ íŠœë‹ ê¶Œì¥ì‚¬í•­
        db_recommendations = await self._generate_database_tuning(performance_data['database'])
        recommendations.extend(db_recommendations)
        
        # ì• í”Œë¦¬ì¼€ì´ì…˜ íŠœë‹ ê¶Œì¥ì‚¬í•­
        app_recommendations = await self._generate_application_tuning(performance_data['application'])
        recommendations.extend(app_recommendations)
        
        return recommendations
    
    async def _generate_system_tuning(self, metrics: List[PerformanceMetric]) -> List[TuningRecommendation]:
        """ì‹œìŠ¤í…œ íŠœë‹ ê¶Œì¥ì‚¬í•­"""
        recommendations = []
        
        # ìŠ¤ì™‘ ì‚¬ìš©ë¥ ì´ ë†’ì€ ê²½ìš°
        swap_metric = next((m for m in metrics if m.name == "swap_usage"), None)
        if swap_metric and swap_metric.value > 10:
            recommendations.append(TuningRecommendation(
                category=TuningCategory.SYSTEM,
                action=TuningAction.PARAMETER_CHANGE,
                description="ìŠ¤ì™‘ ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. ìŠ¤ì™‘ ìŠ¤ì™‘í”¼ë‹ˆìŠ¤ë¥¼ ì¡°ì •í•˜ì„¸ìš”.",
                current_config={"vm.swappiness": "60"},
                recommended_config={"vm.swappiness": "10"},
                expected_improvement=15.0,
                risk_level="low",
                implementation_effort="low"
            ))
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì´ ë‚®ì€ ê²½ìš°
        memory_metric = next((m for m in metrics if m.name == "memory_efficiency"), None)
        if memory_metric and memory_metric.value < 50:
            recommendations.append(TuningRecommendation(
                category=TuningCategory.SYSTEM,
                action=TuningAction.RESOURCE_REALLOCATION,
                description="ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤. ë©”ëª¨ë¦¬ ì¦ì„¤ì„ ê³ ë ¤í•˜ì„¸ìš”.",
                current_config={"memory": "current"},
                recommended_config={"memory": "upgrade"},
                expected_improvement=25.0,
                risk_level="low",
                implementation_effort="medium"
            ))
        
        return recommendations
    
    async def _generate_database_tuning(self, metrics: List[PerformanceMetric]) -> List[TuningRecommendation]:
        """ë°ì´í„°ë² ì´ìŠ¤ íŠœë‹ ê¶Œì¥ì‚¬í•­"""
        recommendations = []
        
        # ìºì‹œ íˆíŠ¸ ë¹„ìœ¨ì´ ë‚®ì€ ê²½ìš°
        cache_metric = next((m for m in metrics if m.name == "cache_hit_ratio"), None)
        if cache_metric and cache_metric.value < 90:
            recommendations.append(TuningRecommendation(
                category=TuningCategory.DATABASE,
                action=TuningAction.CONFIGURATION_UPDATE,
                description="ë°ì´í„°ë² ì´ìŠ¤ ìºì‹œ íˆíŠ¸ ë¹„ìœ¨ì´ ë‚®ìŠµë‹ˆë‹¤. shared_buffersë¥¼ ì¦ê°€ì‹œí‚¤ì„¸ìš”.",
                current_config={"shared_buffers": "128MB"},
                recommended_config={"shared_buffers": "256MB"},
                expected_improvement=20.0,
                risk_level="low",
                implementation_effort="low"
            ))
        
        return recommendations
    
    async def _generate_application_tuning(self, metrics: List[PerformanceMetric]) -> List[TuningRecommendation]:
        """ì• í”Œë¦¬ì¼€ì´ì…˜ íŠœë‹ ê¶Œì¥ì‚¬í•­"""
        recommendations = []
        
        # API ì‘ë‹µ ì‹œê°„ì´ ëŠë¦° ê²½ìš°
        api_metric = next((m for m in metrics if m.name == "api_response_time"), None)
        if api_metric and api_metric.value > 200:
            recommendations.append(TuningRecommendation(
                category=TuningCategory.APPLICATION,
                action=TuningAction.CACHING_OPTIMIZATION,
                description="API ì‘ë‹µ ì‹œê°„ì´ ëŠë¦½ë‹ˆë‹¤. Redis ìºì‹±ì„ í™œì„±í™”í•˜ì„¸ìš”.",
                current_config={"caching": "disabled"},
                recommended_config={"caching": "redis"},
                expected_improvement=30.0,
                risk_level="low",
                implementation_effort="medium"
            ))
        
        return recommendations
    
    async def apply_tuning_recommendation(self, recommendation: TuningRecommendation) -> bool:
        """íŠœë‹ ê¶Œì¥ì‚¬í•­ ì ìš©"""
        
        try:
            self.logger.info(f"Applying tuning recommendation: {recommendation.description}")
            
            if recommendation.category == TuningCategory.SYSTEM:
                return await self._apply_system_tuning(recommendation)
            elif recommendation.category == TuningCategory.DATABASE:
                return await self._apply_database_tuning(recommendation)
            elif recommendation.category == TuningCategory.APPLICATION:
                return await self._apply_application_tuning(recommendation)
            
            return False
            
        except Exception as e:
            self.logger.error(f"Failed to apply tuning recommendation: {e}")
            return False
    
    async def _apply_system_tuning(self, recommendation: TuningRecommendation) -> bool:
        """ì‹œìŠ¤í…œ íŠœë‹ ì ìš©"""
        
        if recommendation.action == TuningAction.PARAMETER_CHANGE:
            for param, value in recommendation.recommended_config.items():
                if param == "vm.swappiness":
                    cmd = ['sysctl', '-w', f'{param}={value}']
                    result = subprocess.run(cmd, capture_output=True, text=True)
                    
                    if result.returncode == 0:
                        # ì˜êµ¬ ì„¤ì •
                        with open('/etc/sysctl.conf', 'a') as f:
                            f.write(f'\n{param}={value}\n')
                        return True
        
        return False
    
    async def _apply_database_tuning(self, recommendation: TuningRecommendation) -> bool:
        """ë°ì´í„°ë² ì´ìŠ¤ íŠœë‹ ì ìš©"""
        
        # PostgreSQL ì„¤ì • íŒŒì¼ ìˆ˜ì •
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” postgresql.conf íŒŒì¼ì„ ìˆ˜ì •í•˜ê³  ì¬ì‹œì‘
        self.logger.info("Database tuning would require manual configuration")
        return True
    
    async def _apply_application_tuning(self, recommendation: TuningRecommendation) -> bool:
        """ì• í”Œë¦¬ì¼€ì´ì…˜ íŠœë‹ ì ìš©"""
        
        # Docker Compose ì„¤ì • ìˆ˜ì • ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ ì„¤ì • ì ìš©
        self.logger.info("Application tuning would require configuration update")
        return True

---

## ğŸ‰ **ê²°ë¡  ë° ë‹¤ìŒ ë‹¨ê³„**

ì´ **03_FUTURES_AUTOMATION_SETUP.md** ê°€ì´ë“œë¥¼ í†µí•´ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ì™„ì „ ìë™í™” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

### âœ… **ë‹¬ì„±ëœ ìë™í™” ìˆ˜ì¤€**

1. **ğŸ—ï¸ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜**: í™•ì¥ ê°€ëŠ¥í•˜ê³  ìœ ì—°í•œ ì‹œìŠ¤í…œ ì„¤ê³„
2. **ğŸ³ ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜**: Dockerì™€ Kubernetesë¥¼ í†µí•œ ì™„ì „ ìë™í™” ë°°í¬
3. **ğŸ”„ CI/CD íŒŒì´í”„ë¼ì¸**: GitHub Actions ê¸°ë°˜ ìë™ í…ŒìŠ¤íŠ¸ ë° ë°°í¬
4. **ğŸ¯ ë©€í‹° ì „ëµ ê´€ë¦¬**: ë™ì  ë¡œë”©ê³¼ ì„±ê³¼ ê¸°ë°˜ ìë™ ìŠ¤ì¼€ì¼ë§
5. **ğŸ“Š ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸**: ê³ ì„±ëŠ¥ ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„
6. **ğŸ“¢ í†µí•© ì•Œë¦¼ ì‹œìŠ¤í…œ**: ì§€ëŠ¥ì  ë¼ìš°íŒ…ê³¼ ì—ìŠ¤ì»¬ë ˆì´ì…˜
7. **ğŸ”„ ìë™ ì¥ì•  ë³µêµ¬**: ì„œí‚· ë¸Œë ˆì´ì»¤ì™€ ìë™ í˜ì¼ì˜¤ë²„
8. **âš¡ ì„±ê³¼ ìµœì í™”**: ML ê¸°ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ê³¼ A/B í…ŒìŠ¤íŠ¸
9. **ğŸ” ë³´ì•ˆ ë° ì»´í”Œë¼ì´ì–¸ìŠ¤**: ì™„ì „ ìë™í™”ëœ ë³´ì•ˆ ê´€ë¦¬
10. **ğŸš€ ìš´ì˜ ìë™í™”**: ë°±ì—…, ì—…ë°ì´íŠ¸, ì„±ëŠ¥ íŠœë‹ê¹Œì§€ ëª¨ë“  ê²ƒ ìë™í™”

### ğŸš€ **ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­**

1. **ë‹¨ê³„ë³„ êµ¬í˜„**: ê¸°ë³¸ Docker ì„¤ì •ë¶€í„° ì‹œì‘í•˜ì—¬ ì ì§„ì ìœ¼ë¡œ í™•ì¥
2. **ëª¨ë‹ˆí„°ë§ ìš°ì„ **: Prometheus + Grafana ëŒ€ì‹œë³´ë“œë¥¼ ë¨¼ì € êµ¬ì¶•
3. **ë³´ì•ˆ ê°•í™”**: HashiCorp Vaultì™€ ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆì„ ì¡°ê¸°ì— ì„¤ì •
4. **ì„±ëŠ¥ ìµœì í™”**: ê¸°ë³¸ ì‹œìŠ¤í…œì´ ì•ˆì •í™”ëœ í›„ ML ê¸°ë°˜ ìµœì í™” ë„ì…
5. **íŒ€ êµìœ¡**: ìš´ì˜íŒ€ì˜ ìë™í™” ì‹œìŠ¤í…œ ì´í•´ë„ í–¥ìƒ

### ğŸ’¡ **í•µì‹¬ ì„±ê³µ ìš”ì†Œ**

- **ì ì§„ì  êµ¬í˜„**: í•œ ë²ˆì— ëª¨ë“  ê²ƒì„ êµ¬í˜„í•˜ì§€ ë§ê³  ë‹¨ê³„ë³„ë¡œ ì ‘ê·¼
- **ì¶©ë¶„í•œ í…ŒìŠ¤íŠ¸**: ê° ìë™í™” ë‹¨ê³„ë§ˆë‹¤ ì² ì €í•œ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
- **ëª¨ë‹ˆí„°ë§ ì¤‘ì‹¬**: ëª¨ë“  ìë™í™”ì— ëŒ€í•œ ê´€ì°°ì„± í™•ë³´
- **ë¬¸ì„œí™”**: ìë™í™”ëœ í”„ë¡œì„¸ìŠ¤ì˜ ëª…í™•í•œ ë¬¸ì„œí™”
- **íŒ€ í˜‘ì—…**: DevOps, ê°œë°œ, ìš´ì˜íŒ€ ê°„ì˜ ê¸´ë°€í•œ í˜‘ë ¥

ì´ì œ **24ì‹œê°„ ë¬´ì¤‘ë‹¨ ë¬´ì¸ ìš´ì˜**ì´ ê°€ëŠ¥í•œ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ìë™í™” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ¯

---

<div align="center">

**ğŸ¤– ì™„ì „ ìë™í™”ëœ Futures íŠ¸ë ˆì´ë”© ì‹œìŠ¤í…œìœ¼ë¡œ ìƒˆë¡œìš´ ì°¨ì›ì˜ íš¨ìœ¨ì„±ì„ ê²½í—˜í•˜ì„¸ìš”! ğŸš€**

[![ì‹œì‘í•˜ê¸°](https://img.shields.io/badge/ì‹œì‘í•˜ê¸°-ğŸš€%20Get%20Started-success?style=for-the-badge)](README_FUTURES.md)
[![ë¬¸ì˜í•˜ê¸°](https://img.shields.io/badge/ë¬¸ì˜í•˜ê¸°-ğŸ’¬%20Contact-blue?style=for-the-badge)](mailto:support@freqtrade.com)

</div>
                        await handler(event)
                    except Exception as e:
                        print(f"ì´ë²¤íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

class AutomationOrchestrator:
    """ìë™í™” ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""
    
    def __init__(self, event_bus: EventBus):
        self.event_bus = event_bus
        self.strategies = {}
        self.risk_manager = None
        self.position_manager = None
        
    async def handle_trade_signal(self, event: Event):
        """ê±°ë˜ ì‹ í˜¸ ì²˜ë¦¬"""
        signal_data = event.data
        
        # ë¦¬ìŠ¤í¬ ê²€ì¦
        if await self.risk_manager.validate_signal(signal_data):
            # í¬ì§€ì…˜ ê´€ë¦¬ìì—ê²Œ ì „ë‹¬
            await self.position_manager.execute_signal(signal_data)
            
            # ì‹¤í–‰ ê²°ê³¼ ì´ë²¤íŠ¸ ë°œí–‰
            result_event = Event(
                event_id=f"execution_{event.event_id}",
                event_type=EventType.POSITION_UPDATE,
                source="automation_orchestrator",
                timestamp=time.time(),
                data={"status": "executed", "original_signal": signal_data}
            )
            await self.event_bus.publish(result_event)
    
    async def start(self):
        """ìë™í™” ì‹œìŠ¤í…œ ì‹œì‘"""
        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ë“±ë¡
        self.event_bus.subscribe(EventType.TRADE_SIGNAL, self.handle_trade_signal)
        self.event_bus.subscribe(EventType.RISK_ALERT, self.handle_risk_alert)
        
        # ì´ë²¤íŠ¸ ì²˜ë¦¬ ì‹œì‘
        await self.event_bus.process_events()
```

### ğŸ”§ **ì„œë¹„ìŠ¤ ë©”ì‹œ êµ¬ì„±**

```yaml
# infrastructure/service-mesh/istio-config.yaml
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  name: freqtrade-control-plane
spec:
  values:
    global:
      meshID: freqtrade-mesh
      network: freqtrade-network
  components:
    pilot:
      k8s:
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
    ingressGateways:
    - name: freqtrade-gateway
      enabled: true
      k8s:
        service:
          type: LoadBalancer
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
---
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: freqtrade-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "freqtrade-api.local"
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: freqtrade-tls
    hosts:
    - "freqtrade-api.local"
```

---

## ğŸ³ **Docker ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜**

### ğŸ“¦ **Multi-container Freqtrade ì„¤ì •**

```yaml
# docker-compose.automation.yml
version: '3.8'

services:
  # ë©”ì¸ íŠ¸ë ˆì´ë”© ì—”ì§„ë“¤
  freqtrade-strategy-1:
    build:
      context: .
      dockerfile: docker/Dockerfile.freqtrade
      args:
        STRATEGY_NAME: FuturesAIRiskStrategy
    environment:
      - FREQTRADE_INSTANCE_ID=strategy-1
      - REDIS_URL=redis://redis-cluster:6379
      - POSTGRES_URL=postgresql://user:pass@timescaledb:5432/freqtrade
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    volumes:
      - ./user_data:/freqtrade/user_data
      - ./strategies:/freqtrade/user_data/strategies
    networks:
      - freqtrade-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/api/v1/ping || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    
  freqtrade-strategy-2:
    build:
      context: .
      dockerfile: docker/Dockerfile.freqtrade
      args:
        STRATEGY_NAME: RossCameronFuturesStrategy
    environment:
      - FREQTRADE_INSTANCE_ID=strategy-2
      - REDIS_URL=redis://redis-cluster:6379
      - POSTGRES_URL=postgresql://user:pass@timescaledb:5432/freqtrade
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    volumes:
      - ./user_data:/freqtrade/user_data
      - ./strategies:/freqtrade/user_data/strategies
    networks:
      - freqtrade-network
    restart: unless-stopped

  # ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì„œë¹„ìŠ¤
  risk-manager:
    build:
      context: .
      dockerfile: docker/Dockerfile.risk-manager
    environment:
      - REDIS_URL=redis://redis-cluster:6379
      - POSTGRES_URL=postgresql://user:pass@timescaledb:5432/freqtrade
      - MAX_PORTFOLIO_RISK=0.02  # 2% VaR
      - MAX_POSITION_SIZE=0.1    # 10% per position
    volumes:
      - ./risk_management:/app/risk_management
    networks:
      - freqtrade-network
    restart: unless-stopped

  # í¬ì§€ì…˜ ê´€ë¦¬ ì„œë¹„ìŠ¤
  position-manager:
    build:
      context: .
      dockerfile: docker/Dockerfile.position-manager
    environment:
      - REDIS_URL=redis://redis-cluster:6379
      - POSTGRES_URL=postgresql://user:pass@timescaledb:5432/freqtrade
      - BINANCE_API_KEY=${BINANCE_API_KEY}
      - BINANCE_API_SECRET=${BINANCE_API_SECRET}
    networks:
      - freqtrade-network
    restart: unless-stopped

  # ë°ì´í„°ë² ì´ìŠ¤ - TimescaleDB (ì‹œê³„ì—´ ë°ì´í„° ìµœì í™”)
  timescaledb:
    image: timescale/timescaledb:latest-pg14
    environment:
      - POSTGRES_DB=freqtrade
      - POSTGRES_USER=freqtrade
      - POSTGRES_PASSWORD=${DB_PASSWORD}
    volumes:
      - timescaledb_data:/var/lib/postgresql/data
      - ./sql/init_timescaledb.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    networks:
      - freqtrade-network
    restart: unless-stopped

  # Redis í´ëŸ¬ìŠ¤í„° (ìºì‹± ë° ì„¸ì…˜ ê´€ë¦¬)
  redis-cluster:
    image: redis/redis-stack-server:latest
    command: redis-server --appendonly yes --cluster-enabled yes --cluster-config-file nodes.conf --cluster-node-timeout 5000 --port 6379
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    networks:
      - freqtrade-network
    restart: unless-stopped

  # Message Queue - RabbitMQ
  rabbitmq:
    image: rabbitmq:3-management-alpine
    environment:
      - RABBITMQ_DEFAULT_USER=freqtrade
      - RABBITMQ_DEFAULT_PASS=${RABBITMQ_PASSWORD}
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
      - ./rabbitmq/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf
    ports:
      - "5672:5672"
      - "15672:15672"
    networks:
      - freqtrade-network
    restart: unless-stopped

  # Apache Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - freqtrade-network

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - freqtrade-network

  # ëª¨ë‹ˆí„°ë§ - Prometheus
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - freqtrade-network
    restart: unless-stopped

  # ëŒ€ì‹œë³´ë“œ - Grafana
  grafana:
    image: grafana/grafana:latest
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
    ports:
      - "3000:3000"
    networks:
      - freqtrade-network
    restart: unless-stopped

  # ë¡œê·¸ ìˆ˜ì§‘ - Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
    networks:
      - freqtrade-network

  # ë¡œê·¸ ì²˜ë¦¬ - Logstash
  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.0
    volumes:
      - ./monitoring/logstash/pipeline:/usr/share/logstash/pipeline
    depends_on:
      - elasticsearch
    networks:
      - freqtrade-network

  # ë¡œê·¸ ì‹œê°í™” - Kibana
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
    networks:
      - freqtrade-network

  # ë¶„ì‚° ì¶”ì  - Jaeger
  jaeger:
    image: jaegertracing/all-in-one:latest
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    ports:
      - "16686:16686"
      - "14250:14250"
    networks:
      - freqtrade-network

  # API Gateway - Kong
  kong:
    image: kong/kong-gateway:latest
    environment:
      - KONG_DATABASE=off
      - KONG_DECLARATIVE_CONFIG=/kong/declarative/kong.yml
      - KONG_PROXY_ACCESS_LOG=/dev/stdout
      - KONG_ADMIN_ACCESS_LOG=/dev/stdout
      - KONG_PROXY_ERROR_LOG=/dev/stderr
      - KONG_ADMIN_ERROR_LOG=/dev/stderr
      - KONG_ADMIN_LISTEN=0.0.0.0:8001
    volumes:
      - ./api-gateway/kong.yml:/kong/declarative/kong.yml
    ports:
      - "8000:8000"
      - "8001:8001"
      - "8443:8443"
      - "8444:8444"
    networks:
      - freqtrade-network

networks:
  freqtrade-network:
    driver: bridge

volumes:
  timescaledb_data:
  redis_data:
  rabbitmq_data:
  prometheus_data:
  grafana_data:
  elasticsearch_data:
```

### ğŸ”§ **ë³¼ë¥¨ ê´€ë¦¬ ë° ë°ì´í„° ì˜ì†ì„±**

```bash
# scripts/volume_management.sh
#!/bin/bash
# ë³¼ë¥¨ ê´€ë¦¬ ìë™í™” ìŠ¤í¬ë¦½íŠ¸

set -e

BACKUP_DIR="/backup/freqtrade"
DATE=$(date +%Y%m%d_%H%M%S)

create_volumes() {
    echo "ğŸ“¦ Docker ë³¼ë¥¨ ìƒì„± ì¤‘..."
    
    # ë°ì´í„° ë³¼ë¥¨ë“¤
    docker volume create freqtrade_timescaledb_data
    docker volume create freqtrade_redis_data
    docker volume create freqtrade_prometheus_data
    docker volume create freqtrade_grafana_data
    docker volume create freqtrade_elasticsearch_data
    
    echo "âœ… ë³¼ë¥¨ ìƒì„± ì™„ë£Œ"
}

backup_volumes() {
    echo "ğŸ’¾ ë³¼ë¥¨ ë°±ì—… ì‹œì‘..."
    
    mkdir -p ${BACKUP_DIR}/${DATE}
    
    # TimescaleDB ë°±ì—…
    docker run --rm \
        -v freqtrade_timescaledb_data:/var/lib/postgresql/data:ro \
        -v ${BACKUP_DIR}/${DATE}:/backup \
        postgres:14 \
        pg_dumpall -h localhost -U freqtrade -f /backup/timescaledb_backup.sql
    
    # Redis ë°±ì—…
    docker run --rm \
        -v freqtrade_redis_data:/data:ro \
        -v ${BACKUP_DIR}/${DATE}:/backup \
        redis:alpine \
        cp /data/dump.rdb /backup/redis_backup.rdb
    
    # Grafana ë°±ì—…
    docker run --rm \
        -v freqtrade_grafana_data:/var/lib/grafana:ro \
        -v ${BACKUP_DIR}/${DATE}:/backup \
        alpine:latest \
        tar -czf /backup/grafana_backup.tar.gz -C /var/lib/grafana .
    
    echo "âœ… ë³¼ë¥¨ ë°±ì—… ì™„ë£Œ: ${BACKUP_DIR}/${DATE}"
}

restore_volumes() {
    local RESTORE_DATE=$1
    
    if [ -z "$RESTORE_DATE" ]; then
        echo "âŒ ë³µì› ë‚ ì§œë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”. ì˜ˆ: 20241201_143000"
        exit 1
    fi
    
    if [ ! -d "${BACKUP_DIR}/${RESTORE_DATE}" ]; then
        echo "âŒ ë°±ì—… ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: ${BACKUP_DIR}/${RESTORE_DATE}"
        exit 1
    fi
    
    echo "ğŸ”„ ë³¼ë¥¨ ë³µì› ì‹œì‘: ${RESTORE_DATE}"
    
    # ì„œë¹„ìŠ¤ ì¤‘ì§€
    docker-compose -f docker-compose.automation.yml down
    
    # TimescaleDB ë³µì›
    docker run --rm \
        -v freqtrade_timescaledb_data:/var/lib/postgresql/data \
        -v ${BACKUP_DIR}/${RESTORE_DATE}:/backup:ro \
        postgres:14 \
        psql -h localhost -U freqtrade -f /backup/timescaledb_backup.sql
    
    # Redis ë³µì›
    docker run --rm \
        -v freqtrade_redis_data:/data \
        -v ${BACKUP_DIR}/${RESTORE_DATE}:/backup:ro \
        redis:alpine \
        cp /backup/redis_backup.rdb /data/dump.rdb
    
    # Grafana ë³µì›
    docker run --rm \
        -v freqtrade_grafana_data:/var/lib/grafana \
        -v ${BACKUP_DIR}/${RESTORE_DATE}:/backup:ro \
        alpine:latest \
        tar -xzf /backup/grafana_backup.tar.gz -C /var/lib/grafana
    
    echo "âœ… ë³¼ë¥¨ ë³µì› ì™„ë£Œ"
}

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
case "$1" in
    "create")
        create_volumes
        ;;
    "backup")
        backup_volumes
        ;;
    "restore")
        restore_volumes $2
        ;;
    *)
        echo "ì‚¬ìš©ë²•: $0 {create|backup|restore <date>}"
        exit 1
        ;;
esac
```

### ğŸŒ **ë„¤íŠ¸ì›Œí¬ êµ¬ì„± ë° ë³´ì•ˆ**

```yaml
# docker/networks/secure-network.yml
version: '3.8'

networks:
  # í”„ë¡ íŠ¸ì—”ë“œ ë„¤íŠ¸ì›Œí¬ (ì™¸ë¶€ ì ‘ê·¼)
  frontend:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: ft-frontend
    ipam:
      config:
        - subnet: 172.20.1.0/24

  # ë°±ì—”ë“œ ë„¤íŠ¸ì›Œí¬ (ë‚´ë¶€ í†µì‹ )
  backend:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: ft-backend
    internal: true
    ipam:
      config:
        - subnet: 172.20.2.0/24

  # ë°ì´í„°ë² ì´ìŠ¤ ë„¤íŠ¸ì›Œí¬ (ê³ ë„ë¡œ ê²©ë¦¬)
  database:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: ft-database
    internal: true
    ipam:
      config:
        - subnet: 172.20.3.0/24

  # ëª¨ë‹ˆí„°ë§ ë„¤íŠ¸ì›Œí¬
  monitoring:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: ft-monitoring
    ipam:
      config:
        - subnet: 172.20.4.0/24
```

---

## ğŸ”„ **CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì¶•**

### ğŸš€ **GitHub Actions ì›Œí¬í”Œë¡œìš°**

```yaml
# .github/workflows/freqtrade-cicd.yml
name: Freqtrade Futures Automation CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: freqtrade-futures-automation

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      redis:
        image: redis:alpine
        ports:
          - 6379:6379
      postgres:
        image: timescale/timescaledb:latest-pg14
        env:
          POSTGRES_PASSWORD: testpassword
          POSTGRES_DB: freqtrade_test
        ports:
          - 5432:5432
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        pip install -r requirements-test.txt
        pip install -e .
    
    - name: Run linting
      run: |
        flake8 user_data/strategies/
        black --check user_data/strategies/
        isort --check-only user_data/strategies/
    
    - name: Run type checking
      run: |
        mypy user_data/strategies/
    
    - name: Run unit tests
      run: |
        pytest tests/unit/ -v --cov=user_data/strategies --cov-report=xml
    
    - name: Run integration tests
      run: |
        pytest tests/integration/ -v
      env:
        REDIS_URL: redis://localhost:6379
        POSTGRES_URL: postgresql://postgres:testpassword@localhost:5432/freqtrade_test
    
    - name: Run strategy backtests
      run: |
        freqtrade backtesting \
          --config tests/configs/test_config_futures.json \
          --strategy FuturesAIRiskStrategy \
          --timerange 20241101-20241130 \
          --breakdown day
    
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml

  security-scan:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Run security scan
      uses: github/super-linter/slim@v4
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        VALIDATE_PYTHON_BLACK: true
        VALIDATE_PYTHON_FLAKE8: true
        VALIDATE_YAML: true
        VALIDATE_DOCKERFILE: true
    
    - name: Run dependency vulnerability scan
      run: |
        pip install safety
        safety check
    
    - name: Scan Docker image for vulnerabilities
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'

  build-and-push:
    needs: [test, security-scan]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    outputs:
      image-digest: ${{ steps.build.outputs.digest }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}
    
    - name: Build and push
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        platforms: linux/amd64,linux/arm64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy-staging:
    needs: build-and-push
    runs-on: ubuntu-latest
    environment: staging
    
    steps:
    - name: Deploy to staging
      run: |
        echo "Deploying to staging environment..."
        # Kubernetes ë°°í¬ ë¡œì§
        kubectl set image deployment/freqtrade-app \
          freqtrade=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ needs.build-and-push.outputs.image-digest }} \
          --namespace=staging
        
        kubectl rollout status deployment/freqtrade-app --namespace=staging

  integration-tests:
    needs: deploy-staging
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Run integration tests against staging
      run: |
        pytest tests/integration/ -v --staging
      env:
        STAGING_API_URL: https://staging-api.freqtrade.local
        API_KEY: ${{ secrets.STAGING_API_KEY }}

  deploy-production:
    needs: integration-tests
    runs-on: ubuntu-latest
    environment: production
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Deploy to production
      run: |
        echo "Deploying to production environment..."
        
        # ë¸”ë£¨-ê·¸ë¦° ë°°í¬
        kubectl patch deployment freqtrade-app \
          -p '{"spec":{"template":{"spec":{"containers":[{"name":"freqtrade","image":"${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ needs.build-and-push.outputs.image-digest }}"}]}}}}' \
          --namespace=production
        
        kubectl rollout status deployment/freqtrade-app --namespace=production
        
        # í—¬ìŠ¤ì²´í¬
        kubectl wait --for=condition=ready pod -l app=freqtrade-app --namespace=production --timeout=300s
    
    - name: Run smoke tests
      run: |
        curl -f https://api.freqtrade.local/health || exit 1
        
    - name: Notify deployment success
      uses: 8398a7/action-slack@v3
      with:
        status: success
        text: "ğŸš€ í”„ë¡œë•ì…˜ ë°°í¬ ì„±ê³µ! ë²„ì „: ${{ github.sha }}"
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}
```

### ğŸ”§ **ìë™ í…ŒìŠ¤íŠ¸ ë° ê²€ì¦**

```python
# tests/integration/test_automation_pipeline.py
import pytest
import asyncio
import aiohttp
import websockets
from datetime import datetime, timedelta

class TestAutomationPipeline:
    """ìë™í™” íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    async def api_client(self):
        """API í´ë¼ì´ì–¸íŠ¸ í”½ìŠ¤ì²˜"""
        async with aiohttp.ClientSession() as session:
            yield session
    
    @pytest.mark.asyncio
    async def test_end_to_end_trading_flow(self, api_client):
        """ì¢…ë‹¨ê°„ ê±°ë˜ í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        
        # 1. ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬
        async with api_client.get("https://api.freqtrade.local/api/v1/ping") as resp:
            assert resp.status == 200
        
        # 2. ì „ëµ ë¡œë“œ í™•ì¸
        async with api_client.get("https://api.freqtrade.local/api/v1/strategies") as resp:
            strategies = await resp.json()
            assert "FuturesAIRiskStrategy" in [s["strategy_name"] for s in strategies]
        
        # 3. ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì‹  í…ŒìŠ¤íŠ¸
        uri = "wss://api.freqtrade.local/api/v1/ws"
        async with websockets.connect(uri) as websocket:
            await websocket.send(json.dumps({"type": "subscribe", "data": "ticker"}))
            
            message = await asyncio.wait_for(websocket.recv(), timeout=30.0)
            data = json.loads(message)
            assert data["type"] == "ticker_update"
            assert "BTCUSDT" in data["data"]
    
    @pytest.mark.asyncio
    async def test_risk_management_triggers(self, api_client):
        """ë¦¬ìŠ¤í¬ ê´€ë¦¬ íŠ¸ë¦¬ê±° í…ŒìŠ¤íŠ¸"""
        
        # ìœ„í—˜í•œ ì‹ í˜¸ ì „ì†¡
        dangerous_signal = {
            "symbol": "BTCUSDT",
            "side": "long",
            "leverage": 50,  # ê³¼ë„í•œ ë ˆë²„ë¦¬ì§€
            "size_percent": 0.8  # ê³¼ë„í•œ í¬ì§€ì…˜ í¬ê¸°
        }
        
        async with api_client.post(
            "https://api.freqtrade.local/api/v1/signals",
            json=dangerous_signal
        ) as resp:
            assert resp.status == 400  # ë¦¬ìŠ¤í¬ ê´€ë¦¬ìê°€ ê±°ë¶€í•´ì•¼ í•¨
            response = await resp.json()
            assert "risk_limit_exceeded" in response["error"]
    
    @pytest.mark.asyncio
    async def test_automated_scaling(self, api_client):
        """ìë™ ìŠ¤ì¼€ì¼ë§ í…ŒìŠ¤íŠ¸"""
        
        # ì‹œìŠ¤í…œ ë¡œë“œ ì¦ê°€ ì‹œë®¬ë ˆì´ì…˜
        tasks = []
        for i in range(100):
            task = asyncio.create_task(
                api_client.get("https://api.freqtrade.local/api/v1/status")
            )
            tasks.append(task)
        
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ëª¨ë“  ìš”ì²­ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì–´ì•¼ í•¨ (ì˜¤í†  ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ)
        success_count = sum(1 for r in responses if not isinstance(r, Exception))
        assert success_count >= 95  # 95% ì´ìƒ ì„±ê³µë¥ 

class TestDisasterRecovery:
    """ì¬í•´ ë³µêµ¬ í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.asyncio
    async def test_database_failover(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì¥ì•  ë³µêµ¬ í…ŒìŠ¤íŠ¸"""
        
        # ë©”ì¸ DB ì¤‘ë‹¨ ì‹œë®¬ë ˆì´ì…˜
        subprocess.run(["docker", "stop", "timescaledb-primary"], check=True)
        
        # ë³µì œ DBë¡œ ìë™ ì „í™˜ í™•ì¸
        await asyncio.sleep(30)  # ì¥ì•  ê°ì§€ ë° ì „í™˜ ëŒ€ê¸°
        
        async with aiohttp.ClientSession() as session:
            async with session.get("https://api.freqtrade.local/api/v1/trades") as resp:
                assert resp.status == 200  # ì—¬ì „íˆ ì‘ë™í•´ì•¼ í•¨
        
        # ë©”ì¸ DB ë³µêµ¬
        subprocess.run(["docker", "start", "timescaledb-primary"], check=True)
    
    @pytest.mark.asyncio
    async def test_service_auto_recovery(self):
        """ì„œë¹„ìŠ¤ ìë™ ë³µêµ¬ í…ŒìŠ¤íŠ¸"""
        
        # Freqtrade ì„œë¹„ìŠ¤ ê°•ì œ ì¢…ë£Œ
        subprocess.run(["docker", "kill", "--signal=KILL", "freqtrade-strategy-1"])
        
        # ìë™ ì¬ì‹œì‘ í™•ì¸ (ìµœëŒ€ 60ì´ˆ ëŒ€ê¸°)
        for _ in range(12):
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get("https://api.freqtrade.local/api/v1/ping") as resp:
                        if resp.status == 200:
                            break
            except:
                pass
            await asyncio.sleep(5)
        else:
            pytest.fail("ì„œë¹„ìŠ¤ê°€ 60ì´ˆ ë‚´ì— ë³µêµ¬ë˜ì§€ ì•ŠìŒ")
```

### ğŸ“‹ **ìŠ¤í…Œì´ì§•/í”„ë¡œë•ì…˜ ë°°í¬**

```bash
# scripts/deploy.sh
#!/bin/bash
# ë°°í¬ ìë™í™” ìŠ¤í¬ë¦½íŠ¸

set -e

ENVIRONMENT=${1:-staging}
IMAGE_TAG=${2:-latest}
NAMESPACE="freqtrade-${ENVIRONMENT}"

echo "ğŸš€ ${ENVIRONMENT} í™˜ê²½ìœ¼ë¡œ ë°°í¬ ì‹œì‘..."

# í™˜ê²½ë³„ ì„¤ì • ê²€ì¦
validate_environment() {
    case $ENVIRONMENT in
        "staging")
            REPLICAS=2
            RESOURCES_LIMIT_CPU="1000m"
            RESOURCES_LIMIT_MEMORY="2Gi"
            ;;
        "production")
            REPLICAS=5
            RESOURCES_LIMIT_CPU="2000m"
            RESOURCES_LIMIT_MEMORY="4Gi"
            ;;
        *)
            echo "âŒ ì§€ì›ë˜ì§€ ì•ŠëŠ” í™˜ê²½: $ENVIRONMENT"
            exit 1
            ;;
    esac
}

# Kubernetes ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì ìš©
deploy_to_kubernetes() {
    echo "ğŸ“¦ Kubernetes ë°°í¬ ì¤‘..."
    
    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±/ì—…ë°ì´íŠ¸
    kubectl create namespace $NAMESPACE --dry-run=client -o yaml | kubectl apply -f -
    
    # ConfigMap ë° Secret ì ìš©
    envsubst < k8s/configmap.yaml | kubectl apply -f -
    envsubst < k8s/secrets.yaml | kubectl apply -f -
    
    # ì„œë¹„ìŠ¤ ë°°í¬
    envsubst < k8s/deployment.yaml | kubectl apply -f -
    envsubst < k8s/service.yaml | kubectl apply -f -
    envsubst < k8s/ingress.yaml | kubectl apply -f -
    
    # ë°°í¬ ìƒíƒœ í™•ì¸
    kubectl rollout status deployment/freqtrade-app -n $NAMESPACE --timeout=300s
}

# í—¬ìŠ¤ì²´í¬
health_check() {
    echo "ğŸ¥ í—¬ìŠ¤ì²´í¬ ìˆ˜í–‰ ì¤‘..."
    
    local max_attempts=30
    local attempt=1
    
    while [ $attempt -le $max_attempts ]; do
        if kubectl get pods -n $NAMESPACE -l app=freqtrade-app | grep -q "Running"; then
            echo "âœ… í—¬ìŠ¤ì²´í¬ í†µê³¼"
            return 0
        fi
        
        echo "ëŒ€ê¸° ì¤‘... ($attempt/$max_attempts)"
        sleep 10
        ((attempt++))
    done
    
    echo "âŒ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨"
    exit 1
}

# ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸
smoke_test() {
    echo "ğŸ’¨ ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰ ì¤‘..."
    
    local api_url
    if [ "$ENVIRONMENT" = "staging" ]; then
        api_url="https://staging-api.freqtrade.local"
    else
        api_url="https://api.freqtrade.local"
    fi
    
    # API í•‘ í…ŒìŠ¤íŠ¸
    if curl -f "$api_url/api/v1/ping" > /dev/null 2>&1; then
        echo "âœ… API í•‘ í…ŒìŠ¤íŠ¸ í†µê³¼"
    else
        echo "âŒ API í•‘ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨"
        exit 1
    fi
    
    # ì „ëµ ë¡œë“œ í…ŒìŠ¤íŠ¸
    if curl -f "$api_url/api/v1/strategies" > /dev/null 2>&1; then
        echo "âœ… ì „ëµ ë¡œë“œ í…ŒìŠ¤íŠ¸ í†µê³¼"
    else
        echo "âŒ ì „ëµ ë¡œë“œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨"
        exit 1
    fi
}

# ë¡¤ë°± í•¨ìˆ˜
rollback() {
    echo "ğŸ”„ ì´ì „ ë²„ì „ìœ¼ë¡œ ë¡¤ë°± ì¤‘..."
    kubectl rollout undo deployment/freqtrade-app -n $NAMESPACE
    kubectl rollout status deployment/freqtrade-app -n $NAMESPACE --timeout=300s
    echo "âœ… ë¡¤ë°± ì™„ë£Œ"
}

# ë©”ì¸ ë°°í¬ ë¡œì§
main() {
    validate_environment
    
    export ENVIRONMENT
    export IMAGE_TAG
    export NAMESPACE
    export REPLICAS
    export RESOURCES_LIMIT_CPU
    export RESOURCES_LIMIT_MEMORY
    
    deploy_to_kubernetes
    health_check
    smoke_test
    
    echo "ğŸ‰ ${ENVIRONMENT} í™˜ê²½ ë°°í¬ ì„±ê³µ!"
}

# íŠ¸ë© ì„¤ì • (ì‹¤íŒ¨ì‹œ ë¡¤ë°±)
trap 'echo "âŒ ë°°í¬ ì‹¤íŒ¨! ë¡¤ë°± ìˆ˜í–‰..."; rollback; exit 1' ERR

# ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
main "$@"
```

---

## ğŸ¯ **ë©€í‹° ì „ëµ ê´€ë¦¬ ì‹œìŠ¤í…œ**

### ğŸ”„ **ì „ëµ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬**

```python
# automation/strategy_lifecycle_manager.py
from typing import Dict, List, Optional, Any
from enum import Enum
from dataclasses import dataclass, asdict
import asyncio
import json
import importlib
import sys
from pathlib import Path

class StrategyStatus(Enum):
    """ì „ëµ ìƒíƒœ"""
    INACTIVE = "inactive"
    LOADING = "loading"
    ACTIVE = "active"
    PAUSED = "paused"
    ERROR = "error"
    UNLOADING = "unloading"

class PerformanceMetric(Enum):
    """ì„±ê³¼ ì§€í‘œ"""
    TOTAL_RETURN = "total_return"
    SHARPE_RATIO = "sharpe_ratio" 
    MAX_DRAWDOWN = "max_drawdown"
    WIN_RATE = "win_rate"
    PROFIT_FACTOR = "profit_factor"
    CALMAR_RATIO = "calmar_ratio"

@dataclass
class StrategyConfig:
    """ì „ëµ ì„¤ì •"""
    name: str
    class_name: str
    module_path: str
    config: Dict[str, Any]
    resource_limits: Dict[str, Any]
    performance_targets: Dict[PerformanceMetric, float]
    auto_scaling: bool = True
    auto_pause_on_drawdown: float = 0.15  # 15% ë“œë¡œë‹¤ìš´ì‹œ ìë™ ì¼ì‹œì •ì§€

@dataclass
class StrategyInstance:
    """ì „ëµ ì¸ìŠ¤í„´ìŠ¤"""
    id: str
    config: StrategyConfig
    status: StrategyStatus
    instance: Optional[Any] = None
    performance: Dict[str, float] = None
    error_count: int = 0
    last_error: Optional[str] = None
    created_at: float = 0
    updated_at: float = 0

class StrategyLifecycleManager:
    """ì „ëµ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬ì"""
    
    def __init__(self, event_bus, resource_monitor, performance_analyzer):
        self.event_bus = event_bus
        self.resource_monitor = resource_monitor
        self.performance_analyzer = performance_analyzer
        self.strategies: Dict[str, StrategyInstance] = {}
        self.strategy_configs: Dict[str, StrategyConfig] = {}
        self.is_running = False
        
    async def initialize(self):
        """ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        # ì„¤ì • íŒŒì¼ì—ì„œ ì „ëµ ë¡œë“œ
        await self._load_strategy_configs()
        
        # ì´ë²¤íŠ¸ êµ¬ë…
        self.event_bus.subscribe("strategy_performance_update", self._handle_performance_update)
        self.event_bus.subscribe("resource_usage_update", self._handle_resource_update)
        
        self.is_running = True
        
        # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì‹œì‘
        asyncio.create_task(self._lifecycle_monitor())
    
    async def _load_strategy_configs(self):
        """ì „ëµ ì„¤ì • ë¡œë“œ"""
        config_path = Path("strategies/configs")
        
        for config_file in config_path.glob("*.json"):
            with open(config_file, 'r') as f:
                config_data = json.load(f)
                
            strategy_config = StrategyConfig(**config_data)
            self.strategy_configs[strategy_config.name] = strategy_config
    
    async def load_strategy(self, strategy_name: str) -> bool:
        """ì „ëµ ë¡œë“œ"""
        if strategy_name not in self.strategy_configs:
            raise ValueError(f"Unknown strategy: {strategy_name}")
        
        config = self.strategy_configs[strategy_name]
        strategy_id = f"{strategy_name}_{int(time.time())}"
        
        # ì „ëµ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        instance = StrategyInstance(
            id=strategy_id,
            config=config,
            status=StrategyStatus.LOADING,
            created_at=time.time(),
            updated_at=time.time()
        )
        
        self.strategies[strategy_id] = instance
        
        try:
            # ì „ëµ ëª¨ë“ˆ ë™ì  ë¡œë“œ
            module = importlib.import_module(config.module_path)
            strategy_class = getattr(module, config.class_name)
            
            # ì „ëµ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            strategy_instance = strategy_class(config.config)
            
            instance.instance = strategy_instance
            instance.status = StrategyStatus.ACTIVE
            instance.updated_at = time.time()
            
            # ì´ë²¤íŠ¸ ë°œí–‰
            await self.event_bus.publish({
                "type": "strategy_loaded",
                "strategy_id": strategy_id,
                "strategy_name": strategy_name
            })
            
            return True
            
        except Exception as e:
            instance.status = StrategyStatus.ERROR
            instance.error_count += 1
            instance.last_error = str(e)
            
            await self.event_bus.publish({
                "type": "strategy_load_failed",
                "strategy_id": strategy_id,
                "error": str(e)
            })
            
            return False
    
    async def unload_strategy(self, strategy_id: str) -> bool:
        """ì „ëµ ì–¸ë¡œë“œ"""
        if strategy_id not in self.strategies:
            return False
        
        instance = self.strategies[strategy_id]
        instance.status = StrategyStatus.UNLOADING
        
        try:
            # ì§„í–‰ ì¤‘ì¸ ê±°ë˜ ì™„ë£Œ ëŒ€ê¸°
            if hasattr(instance.instance, 'close_all_positions'):
                await instance.instance.close_all_positions()
            
            # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            if hasattr(instance.instance, 'cleanup'):
                await instance.instance.cleanup()
            
            # ì¸ìŠ¤í„´ìŠ¤ ì œê±°
            del self.strategies[strategy_id]
            
            await self.event_bus.publish({
                "type": "strategy_unloaded",
                "strategy_id": strategy_id
            })
            
            return True
            
        except Exception as e:
            instance.status = StrategyStatus.ERROR
            instance.last_error = str(e)
            
            return False
    
    async def _handle_performance_update(self, event):
        """ì„±ê³¼ ì—…ë°ì´íŠ¸ ì²˜ë¦¬"""
        strategy_id = event.get("strategy_id")
        performance_data = event.get("performance_data")
        
        if strategy_id in self.strategies:
            instance = self.strategies[strategy_id]
            instance.performance = performance_data
            instance.updated_at = time.time()
            
            # ì„±ê³¼ ê¸°ë°˜ ìë™ ì•¡ì…˜
            await self._evaluate_performance_actions(instance)
    
    async def _evaluate_performance_actions(self, instance: StrategyInstance):
        """ì„±ê³¼ ê¸°ë°˜ ìë™ ì•¡ì…˜ í‰ê°€"""
        if not instance.performance:
            return
        
        config = instance.config
        performance = instance.performance
        
        # ë“œë¡œë‹¤ìš´ ì²´í¬
        current_drawdown = performance.get("max_drawdown", 0)
        if current_drawdown > config.auto_pause_on_drawdown:
            await self._pause_strategy(instance.id)
            
            await self.event_bus.publish({
                "type": "strategy_auto_paused",
                "strategy_id": instance.id,
                "reason": f"Drawdown exceeded {config.auto_pause_on_drawdown*100}%"
            })
        
        # ì„±ê³¼ ëª©í‘œ ë‹¬ì„± ì²´í¬
        for metric, target in config.performance_targets.items():
            current_value = performance.get(metric.value, 0)
            
            if current_value < target:
                # ì„±ê³¼ ë¯¸ë‹¬ì‹œ ë¦¬ì†ŒìŠ¤ ì¡°ì •
                await self._adjust_strategy_resources(instance.id, reduce=True)
    
    async def _pause_strategy(self, strategy_id: str):
        """ì „ëµ ì¼ì‹œì •ì§€"""
        if strategy_id in self.strategies:
            instance = self.strategies[strategy_id]
            instance.status = StrategyStatus.PAUSED
            
            if hasattr(instance.instance, 'pause'):
                await instance.instance.pause()
    
    async def _adjust_strategy_resources(self, strategy_id: str, reduce: bool = False):
        """ì „ëµ ë¦¬ì†ŒìŠ¤ ì¡°ì •"""
        # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ì™€ ì—°ë™í•˜ì—¬ CPU/ë©”ëª¨ë¦¬ í• ë‹¹ ì¡°ì •
        pass
    
    async def _lifecycle_monitor(self):
        """ì „ëµ ë¼ì´í”„ì‚¬ì´í´ ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.is_running:
            try:
                # ì „ëµ ìƒíƒœ í™•ì¸
                for strategy_id, instance in self.strategies.items():
                    
                    # ì˜¤ë¥˜ ìƒíƒœ ì „ëµ ì¬ì‹œì‘ ì‹œë„
                    if instance.status == StrategyStatus.ERROR and instance.error_count < 3:
                        await self._attempt_strategy_recovery(strategy_id)
                    
                    # ì¥ì‹œê°„ ë¹„í™œì„± ì „ëµ ì •ë¦¬
                    if (time.time() - instance.updated_at) > 3600:  # 1ì‹œê°„
                        await self._cleanup_inactive_strategy(strategy_id)
                
                await asyncio.sleep(30)  # 30ì´ˆë§ˆë‹¤ ì²´í¬
                
            except Exception as e:
                print(f"Lifecycle monitor error: {e}")
                await asyncio.sleep(60)
    
    async def _attempt_strategy_recovery(self, strategy_id: str):
        """ì „ëµ ë³µêµ¬ ì‹œë„"""
        instance = self.strategies[strategy_id]
        
        try:
            # ì „ëµ ì¸ìŠ¤í„´ìŠ¤ ì¬ìƒì„±
            config = instance.config
            module = importlib.import_module(config.module_path)
            strategy_class = getattr(module, config.class_name)
            
            new_instance = strategy_class(config.config)
            
            instance.instance = new_instance
            instance.status = StrategyStatus.ACTIVE
            instance.updated_at = time.time()
            
            await self.event_bus.publish({
                "type": "strategy_recovered",
                "strategy_id": strategy_id
            })
            
        except Exception as e:
            instance.error_count += 1
            instance.last_error = str(e)

class DynamicStrategyLoader:
    """ë™ì  ì „ëµ ë¡œë”"""
    
    def __init__(self, strategies_path: Path):
        self.strategies_path = strategies_path
        self.loaded_modules = {}
        
        # íŒŒì¼ ì‹œìŠ¤í…œ ì›Œì²˜ ì„¤ì •
        self.watcher = None
        
    async def hot_reload_strategy(self, strategy_name: str):
        """ì „ëµ í•« ë¦¬ë¡œë“œ"""
        module_name = f"user_data.strategies.{strategy_name}"
        
        if module_name in sys.modules:
            # ëª¨ë“ˆ ë¦¬ë¡œë“œ
            importlib.reload(sys.modules[module_name])
        else:
            # ìƒˆ ëª¨ë“ˆ ë¡œë“œ
            importlib.import_module(module_name)
        
        # ë¡œë“œëœ ëª¨ë“ˆ ì¶”ì 
        self.loaded_modules[strategy_name] = sys.modules[module_name]
        
        return True
    
    async def start_file_watcher(self):
        """íŒŒì¼ ë³€ê²½ ê°ì§€ ì‹œì‘"""
        # watchdog ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ íŒŒì¼ ë³€ê²½ ê°ì§€
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” íŒŒì¼ ë³€ê²½ì‹œ ìë™ ë¦¬ë¡œë“œ
        pass
```

### ğŸ”§ **ë™ì  ì „ëµ ë¡œë”©/ì–¸ë¡œë”©**

```python
# automation/dynamic_loading.py
import os
import sys
import hashlib
from pathlib import Path
from typing import Dict, Any, Optional
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import importlib.util

class StrategyFileHandler(FileSystemEventHandler):
    """ì „ëµ íŒŒì¼ ë³€ê²½ í•¸ë“¤ëŸ¬"""
    
    def __init__(self, strategy_manager):
        self.strategy_manager = strategy_manager
        self.file_hashes = {}
    
    def on_modified(self, event):
        """íŒŒì¼ ìˆ˜ì • ì‹œ í˜¸ì¶œ"""
        if not event.is_directory and event.src_path.endswith('.py'):
            self._handle_strategy_change(event.src_path)
    
    def _handle_strategy_change(self, file_path: str):
        """ì „ëµ íŒŒì¼ ë³€ê²½ ì²˜ë¦¬"""
        # íŒŒì¼ í•´ì‹œ í™•ì¸ (ì¤‘ë³µ ì´ë²¤íŠ¸ ë°©ì§€)
        with open(file_path, 'rb') as f:
            file_hash = hashlib.md5(f.read()).hexdigest()
        
        if file_path in self.file_hashes and self.file_hashes[file_path] == file_hash:
            return
        
        self.file_hashes[file_path] = file_hash
        
        # ì „ëµ ì´ë¦„ ì¶”ì¶œ
        strategy_name = Path(file_path).stem
        
        # ì „ëµ ë¦¬ë¡œë“œ
        asyncio.create_task(self.strategy_manager.reload_strategy(strategy_name))

class DynamicConfigManager:
    """ë™ì  ì„¤ì • ê´€ë¦¬ì"""
    
    def __init__(self):
        self.config_cache = {}
        self.config_watchers = {}
    
    async def load_config(self, config_path: str) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        if config_path in self.config_cache:
            return self.config_cache[config_path]
        
        with open(config_path, 'r') as f:
            if config_path.endswith('.json'):
                config = json.load(f)
            elif config_path.endswith('.yaml') or config_path.endswith('.yml'):
                config = yaml.safe_load(f)
            else:
                raise ValueError(f"Unsupported config format: {config_path}")
        
        self.config_cache[config_path] = config
        return config
    
    async def watch_config_changes(self, config_path: str, callback):
        """ì„¤ì • íŒŒì¼ ë³€ê²½ ê°ì§€"""
        observer = Observer()
        handler = ConfigChangeHandler(config_path, callback)
        
        observer.schedule(handler, Path(config_path).parent, recursive=False)
        observer.start()
        
        self.config_watchers[config_path] = observer

class ResourceIsolationManager:
    """ë¦¬ì†ŒìŠ¤ ê²©ë¦¬ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.strategy_resources = {}
        self.resource_limits = {
            'cpu_percent': 80,
            'memory_mb': 1024,
            'max_threads': 4
        }
    
    async def allocate_resources(self, strategy_id: str, requirements: Dict[str, Any]):
        """ì „ëµë³„ ë¦¬ì†ŒìŠ¤ í• ë‹¹"""
        
        # CPU ì œí•œ ì„¤ì •
        if 'cpu_limit' in requirements:
            await self._set_cpu_limit(strategy_id, requirements['cpu_limit'])
        
        # ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •
        if 'memory_limit' in requirements:
            await self._set_memory_limit(strategy_id, requirements['memory_limit'])
        
        # ìŠ¤ë ˆë“œ í’€ ì œí•œ
        if 'thread_limit' in requirements:
            await self._set_thread_limit(strategy_id, requirements['thread_limit'])
        
        self.strategy_resources[strategy_id] = requirements
    
    async def _set_cpu_limit(self, strategy_id: str, cpu_limit: float):
        """CPU ì‚¬ìš©ë¥  ì œí•œ"""
        # cgroups ë˜ëŠ” Docker ì œí•œ ì‚¬ìš©
        pass
    
    async def _set_memory_limit(self, strategy_id: str, memory_limit: int):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ"""
        # cgroups ë˜ëŠ” Docker ì œí•œ ì‚¬ìš©
        pass
    
    async def monitor_resource_usage(self, strategy_id: str) -> Dict[str, float]:
        """ì „ëµë³„ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
        # psutil ë“±ì„ ì‚¬ìš©í•œ ì‹¤ì œ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        return {
            'cpu_percent': 0.0,
            'memory_mb': 0.0,
            'thread_count': 0
        }
```

---

## ğŸ“Š **ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸**

### ğŸŒŠ **WebSocket ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°**

```python
# automation/data_pipeline.py
import asyncio
import websockets
import json
import aioredis
import aiokafka
from typing import Dict, List, Callable, Optional, Any
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from dataclasses import dataclass
import logging
from collections import deque
import time

@dataclass
class MarketData:
    """ì‹œì¥ ë°ì´í„° êµ¬ì¡°"""
    symbol: str
    timestamp: float
    price: float
    volume: float
    bid_price: float
    ask_price: float
    funding_rate: Optional[float] = None
    open_interest: Optional[float] = None

class WebSocketDataStreamer:
    """WebSocket ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë¨¸"""
    
    def __init__(self, exchange_urls: Dict[str, str], redis_client, kafka_producer):
        self.exchange_urls = exchange_urls
        self.redis_client = redis_client
        self.kafka_producer = kafka_producer
        self.connections = {}
        self.subscribed_symbols = set()
        self.data_handlers = []
        self.is_running = False
        
        # ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
        self.data_quality_metrics = {
            'messages_received': 0,
            'messages_processed': 0,
            'errors': 0,
            'latency_ms': deque(maxlen=1000)
        }
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    async def start_streaming(self, symbols: List[str]):
        """ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘"""
        self.subscribed_symbols.update(symbols)
        self.is_running = True
        
        # ê° ê±°ë˜ì†Œë³„ ì—°ê²°
        tasks = []
        for exchange, url in self.exchange_urls.items():
            task = asyncio.create_task(self._connect_to_exchange(exchange, url, symbols))
            tasks.append(task)
        
        # ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬
        tasks.append(asyncio.create_task(self._monitor_data_quality()))
        
        await asyncio.gather(*tasks)
    
    async def _connect_to_exchange(self, exchange: str, url: str, symbols: List[str]):
        """ê±°ë˜ì†Œ ì—°ê²°"""
        retry_count = 0
        max_retries = 5
        
        while self.is_running and retry_count < max_retries:
            try:
                async with websockets.connect(
                    url,
                    ping_interval=20,
                    ping_timeout=10,
                    close_timeout=10
                ) as websocket:
                    
                    self.connections[exchange] = websocket
                    self.logger.info(f"Connected to {exchange}")
                    
                    # êµ¬ë… ë©”ì‹œì§€ ì „ì†¡
                    await self._subscribe_to_symbols(websocket, exchange, symbols)
                    
                    # ë©”ì‹œì§€ ìˆ˜ì‹  ë£¨í”„
                    async for message in websocket:
                        await self._process_message(exchange, message)
                        
            except Exception as e:
                retry_count += 1
                self.logger.error(f"Connection error for {exchange}: {e}")
                await asyncio.sleep(min(retry_count * 2, 30))  # ì§€ìˆ˜ ë°±ì˜¤í”„
        
        self.logger.error(f"Max retries exceeded for {exchange}")
    
    async def _subscribe_to_symbols(self, websocket, exchange: str, symbols: List[str]):
        """ì‹¬ë³¼ êµ¬ë…"""
        if exchange == "binance":
            # Binance êµ¬ë… í˜•ì‹
            streams = []
            for symbol in symbols:
                streams.extend([
                    f"{symbol.lower()}@ticker",
                    f"{symbol.lower()}@bookTicker",
                    f"{symbol.lower()}@kline_1m"
                ])
            
            subscribe_msg = {
                "method": "SUBSCRIBE",
                "params": streams,
                "id": 1
            }
            
            await websocket.send(json.dumps(subscribe_msg))
    
    async def _process_message(self, exchange: str, message: str):
        """ë©”ì‹œì§€ ì²˜ë¦¬"""
        start_time = time.time()
        
        try:
            data = json.loads(message)
            self.data_quality_metrics['messages_received'] += 1
            
            # ë°ì´í„° íŒŒì‹± ë° ì •ê·œí™”
            market_data = await self._parse_market_data(exchange, data)
            
            if market_data:
                # Redisì— ì‹¤ì‹œê°„ ë°ì´í„° ì €ì¥
                await self._store_realtime_data(market_data)
                
                # Kafkaë¡œ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°
                await self._stream_to_kafka(market_data)
                
                # ë“±ë¡ëœ í•¸ë“¤ëŸ¬ë“¤ì—ê²Œ ë°ì´í„° ì „ë‹¬
                for handler in self.data_handlers:
                    asyncio.create_task(handler(market_data))
                
                self.data_quality_metrics['messages_processed'] += 1
            
            # ë ˆì´í„´ì‹œ ì¸¡ì •
            latency = (time.time() - start_time) * 1000
            self.data_quality_metrics['latency_ms'].append(latency)
            
        except Exception as e:
            self.data_quality_metrics['errors'] += 1
            self.logger.error(f"Message processing error: {e}")
    
    async def _parse_market_data(self, exchange: str, data: Dict) -> Optional[MarketData]:
        """ì‹œì¥ ë°ì´í„° íŒŒì‹±"""
        try:
            if exchange == "binance":
                if 'stream' in data and data['stream'].endswith('@ticker'):
                    ticker_data = data['data']
                    return MarketData(
                        symbol=ticker_data['s'],
                        timestamp=float(ticker_data['E']) / 1000,
                        price=float(ticker_data['c']),
                        volume=float(ticker_data['v']),
                        bid_price=float(ticker_data['b']),
                        ask_price=float(ticker_data['a'])
                    )
            
            return None
            
        except KeyError as e:
            self.logger.warning(f"Missing field in market data: {e}")
            return None
    
    async def _store_realtime_data(self, market_data: MarketData):
        """ì‹¤ì‹œê°„ ë°ì´í„° ì €ì¥"""
        key = f"realtime:{market_data.symbol}"
        
        data_dict = {
            'price': market_data.price,
            'volume': market_data.volume,
            'bid': market_data.bid_price,
            'ask': market_data.ask_price,
            'timestamp': market_data.timestamp
        }
        
        # Redisì— ìµœì‹  ë°ì´í„° ì €ì¥ (TTL 1ì‹œê°„)
        await self.redis_client.setex(key, 3600, json.dumps(data_dict))
        
        # ì‹œê³„ì—´ ë°ì´í„°ë¡œë„ ì €ì¥ (ìµœê·¼ 1000ê°œ í‹±)
        list_key = f"ticks:{market_data.symbol}"
        await self.redis_client.lpush(list_key, json.dumps(data_dict))
        await self.redis_client.ltrim(list_key, 0, 999)
    
    async def _stream_to_kafka(self, market_data: MarketData):
        """Kafkaë¡œ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°"""
        topic = f"market-data-{market_data.symbol.lower()}"
        
        message = {
            'symbol': market_data.symbol,
            'timestamp': market_data.timestamp,
            'price': market_data.price,
            'volume': market_data.volume,
            'bid_price': market_data.bid_price,
            'ask_price': market_data.ask_price
        }
        
        await self.kafka_producer.send(topic, json.dumps(message).encode())
    
    async def _monitor_data_quality(self):
        """ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§"""
        while self.is_running:
            await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬
            
            metrics = self.data_quality_metrics
            
            # ì²˜ë¦¬ìœ¨ ê³„ì‚°
            processing_rate = metrics['messages_processed'] / max(metrics['messages_received'], 1)
            
            # í‰ê·  ë ˆì´í„´ì‹œ ê³„ì‚°
            avg_latency = np.mean(metrics['latency_ms']) if metrics['latency_ms'] else 0
            
            # í’ˆì§ˆ ì§€í‘œ ë¡œê¹…
            self.logger.info(f"Data Quality - Processing Rate: {processing_rate:.2%}, "
                           f"Avg Latency: {avg_latency:.2f}ms, "
                           f"Errors: {metrics['errors']}")
            
            # í’ˆì§ˆ ì´ìŠˆ ê°ì§€
            if processing_rate < 0.95:  # 95% ë¯¸ë§Œ
                self.logger.warning("Low processing rate detected!")
            
            if avg_latency > 100:  # 100ms ì´ˆê³¼
                self.logger.warning("High latency detected!")
    
    def add_data_handler(self, handler: Callable):
        """ë°ì´í„° í•¸ë“¤ëŸ¬ ì¶”ê°€"""
        self.data_handlers.append(handler)

class RealTimeDataProcessor:
    """ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ê¸°"""
    
    def __init__(self, redis_client):
        self.redis_client = redis_client
        self.technical_indicators = {}
        self.data_windows = {}  # ìœˆë„ìš° ë°ì´í„° ì €ì¥
        
    async def process_tick_data(self, market_data: MarketData):
        """í‹± ë°ì´í„° ì²˜ë¦¬"""
        symbol = market_data.symbol
        
        # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
        await self._update_technical_indicators(market_data)
        
        # ìœˆë„ìš° ë°ì´í„° ì—…ë°ì´íŠ¸
        await self._update_data_windows(market_data)
        
        # ì‹ í˜¸ ìƒì„± (í•„ìš”ì‹œ)
        signals = await self._generate_signals(market_data)
        
        if signals:
            await self._publish_signals(signals)
    
    async def _update_technical_indicators(self, market_data: MarketData):
        """ê¸°ìˆ ì  ì§€í‘œ ì—…ë°ì´íŠ¸"""
        symbol = market_data.symbol
        
        if symbol not in self.technical_indicators:
            self.technical_indicators[symbol] = {
                'prices': deque(maxlen=200),
                'volumes': deque(maxlen=200),
                'ema_12': None,
                'ema_26': None,
                'rsi': None,
                'macd': None
            }
        
        indicators = self.technical_indicators[symbol]
        indicators['prices'].append(market_data.price)
        indicators['volumes'].append(market_data.volume)
        
        # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ ì§€í‘œ ê³„ì‚°
        if len(indicators['prices']) >= 26:
            prices = np.array(indicators['prices'])
            
            # EMA ê³„ì‚°
            indicators['ema_12'] = self._calculate_ema(prices, 12)
            indicators['ema_26'] = self._calculate_ema(prices, 26)
            
            # RSI ê³„ì‚°
            if len(indicators['prices']) >= 14:
                indicators['rsi'] = self._calculate_rsi(prices, 14)
            
            # MACD ê³„ì‚°
            if indicators['ema_12'] is not None and indicators['ema_26'] is not None:
                indicators['macd'] = indicators['ema_12'] - indicators['ema_26']
        
        # Redisì— ì§€í‘œ ì €ì¥
        await self._store_indicators(symbol, indicators)
    
    def _calculate_ema(self, prices: np.ndarray, period: int) -> float:
        """EMA ê³„ì‚°"""
        alpha = 2 / (period + 1)
        ema = prices[0]
        
        for price in prices[1:]:
            ema = alpha * price + (1 - alpha) * ema
        
        return ema
    
    def _calculate_rsi(self, prices: np.ndarray, period: int) -> float:
        """RSI ê³„ì‚°"""
        deltas = np.diff(prices)
        gains = np.where(deltas > 0, deltas, 0)
        losses = np.where(deltas < 0, -deltas, 0)
        
        avg_gain = np.mean(gains[-period:])
        avg_loss = np.mean(losses[-period:])
        
        if avg_loss == 0:
            return 100
        
        rs = avg_gain / avg_loss
        rsi = 100 - (100 / (1 + rs))
        
        return rsi
    
    async def _store_indicators(self, symbol: str, indicators: Dict):
        """ì§€í‘œë¥¼ Redisì— ì €ì¥"""
        key = f"indicators:{symbol}"
        
        data = {
            'ema_12': indicators.get('ema_12'),
            'ema_26': indicators.get('ema_26'),
            'rsi': indicators.get('rsi'),
            'macd': indicators.get('macd'),
            'timestamp': time.time()
        }
        
        # None ê°’ ì œê±°
        data = {k: v for k, v in data.items() if v is not None}
        
        await self.redis_client.setex(key, 300, json.dumps(data))  # 5ë¶„ TTL

class DistributedCacheManager:
    """ë¶„ì‚° ìºì‹œ ê´€ë¦¬ì"""
    
    def __init__(self, redis_cluster_nodes: List[str]):
        self.redis_cluster_nodes = redis_cluster_nodes
        self.redis_cluster = None
        self.cache_stats = {
            'hits': 0,
            'misses': 0,
            'errors': 0
        }
    
    async def initialize(self):
        """ìºì‹œ í´ëŸ¬ìŠ¤í„° ì´ˆê¸°í™”"""
        self.redis_cluster = aioredis.RedisCluster(
            startup_nodes=[{"host": node.split(':')[0], "port": int(node.split(':')[1])} 
                          for node in self.redis_cluster_nodes],
            decode_responses=True,
            skip_full_coverage_check=True
        )
    
    async def get_market_data(self, symbol: str, timeframe: str = "1m") -> Optional[Dict]:
        """ì‹œì¥ ë°ì´í„° ì¡°íšŒ"""
        try:
            key = f"market:{symbol}:{timeframe}"
            data = await self.redis_cluster.get(key)
            
            if data:
                self.cache_stats['hits'] += 1
                return json.loads(data)
            else:
                self.cache_stats['misses'] += 1
                return None
                
        except Exception as e:
            self.cache_stats['errors'] += 1
            logging.error(f"Cache get error: {e}")
            return None
    
    async def set_market_data(self, symbol: str, timeframe: str, data: Dict, ttl: int = 300):
        """ì‹œì¥ ë°ì´í„° ì €ì¥"""
        try:
            key = f"market:{symbol}:{timeframe}"
            await self.redis_cluster.setex(key, ttl, json.dumps(data))
            
        except Exception as e:
            self.cache_stats['errors'] += 1
            logging.error(f"Cache set error: {e}")
    
    async def invalidate_cache(self, pattern: str):
        """ìºì‹œ ë¬´íš¨í™”"""
        try:
            keys = await self.redis_cluster.keys(pattern)
            if keys:
                await self.redis_cluster.delete(*keys)
                
        except Exception as e:
            logging.error(f"Cache invalidation error: {e}")
    
    def get_cache_stats(self) -> Dict[str, int]:
        """ìºì‹œ í†µê³„ ì¡°íšŒ"""
        total_requests = self.cache_stats['hits'] + self.cache_stats['misses']
        hit_rate = self.cache_stats['hits'] / max(total_requests, 1)
        
        return {
            **self.cache_stats,
            'hit_rate': hit_rate,
            'total_requests': total_requests
        }

class DataQualityMonitor:
    """ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°"""
    
    def __init__(self, alert_manager):
        self.alert_manager = alert_manager
        self.quality_metrics = {}
        self.thresholds = {
            'latency_ms': 100,
            'error_rate': 0.01,  # 1%
            'data_freshness_seconds': 60,
            'missing_data_rate': 0.05  # 5%
        }
    
    async def monitor_data_stream(self, symbol: str, data_source: str):
        """ë°ì´í„° ìŠ¤íŠ¸ë¦¼ ëª¨ë‹ˆí„°ë§"""
        while True:
            try:
                # ë°ì´í„° ì‹ ì„ ë„ ì²´í¬
                await self._check_data_freshness(symbol, data_source)
                
                # ëˆ„ë½ ë°ì´í„° ì²´í¬
                await self._check_missing_data(symbol, data_source)
                
                # ì´ìƒì¹˜ ê²€ì¶œ
                await self._detect_anomalies(symbol, data_source)
                
                await asyncio.sleep(30)  # 30ì´ˆë§ˆë‹¤ ì²´í¬
                
            except Exception as e:
                logging.error(f"Data quality monitoring error: {e}")
                await asyncio.sleep(60)
    
    async def _check_data_freshness(self, symbol: str, data_source: str):
        """ë°ì´í„° ì‹ ì„ ë„ ì²´í¬"""
        # ë§ˆì§€ë§‰ ë°ì´í„° íƒ€ì„ìŠ¤íƒ¬í”„ í™•ì¸
        # ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ë©´ ì•Œë¦¼
        pass
    
    async def _check_missing_data(self, symbol: str, data_source: str):
        """ëˆ„ë½ ë°ì´í„° ì²´í¬"""
        # ì˜ˆìƒ ë°ì´í„° í¬ì¸íŠ¸ ëŒ€ë¹„ ì‹¤ì œ ìˆ˜ì‹  ë°ì´í„° ë¹„êµ
        pass
    
    async def _detect_anomalies(self, symbol: str, data_source: str):
        """ì´ìƒì¹˜ ê²€ì¶œ"""
        # í†µê³„ì  ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ ê²€ì¶œ
        # ê°€ê²© ê¸‰ë“±ë½, ë¹„ì •ìƒì  ê±°ë˜ëŸ‰ ë“±
        pass

---

## ğŸ“¢ **í†µí•© ì•Œë¦¼ ì‹œìŠ¤í…œ**

### ğŸ“± **ë©€í‹°ì±„ë„ ì•Œë¦¼ (í…”ë ˆê·¸ë¨/ìŠ¬ë™/ì´ë©”ì¼/SMS)**

```python
# automation/notification_system.py
import asyncio
import aiohttp
import smtplib
from email.mime.text import MimeText
from email.mime.multipart import MimeMultipart
from typing import Dict, List, Optional, Any
from enum import Enum
from dataclasses import dataclass
import json
import logging
from datetime import datetime, timedelta

class NotificationChannel(Enum):
    """ì•Œë¦¼ ì±„ë„ íƒ€ì…"""
    TELEGRAM = "telegram"
    SLACK = "slack"
    EMAIL = "email"
    SMS = "sms"
    WEBHOOK = "webhook"
    DISCORD = "discord"

class NotificationPriority(Enum):
    """ì•Œë¦¼ ìš°ì„ ìˆœìœ„"""
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    CRITICAL = 4
    EMERGENCY = 5

@dataclass
class NotificationConfig:
    """ì•Œë¦¼ ì„¤ì •"""
    channel: NotificationChannel
    enabled: bool
    config: Dict[str, Any]
    priority_threshold: NotificationPriority = NotificationPriority.LOW
    rate_limit: Optional[int] = None  # ë¶„ë‹¹ ìµœëŒ€ ì•Œë¦¼ ìˆ˜
    quiet_hours: Optional[List[int]] = None  # ì•Œë¦¼ ì œí•œ ì‹œê°„ëŒ€

@dataclass
class Notification:
    """ì•Œë¦¼ ë©”ì‹œì§€"""
    title: str
    message: str
    priority: NotificationPriority
    tags: List[str] = None
    metadata: Dict[str, Any] = None
    channel_override: Optional[NotificationChannel] = None
    timestamp: datetime = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()

class NotificationTemplate:
    """ì•Œë¦¼ í…œí”Œë¦¿"""
    
    def __init__(self):
        self.templates = {
            'trade_entry': {
                'title': 'ğŸ”µ í¬ì§€ì…˜ ì§„ì…',
                'message': '{side} {symbol} | ê°€ê²©: ${price} | ë ˆë²„ë¦¬ì§€: {leverage}x | í¬ê¸°: ${position_size}'
            },
            'trade_exit': {
                'title': 'ğŸ”´ í¬ì§€ì…˜ ì²­ì‚°',
                'message': '{side} {symbol} ì²­ì‚° | ê°€ê²©: ${price} | P&L: ${pnl} ({pnl_percent}%)'
            },
            'risk_alert': {
                'title': 'âš ï¸ ë¦¬ìŠ¤í¬ ê²½ê³ ',
                'message': '{alert_type}: {symbol} | í˜„ì¬ê°’: {current_value} | ì„ê³„ê°’: {threshold}'
            },
            'system_error': {
                'title': 'ğŸš¨ ì‹œìŠ¤í…œ ì˜¤ë¥˜',
                'message': 'ì˜¤ë¥˜ ë°œìƒ: {error_type} | ìƒì„¸: {error_details} | ì‹œê°„: {timestamp}'
            },
            'funding_rate': {
                'title': 'ğŸ’° ìê¸ˆ ì¡°ë‹¬ë£Œ',
                'message': '{symbol} ìê¸ˆ ì¡°ë‹¬ë£Œ: {rate}% | ë‹¤ìŒ ì •ì‚°: {next_funding}'
            },
            'liquidation_warning': {
                'title': 'ğŸš¨ ì²­ì‚° ìœ„í—˜',
                'message': '{symbol} ì²­ì‚° ìœ„í—˜! | í˜„ì¬ê°€: ${current_price} | ì²­ì‚°ê°€: ${liquidation_price} | ê±°ë¦¬: {distance}%'
            }
        }
    
    def format_notification(self, template_name: str, **kwargs) -> Notification:
        """í…œí”Œë¦¿ì„ ì‚¬ìš©í•œ ì•Œë¦¼ ìƒì„±"""
        template = self.templates.get(template_name)
        if not template:
            raise ValueError(f"Template not found: {template_name}")
        
        title = template['title'].format(**kwargs)
        message = template['message'].format(**kwargs)
        
        # ìš°ì„ ìˆœìœ„ ìë™ ê²°ì •
        priority = self._determine_priority(template_name, kwargs)
        
        return Notification(
            title=title,
            message=message,
            priority=priority,
            tags=[template_name],
            metadata=kwargs
        )
    
    def _determine_priority(self, template_name: str, kwargs: Dict) -> NotificationPriority:
        """í…œí”Œë¦¿ê³¼ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìš°ì„ ìˆœìœ„ ê²°ì •"""
        if template_name == 'liquidation_warning':
            return NotificationPriority.EMERGENCY
        elif template_name == 'system_error':
            return NotificationPriority.CRITICAL
        elif template_name == 'risk_alert':
            return NotificationPriority.HIGH
        elif template_name in ['trade_entry', 'trade_exit']:
            # ì†ì‹¤ í¬ê¸°ì— ë”°ë¼ ìš°ì„ ìˆœìœ„ ê²°ì •
            pnl_percent = kwargs.get('pnl_percent', 0)
            if pnl_percent < -10:
                return NotificationPriority.HIGH
            elif abs(pnl_percent) > 5:
                return NotificationPriority.MEDIUM
            else:
                return NotificationPriority.LOW
        else:
            return NotificationPriority.LOW

class TelegramNotifier:
    """í…”ë ˆê·¸ë¨ ì•Œë¦¼"""
    
    def __init__(self, bot_token: str, chat_ids: List[str]):
        self.bot_token = bot_token
        self.chat_ids = chat_ids
        self.api_url = f"https://api.telegram.org/bot{bot_token}"
        
    async def send_notification(self, notification: Notification) -> bool:
        """í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡"""
        try:
            # ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ì´ëª¨ì§€ ì¶”ê°€
            priority_emojis = {
                NotificationPriority.LOW: "â„¹ï¸",
                NotificationPriority.MEDIUM: "âš ï¸",
                NotificationPriority.HIGH: "ğŸ”¥",
                NotificationPriority.CRITICAL: "ğŸš¨",
                NotificationPriority.EMERGENCY: "ğŸ†˜"
            }
            
            emoji = priority_emojis.get(notification.priority, "ğŸ“¢")
            message = f"{emoji} {notification.title}\n\n{notification.message}"
            
            # ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ…
            if notification.priority >= NotificationPriority.HIGH:
                message = f"*{message}*"
            
            async with aiohttp.ClientSession() as session:
                success_count = 0
                
                for chat_id in self.chat_ids:
                    payload = {
                        'chat_id': chat_id,
                        'text': message,
                        'parse_mode': 'Markdown'
                    }
                    
                    async with session.post(f"{self.api_url}/sendMessage", json=payload) as resp:
                        if resp.status == 200:
                            success_count += 1
                        else:
                            logging.error(f"Telegram send failed: {await resp.text()}")
                
                return success_count > 0
                
        except Exception as e:
            logging.error(f"Telegram notification error: {e}")
            return False

class SlackNotifier:
    """ìŠ¬ë™ ì•Œë¦¼"""
    
    def __init__(self, webhook_url: str, channels: List[str]):
        self.webhook_url = webhook_url
        self.channels = channels
        
    async def send_notification(self, notification: Notification) -> bool:
        """ìŠ¬ë™ ì•Œë¦¼ ì „ì†¡"""
        try:
            # ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ìƒ‰ìƒ ì„¤ì •
            colors = {
                NotificationPriority.LOW: "#36a64f",      # ë…¹ìƒ‰
                NotificationPriority.MEDIUM: "#ff9800",   # ì£¼í™©ìƒ‰
                NotificationPriority.HIGH: "#f44336",     # ë¹¨ê°„ìƒ‰
                NotificationPriority.CRITICAL: "#9c27b0", # ë³´ë¼ìƒ‰
                NotificationPriority.EMERGENCY: "#000000" # ê²€ì€ìƒ‰
            }
            
            color = colors.get(notification.priority, "#36a64f")
            
            payload = {
                "attachments": [
                    {
                        "color": color,
                        "title": notification.title,
                        "text": notification.message,
                        "footer": "Freqtrade Automation",
                        "ts": int(notification.timestamp.timestamp())
                    }
                ]
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(self.webhook_url, json=payload) as resp:
                    return resp.status == 200
                    
        except Exception as e:
            logging.error(f"Slack notification error: {e}")
            return False

class EmailNotifier:
    """ì´ë©”ì¼ ì•Œë¦¼"""
    
    def __init__(self, smtp_config: Dict[str, Any], recipients: List[str]):
        self.smtp_config = smtp_config
        self.recipients = recipients
        
    async def send_notification(self, notification: Notification) -> bool:
        """ì´ë©”ì¼ ì•Œë¦¼ ì „ì†¡"""
        try:
            # HTML í…œí”Œë¦¿ ìƒì„±
            html_content = self._create_html_template(notification)
            
            msg = MimeMultipart('alternative')
            msg['Subject'] = notification.title
            msg['From'] = self.smtp_config['username']
            msg['To'] = ', '.join(self.recipients)
            
            # HTML íŒŒíŠ¸ ì¶”ê°€
            html_part = MimeText(html_content, 'html')
            msg.attach(html_part)
            
            # SMTP ì„œë²„ ì—°ê²° ë° ì „ì†¡
            with smtplib.SMTP(self.smtp_config['host'], self.smtp_config['port']) as server:
                server.starttls()
                server.login(self.smtp_config['username'], self.smtp_config['password'])
                server.send_message(msg)
                
            return True
            
        except Exception as e:
            logging.error(f"Email notification error: {e}")
            return False
    
    def _create_html_template(self, notification: Notification) -> str:
        """HTML ì´ë©”ì¼ í…œí”Œë¦¿ ìƒì„±"""
        priority_colors = {
            NotificationPriority.LOW: "#4CAF50",
            NotificationPriority.MEDIUM: "#FF9800",
            NotificationPriority.HIGH: "#F44336",
            NotificationPriority.CRITICAL: "#9C27B0",
            NotificationPriority.EMERGENCY: "#000000"
        }
        
        color = priority_colors.get(notification.priority, "#4CAF50")
        
        return f"""
        <html>
        <body style="font-family: Arial, sans-serif; margin: 0; padding: 20px;">
            <div style="max-width: 600px; margin: 0 auto;">
                <div style="background-color: {color}; color: white; padding: 20px; border-radius: 5px 5px 0 0;">
                    <h2 style="margin: 0;">{notification.title}</h2>
                </div>
                <div style="background-color: #f5f5f5; padding: 20px; border-radius: 0 0 5px 5px;">
                    <p style="font-size: 16px; line-height: 1.6;">{notification.message}</p>
                    <hr style="border: none; border-top: 1px solid #ddd; margin: 20px 0;">
                    <p style="font-size: 12px; color: #666;">
                        ì‹œê°„: {notification.timestamp.strftime('%Y-%m-%d %H:%M:%S')}<br>
                        ìš°ì„ ìˆœìœ„: {notification.priority.name}
                    </p>
                </div>
            </div>
        </body>
        </html>
        """

class NotificationRouter:
    """ì•Œë¦¼ ë¼ìš°íŒ… ë° í•„í„°ë§"""
    
    def __init__(self):
        self.channels = {}
        self.routing_rules = []
        self.rate_limiters = {}
        
    def add_channel(self, name: str, notifier, config: NotificationConfig):
        """ì•Œë¦¼ ì±„ë„ ì¶”ê°€"""
        self.channels[name] = {
            'notifier': notifier,
            'config': config
        }
        
        # ë ˆì´íŠ¸ ë¦¬ë¯¸í„° ì´ˆê¸°í™”
        if config.rate_limit:
            self.rate_limiters[name] = {
                'count': 0,
                'reset_time': datetime.now() + timedelta(minutes=1)
            }
    
    def add_routing_rule(self, condition: Callable, channels: List[str]):
        """ë¼ìš°íŒ… ê·œì¹™ ì¶”ê°€"""
        self.routing_rules.append({
            'condition': condition,
            'channels': channels
        })
    
    async def route_notification(self, notification: Notification) -> Dict[str, bool]:
        """ì•Œë¦¼ ë¼ìš°íŒ…"""
        results = {}
        
        # ì ìš©í•  ì±„ë„ ê²°ì •
        target_channels = self._determine_channels(notification)
        
        # ê° ì±„ë„ë¡œ ì•Œë¦¼ ì „ì†¡
        for channel_name in target_channels:
            if channel_name not in self.channels:
                continue
                
            channel_info = self.channels[channel_name]
            config = channel_info['config']
            
            # ì±„ë„ í™œì„±í™” ì²´í¬
            if not config.enabled:
                continue
            
            # ìš°ì„ ìˆœìœ„ ì²´í¬
            if notification.priority.value < config.priority_threshold.value:
                continue
            
            # ì¡°ìš©í•œ ì‹œê°„ ì²´í¬
            if self._is_quiet_hours(config):
                continue
            
            # ë ˆì´íŠ¸ ë¦¬ë¯¸íŠ¸ ì²´í¬
            if not self._check_rate_limit(channel_name, config):
                continue
            
            # ì•Œë¦¼ ì „ì†¡
            try:
                success = await channel_info['notifier'].send_notification(notification)
                results[channel_name] = success
                
                if success and config.rate_limit:
                    self._update_rate_limit(channel_name)
                    
            except Exception as e:
                logging.error(f"Notification routing error for {channel_name}: {e}")
                results[channel_name] = False
        
        return results
    
    def _determine_channels(self, notification: Notification) -> List[str]:
        """ì•Œë¦¼ì— ì ìš©í•  ì±„ë„ ê²°ì •"""
        if notification.channel_override:
            return [ch for ch, info in self.channels.items() 
                   if info['config'].channel == notification.channel_override]
        
        # ë¼ìš°íŒ… ê·œì¹™ ì ìš©
        for rule in self.routing_rules:
            if rule['condition'](notification):
                return rule['channels']
        
        # ê¸°ë³¸: ëª¨ë“  ì±„ë„
        return list(self.channels.keys())
    
    def _is_quiet_hours(self, config: NotificationConfig) -> bool:
        """ì¡°ìš©í•œ ì‹œê°„ ì²´í¬"""
        if not config.quiet_hours:
            return False
        
        current_hour = datetime.now().hour
        return current_hour in config.quiet_hours
    
    def _check_rate_limit(self, channel_name: str, config: NotificationConfig) -> bool:
        """ë ˆì´íŠ¸ ë¦¬ë¯¸íŠ¸ ì²´í¬"""
        if not config.rate_limit or channel_name not in self.rate_limiters:
            return True
        
        limiter = self.rate_limiters[channel_name]
        
        # ë¦¬ì…‹ ì‹œê°„ ì²´í¬
        if datetime.now() > limiter['reset_time']:
            limiter['count'] = 0
            limiter['reset_time'] = datetime.now() + timedelta(minutes=1)
        
        return limiter['count'] < config.rate_limit
    
    def _update_rate_limit(self, channel_name: str):
        """ë ˆì´íŠ¸ ë¦¬ë¯¸íŠ¸ ì—…ë°ì´íŠ¸"""
        if channel_name in self.rate_limiters:
            self.rate_limiters[channel_name]['count'] += 1

class EscalationManager:
    """ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì •ì±… ê´€ë¦¬"""
    
    def __init__(self, notification_router: NotificationRouter):
        self.notification_router = notification_router
        self.escalation_policies = {}
        self.pending_escalations = {}
        
    def add_escalation_policy(self, name: str, policy: Dict[str, Any]):
        """ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì •ì±… ì¶”ê°€"""
        self.escalation_policies[name] = policy
    
    async def handle_notification(self, notification: Notification, policy_name: str):
        """ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì •ì±…ì— ë”°ë¥¸ ì•Œë¦¼ ì²˜ë¦¬"""
        if policy_name not in self.escalation_policies:
            # ê¸°ë³¸ ë¼ìš°íŒ…
            return await self.notification_router.route_notification(notification)
        
        policy = self.escalation_policies[policy_name]
        
        # 1ì°¨ ì•Œë¦¼ ì „ì†¡
        primary_result = await self._send_primary_notification(notification, policy)
        
        # ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš” ì—¬ë¶€ í™•ì¸
        if self._should_escalate(notification, primary_result, policy):
            await self._schedule_escalation(notification, policy)
        
        return primary_result
    
    async def _send_primary_notification(self, notification: Notification, policy: Dict) -> Dict[str, bool]:
        """1ì°¨ ì•Œë¦¼ ì „ì†¡"""
        # ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ì±„ë„ ì„ íƒ
        primary_channels = policy.get('primary_channels', {})
        channels = primary_channels.get(notification.priority.name.lower(), [])
        
        if channels:
            # íŠ¹ì • ì±„ë„ë¡œë§Œ ì „ì†¡
            original_override = notification.channel_override
            results = {}
            
            for channel in channels:
                notification.channel_override = NotificationChannel(channel)
                result = await self.notification_router.route_notification(notification)
                results.update(result)
            
            notification.channel_override = original_override
            return results
        else:
            # ê¸°ë³¸ ë¼ìš°íŒ…
            return await self.notification_router.route_notification(notification)
    
    def _should_escalate(self, notification: Notification, result: Dict[str, bool], policy: Dict) -> bool:
        """ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš” ì—¬ë¶€ íŒë‹¨"""
        # ê¸´ê¸‰ ìƒí™©ì€ í•­ìƒ ì—ìŠ¤ì»¬ë ˆì´ì…˜
        if notification.priority == NotificationPriority.EMERGENCY:
            return True
        
        # 1ì°¨ ì•Œë¦¼ ì‹¤íŒ¨ì‹œ ì—ìŠ¤ì»¬ë ˆì´ì…˜
        if not any(result.values()):
            return True
        
        # ì •ì±…ë³„ ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì¡°ê±´ ì²´í¬
        escalation_conditions = policy.get('escalation_conditions', {})
        
        return any(condition(notification, result) for condition in escalation_conditions)
    
    async def _schedule_escalation(self, notification: Notification, policy: Dict):
        """ì—ìŠ¤ì»¬ë ˆì´ì…˜ ìŠ¤ì¼€ì¤„ë§"""
        escalation_delay = policy.get('escalation_delay_minutes', 5)
        
        # ì—ìŠ¤ì»¬ë ˆì´ì…˜ ëŒ€ê¸° ëª©ë¡ì— ì¶”ê°€
        escalation_id = f"{notification.timestamp.isoformat()}_{hash(notification.message)}"
        self.pending_escalations[escalation_id] = {
            'notification': notification,
            'policy': policy,
            'escalation_time': datetime.now() + timedelta(minutes=escalation_delay)
        }
        
        # ì§€ì—° í›„ ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì‹¤í–‰
        await asyncio.sleep(escalation_delay * 60)
        await self._execute_escalation(escalation_id)
    
    async def _execute_escalation(self, escalation_id: str):
        """ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì‹¤í–‰"""
        if escalation_id not in self.pending_escalations:
            return
        
        escalation_info = self.pending_escalations[escalation_id]
        notification = escalation_info['notification']
        policy = escalation_info['policy']
        
        # ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì•Œë¦¼ ìƒì„±
        escalated_notification = Notification(
            title=f"ğŸš¨ ESCALATED: {notification.title}",
            message=f"ì›ë³¸ ì•Œë¦¼ì´ ì²˜ë¦¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\n{notification.message}",
            priority=NotificationPriority.CRITICAL,
            tags=['escalated'] + (notification.tags or [])
        )
        
        # ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì±„ë„ë¡œ ì „ì†¡
        escalation_channels = policy.get('escalation_channels', [])
        for channel in escalation_channels:
            escalated_notification.channel_override = NotificationChannel(channel)
            await self.notification_router.route_notification(escalated_notification)
        
        # ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì™„ë£Œ í›„ ì œê±°
        del self.pending_escalations[escalation_id]

# ì‚¬ìš© ì˜ˆì‹œ
async def setup_notification_system():
    """ì•Œë¦¼ ì‹œìŠ¤í…œ ì„¤ì • ì˜ˆì‹œ"""
    
    # ì•Œë¦¼ ë¼ìš°í„° ìƒì„±
    router = NotificationRouter()
    
    # í…”ë ˆê·¸ë¨ ì±„ë„ ì„¤ì •
    telegram_notifier = TelegramNotifier(
        bot_token="YOUR_BOT_TOKEN",
        chat_ids=["CHAT_ID_1", "CHAT_ID_2"]
    )
    
    telegram_config = NotificationConfig(
        channel=NotificationChannel.TELEGRAM,
        enabled=True,
        config={},
        priority_threshold=NotificationPriority.LOW,
        rate_limit=10  # ë¶„ë‹¹ 10ê°œ
    )
    
    router.add_channel("telegram_main", telegram_notifier, telegram_config)
    
    # ìŠ¬ë™ ì±„ë„ ì„¤ì •
    slack_notifier = SlackNotifier(
        webhook_url="SLACK_WEBHOOK_URL",
        channels=["#trading-alerts"]
    )
    
    slack_config = NotificationConfig(
        channel=NotificationChannel.SLACK,
        enabled=True,
        config={},
        priority_threshold=NotificationPriority.MEDIUM,
        quiet_hours=[0, 1, 2, 3, 4, 5, 6]  # ìƒˆë²½ ì‹œê°„ ì œí•œ
    )
    
    router.add_channel("slack_main", slack_notifier, slack_config)
    
    # ë¼ìš°íŒ… ê·œì¹™ ì„¤ì •
    router.add_routing_rule(
        condition=lambda n: n.priority >= NotificationPriority.CRITICAL,
        channels=["telegram_main", "slack_main"]  # ì¤‘ìš” ì•Œë¦¼ì€ ëª¨ë“  ì±„ë„
    )
    
    router.add_routing_rule(
        condition=lambda n: 'trade' in (n.tags or []),
        channels=["telegram_main"]  # ê±°ë˜ ì•Œë¦¼ì€ í…”ë ˆê·¸ë¨ë§Œ
    )
    
    # ì—ìŠ¤ì»¬ë ˆì´ì…˜ ë§¤ë‹ˆì € ì„¤ì •
    escalation_manager = EscalationManager(router)
    
    escalation_policy = {
        'primary_channels': {
            'emergency': ['telegram'],
            'critical': ['telegram', 'slack'],
            'high': ['telegram']
        },
        'escalation_delay_minutes': 5,
        'escalation_channels': ['email', 'sms'],
        'escalation_conditions': [
            lambda n, r: not any(r.values()),  # ëª¨ë“  ì•Œë¦¼ ì‹¤íŒ¨
            lambda n, r: n.priority == NotificationPriority.EMERGENCY  # ê¸´ê¸‰ ìƒí™©
        ]
    }
    
    escalation_manager.add_escalation_policy("default", escalation_policy)
    
    return router, escalation_manager

---

## ğŸ”„ **ìë™ ì¥ì•  ë³µêµ¬ ì‹œìŠ¤í…œ**

### ğŸ¥ **í—¬ìŠ¤ì²´í¬ ë° ìë™ ì¬ì‹œì‘**

```python
# automation/health_monitoring.py
import asyncio
import aiohttp
import psutil
import docker
from typing import Dict, List, Optional, Callable, Any
from enum import Enum
from dataclasses import dataclass
import logging
import time
from datetime import datetime, timedelta

class HealthStatus(Enum):
    """í—¬ìŠ¤ ìƒíƒœ"""
    HEALTHY = "healthy"
    WARNING = "warning"
    UNHEALTHY = "unhealthy"
    CRITICAL = "critical"
    UNKNOWN = "unknown"

class ServiceType(Enum):
    """ì„œë¹„ìŠ¤ íƒ€ì…"""
    FREQTRADE = "freqtrade"
    DATABASE = "database"
    REDIS = "redis"
    MESSAGE_QUEUE = "message_queue"
    API_GATEWAY = "api_gateway"
    MONITORING = "monitoring"

@dataclass
class HealthCheck:
    """í—¬ìŠ¤ì²´í¬ ì„¤ì •"""
    name: str
    service_type: ServiceType
    check_function: Callable
    check_interval: int = 30  # ì´ˆ
    timeout: int = 10
    failure_threshold: int = 3
    success_threshold: int = 2
    recovery_action: Optional[Callable] = None

@dataclass
class HealthResult:
    """í—¬ìŠ¤ì²´í¬ ê²°ê³¼"""
    name: str
    status: HealthStatus
    message: str
    response_time: float
    timestamp: datetime
    metadata: Dict[str, Any] = None

class HealthMonitor:
    """í—¬ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self, notification_system, recovery_manager):
        self.notification_system = notification_system
        self.recovery_manager = recovery_manager
        self.health_checks: Dict[str, HealthCheck] = {}
        self.health_states: Dict[str, Dict] = {}
        self.is_running = False
        
        # Docker í´ë¼ì´ì–¸íŠ¸
        self.docker_client = docker.from_env()
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def register_health_check(self, health_check: HealthCheck):
        """í—¬ìŠ¤ì²´í¬ ë“±ë¡"""
        self.health_checks[health_check.name] = health_check
        self.health_states[health_check.name] = {
            'current_status': HealthStatus.UNKNOWN,
            'consecutive_failures': 0,
            'consecutive_successes': 0,
            'last_check': None,
            'last_recovery_attempt': None
        }
    
    async def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.is_running = True
        
        # ê° í—¬ìŠ¤ì²´í¬ì— ëŒ€í•œ íƒœìŠ¤í¬ ìƒì„±
        tasks = []
        for name, health_check in self.health_checks.items():
            task = asyncio.create_task(self._monitor_service(name, health_check))
            tasks.append(task)
        
        await asyncio.gather(*tasks)
    
    async def _monitor_service(self, name: str, health_check: HealthCheck):
        """ì„œë¹„ìŠ¤ ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.is_running:
            try:
                # í—¬ìŠ¤ì²´í¬ ì‹¤í–‰
                result = await self._execute_health_check(health_check)
                
                # ìƒíƒœ ì—…ë°ì´íŠ¸
                await self._update_health_state(name, result, health_check)
                
                # ì§€ì •ëœ ê°„ê²© ëŒ€ê¸°
                await asyncio.sleep(health_check.check_interval)
                
            except Exception as e:
                self.logger.error(f"Health check error for {name}: {e}")
                await asyncio.sleep(health_check.check_interval)
    
    async def _execute_health_check(self, health_check: HealthCheck) -> HealthResult:
        """í—¬ìŠ¤ì²´í¬ ì‹¤í–‰"""
        start_time = time.time()
        
        try:
            # íƒ€ì„ì•„ì›ƒê³¼ í•¨ê»˜ í—¬ìŠ¤ì²´í¬ í•¨ìˆ˜ ì‹¤í–‰
            result = await asyncio.wait_for(
                health_check.check_function(),
                timeout=health_check.timeout
            )
            
            response_time = (time.time() - start_time) * 1000  # ms
            
            return HealthResult(
                name=health_check.name,
                status=result.get('status', HealthStatus.HEALTHY),
                message=result.get('message', 'OK'),
                response_time=response_time,
                timestamp=datetime.now(),
                metadata=result.get('metadata', {})
            )
            
        except asyncio.TimeoutError:
            return HealthResult(
                name=health_check.name,
                status=HealthStatus.UNHEALTHY,
                message=f"Health check timeout ({health_check.timeout}s)",
                response_time=(time.time() - start_time) * 1000,
                timestamp=datetime.now()
            )
        except Exception as e:
            return HealthResult(
                name=health_check.name,
                status=HealthStatus.UNHEALTHY,
                message=f"Health check failed: {str(e)}",
                response_time=(time.time() - start_time) * 1000,
                timestamp=datetime.now()
            )
    
    async def _update_health_state(self, name: str, result: HealthResult, health_check: HealthCheck):
        """í—¬ìŠ¤ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        state = self.health_states[name]
        previous_status = state['current_status']
        
        # ì—°ì† ì‹¤íŒ¨/ì„±ê³µ ì¹´ìš´í„° ì—…ë°ì´íŠ¸
        if result.status in [HealthStatus.HEALTHY, HealthStatus.WARNING]:
            state['consecutive_failures'] = 0
            state['consecutive_successes'] += 1
        else:
            state['consecutive_successes'] = 0
            state['consecutive_failures'] += 1
        
        state['last_check'] = result.timestamp
        
        # ìƒíƒœ ë³€ê²½ ê°ì§€ ë° ì²˜ë¦¬
        new_status = self._determine_service_status(state, health_check, result.status)
        
        if new_status != previous_status:
            state['current_status'] = new_status
            await self._handle_status_change(name, previous_status, new_status, result, health_check)
    
    def _determine_service_status(self, state: Dict, health_check: HealthCheck, check_status: HealthStatus) -> HealthStatus:
        """ì„œë¹„ìŠ¤ ìƒíƒœ ê²°ì •"""
        # ì—°ì† ì‹¤íŒ¨ê°€ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ë©´ UNHEALTHY
        if state['consecutive_failures'] >= health_check.failure_threshold:
            return HealthStatus.UNHEALTHY
        
        # ì—°ì† ì„±ê³µì´ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ë©´ HEALTHY
        if state['consecutive_successes'] >= health_check.success_threshold:
            return HealthStatus.HEALTHY
        
        # í˜„ì¬ ì²´í¬ ê²°ê³¼ ë°˜í™˜
        return check_status
    
    async def _handle_status_change(self, name: str, old_status: HealthStatus, new_status: HealthStatus, 
                                  result: HealthResult, health_check: HealthCheck):
        """ìƒíƒœ ë³€ê²½ ì²˜ë¦¬"""
        self.logger.info(f"Service {name} status changed: {old_status.value} -> {new_status.value}")
        
        # ì•Œë¦¼ ì „ì†¡
        await self._send_health_notification(name, old_status, new_status, result)
        
        # ìë™ ë³µêµ¬ ì‹œë„
        if new_status == HealthStatus.UNHEALTHY and health_check.recovery_action:
            await self._attempt_recovery(name, health_check, result)
    
    async def _send_health_notification(self, name: str, old_status: HealthStatus, 
                                      new_status: HealthStatus, result: HealthResult):
        """í—¬ìŠ¤ ìƒíƒœ ì•Œë¦¼ ì „ì†¡"""
        if new_status == HealthStatus.UNHEALTHY:
            priority = NotificationPriority.CRITICAL
            title = f"ğŸš¨ ì„œë¹„ìŠ¤ ì¥ì•  ê°ì§€: {name}"
        elif new_status == HealthStatus.HEALTHY and old_status == HealthStatus.UNHEALTHY:
            priority = NotificationPriority.MEDIUM
            title = f"âœ… ì„œë¹„ìŠ¤ ë³µêµ¬ë¨: {name}"
        else:
            priority = NotificationPriority.LOW
            title = f"âš ï¸ ì„œë¹„ìŠ¤ ìƒíƒœ ë³€ê²½: {name}"
        
        message = f"ìƒíƒœ: {old_status.value} â†’ {new_status.value}\n"
        message += f"ë©”ì‹œì§€: {result.message}\n"
        message += f"ì‘ë‹µì‹œê°„: {result.response_time:.2f}ms\n"
        message += f"ì‹œê°„: {result.timestamp.strftime('%Y-%m-%d %H:%M:%S')}"
        
        notification = Notification(
            title=title,
            message=message,
            priority=priority,
            tags=['health_check', name],
            metadata={'service': name, 'status': new_status.value}
        )
        
        await self.notification_system.route_notification(notification)
    
    async def _attempt_recovery(self, name: str, health_check: HealthCheck, result: HealthResult):
        """ìë™ ë³µêµ¬ ì‹œë„"""
        state = self.health_states[name]
        
        # ìµœê·¼ ë³µêµ¬ ì‹œë„ ì‹œê°„ ì²´í¬ (ë„ˆë¬´ ìì£¼ ì‹œë„í•˜ì§€ ì•Šë„ë¡)
        if state['last_recovery_attempt']:
            time_since_last_attempt = datetime.now() - state['last_recovery_attempt']
            if time_since_last_attempt < timedelta(minutes=5):
                self.logger.info(f"Skipping recovery for {name} - too soon since last attempt")
                return
        
        state['last_recovery_attempt'] = datetime.now()
        
        try:
            self.logger.info(f"Attempting recovery for service {name}")
            
            # ë³µêµ¬ ì•¡ì…˜ ì‹¤í–‰
            recovery_result = await health_check.recovery_action(name, result)
            
            # ë³µêµ¬ ê²°ê³¼ ì•Œë¦¼
            if recovery_result:
                message = f"ì„œë¹„ìŠ¤ {name} ìë™ ë³µêµ¬ ì‹œë„ ì„±ê³µ"
                priority = NotificationPriority.MEDIUM
            else:
                message = f"ì„œë¹„ìŠ¤ {name} ìë™ ë³µêµ¬ ì‹œë„ ì‹¤íŒ¨"
                priority = NotificationPriority.HIGH
            
            notification = Notification(
                title="ğŸ”§ ìë™ ë³µêµ¬ ì‹œë„",
                message=message,
                priority=priority,
                tags=['auto_recovery', name]
            )
            
            await self.notification_system.route_notification(notification)
            
        except Exception as e:
            self.logger.error(f"Recovery attempt failed for {name}: {e}")
            
            notification = Notification(
                title="âŒ ìë™ ë³µêµ¬ ì‹¤íŒ¨",
                message=f"ì„œë¹„ìŠ¤ {name} ìë™ ë³µêµ¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                priority=NotificationPriority.CRITICAL,
                tags=['auto_recovery_failed', name]
            )
            
            await self.notification_system.route_notification(notification)

# êµ¬ì²´ì ì¸ í—¬ìŠ¤ì²´í¬ í•¨ìˆ˜ë“¤
class HealthChecks:
    """í—¬ìŠ¤ì²´í¬ í•¨ìˆ˜ ëª¨ìŒ"""
    
    @staticmethod
    async def check_freqtrade_api(api_url: str) -> Dict[str, Any]:
        """Freqtrade API í—¬ìŠ¤ì²´í¬"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{api_url}/api/v1/ping", timeout=5) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        return {
                            'status': HealthStatus.HEALTHY,
                            'message': 'API responding normally',
                            'metadata': data
                        }
                    else:
                        return {
                            'status': HealthStatus.UNHEALTHY,
                            'message': f'API returned status {resp.status}'
                        }
        except Exception as e:
            return {
                'status': HealthStatus.UNHEALTHY,
                'message': f'API connection failed: {str(e)}'
            }
    
    @staticmethod
    async def check_database_connection(db_url: str) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í—¬ìŠ¤ì²´í¬"""
        try:
            import asyncpg
            conn = await asyncpg.connect(db_url)
            
            # ê°„ë‹¨í•œ ì¿¼ë¦¬ ì‹¤í–‰
            result = await conn.fetchval('SELECT 1')
            await conn.close()
            
            if result == 1:
                return {
                    'status': HealthStatus.HEALTHY,
                    'message': 'Database connection OK'
                }
            else:
                return {
                    'status': HealthStatus.UNHEALTHY,
                    'message': 'Database query failed'
                }
                
        except Exception as e:
            return {
                'status': HealthStatus.UNHEALTHY,
                'message': f'Database connection failed: {str(e)}'
            }
    
    @staticmethod
    async def check_redis_connection(redis_url: str) -> Dict[str, Any]:
        """Redis ì—°ê²° í—¬ìŠ¤ì²´í¬"""
        try:
            import aioredis
            redis = aioredis.from_url(redis_url)
            
            # PING ëª…ë ¹ ì‹¤í–‰
            pong = await redis.ping()
            await redis.close()
            
            if pong:
                return {
                    'status': HealthStatus.HEALTHY,
                    'message': 'Redis connection OK'
                }
            else:
                return {
                    'status': HealthStatus.UNHEALTHY,
                    'message': 'Redis ping failed'
                }
                
        except Exception as e:
            return {
                'status': HealthStatus.UNHEALTHY,
                'message': f'Redis connection failed: {str(e)}'
            }
    
    @staticmethod
    async def check_system_resources() -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í—¬ìŠ¤ì²´í¬"""
        try:
            # CPU ì‚¬ìš©ë¥ 
            cpu_percent = psutil.cpu_percent(interval=1)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            
            # ë””ìŠ¤í¬ ì‚¬ìš©ë¥ 
            disk = psutil.disk_usage('/')
            disk_percent = disk.percent
            
            # ì„ê³„ê°’ ì²´í¬
            warnings = []
            if cpu_percent > 80:
                warnings.append(f"High CPU usage: {cpu_percent}%")
            if memory_percent > 85:
                warnings.append(f"High memory usage: {memory_percent}%")
            if disk_percent > 90:
                warnings.append(f"High disk usage: {disk_percent}%")
            
            if warnings:
                status = HealthStatus.WARNING if len(warnings) == 1 else HealthStatus.UNHEALTHY
                message = "; ".join(warnings)
            else:
                status = HealthStatus.HEALTHY
                message = f"System resources OK (CPU: {cpu_percent}%, Memory: {memory_percent}%, Disk: {disk_percent}%)"
            
            return {
                'status': status,
                'message': message,
                'metadata': {
                    'cpu_percent': cpu_percent,
                    'memory_percent': memory_percent,
                    'disk_percent': disk_percent
                }
            }
            
        except Exception as e:
            return {
                'status': HealthStatus.UNHEALTHY,
                'message': f'System resource check failed: {str(e)}'
            }

class RecoveryActions:
    """ë³µêµ¬ ì•¡ì…˜ í•¨ìˆ˜ ëª¨ìŒ"""
    
    def __init__(self, docker_client):
        self.docker_client = docker_client
    
    async def restart_container(self, container_name: str, result: HealthResult) -> bool:
        """ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘"""
        try:
            container = self.docker_client.containers.get(container_name)
            
            logging.info(f"Restarting container: {container_name}")
            container.restart(timeout=30)
            
            # ì¬ì‹œì‘ í›„ ì ì‹œ ëŒ€ê¸°
            await asyncio.sleep(10)
            
            # ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
            container.reload()
            if container.status == 'running':
                logging.info(f"Container {container_name} restarted successfully")
                return True
            else:
                logging.error(f"Container {container_name} failed to start after restart")
                return False
                
        except Exception as e:
            logging.error(f"Failed to restart container {container_name}: {e}")
            return False
    
    async def restart_service(self, service_name: str, result: HealthResult) -> bool:
        """systemd ì„œë¹„ìŠ¤ ì¬ì‹œì‘"""
        try:
            import subprocess
            
            # ì„œë¹„ìŠ¤ ì¬ì‹œì‘
            subprocess.run(['systemctl', 'restart', service_name], check=True)
            
            # ì¬ì‹œì‘ í›„ ìƒíƒœ í™•ì¸
            await asyncio.sleep(5)
            result = subprocess.run(['systemctl', 'is-active', service_name], 
                                 capture_output=True, text=True)
            
            if result.stdout.strip() == 'active':
                logging.info(f"Service {service_name} restarted successfully")
                return True
            else:
                logging.error(f"Service {service_name} failed to start after restart")
                return False
                
        except Exception as e:
            logging.error(f"Failed to restart service {service_name}: {e}")
            return False
    
    async def clear_cache(self, cache_type: str, result: HealthResult) -> bool:
        """ìºì‹œ ì •ë¦¬"""
        try:
            if cache_type == 'redis':
                import aioredis
                redis = aioredis.from_url("redis://localhost:6379")
                await redis.flushdb()
                await redis.close()
                
                logging.info("Redis cache cleared successfully")
                return True
            
            return False
            
        except Exception as e:
            logging.error(f"Failed to clear {cache_type} cache: {e}")
            return False

### ğŸ”¥ **ì„œí‚· ë¸Œë ˆì´ì»¤ íŒ¨í„´**

```python
# automation/circuit_breaker.py
from typing import Optional, Callable, Any
from enum import Enum
import asyncio
import time
from dataclasses import dataclass

class CircuitState(Enum):
    """ì„œí‚· ë¸Œë ˆì´ì»¤ ìƒíƒœ"""
    CLOSED = "closed"      # ì •ìƒ ë™ì‘
    OPEN = "open"          # ì°¨ë‹¨ë¨
    HALF_OPEN = "half_open"  # í…ŒìŠ¤íŠ¸ ì¤‘

@dataclass
class CircuitBreakerConfig:
    """ì„œí‚· ë¸Œë ˆì´ì»¤ ì„¤ì •"""
    failure_threshold: int = 5         # ì‹¤íŒ¨ ì„ê³„ê°’
    recovery_timeout: int = 60         # ë³µêµ¬ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
    expected_exception: type = Exception
    success_threshold: int = 3         # ë°˜ì—´ë¦¼ ìƒíƒœì—ì„œ ì„±ê³µ ì„ê³„ê°’

class CircuitBreaker:
    """ì„œí‚· ë¸Œë ˆì´ì»¤ êµ¬í˜„"""
    
    def __init__(self, config: CircuitBreakerConfig):
        self.config = config
        self.state = CircuitState.CLOSED
        self.failure_count = 0
        self.success_count = 0
        self.last_failure_time = None
        self.listeners = []
    
    async def call(self, func: Callable, *args, **kwargs) -> Any:
        """ì„œí‚· ë¸Œë ˆì´ì»¤ë¥¼ í†µí•œ í•¨ìˆ˜ í˜¸ì¶œ"""
        
        if self.state == CircuitState.OPEN:
            if self._should_attempt_reset():
                self.state = CircuitState.HALF_OPEN
                self.success_count = 0
            else:
                raise CircuitOpenException("Circuit breaker is OPEN")
        
        try:
            result = await func(*args, **kwargs)
            await self._on_success()
            return result
            
        except self.config.expected_exception as e:
            await self._on_failure()
            raise e
    
    def _should_attempt_reset(self) -> bool:
        """ë¦¬ì…‹ ì‹œë„ ì—¬ë¶€ í™•ì¸"""
        return (
            self.last_failure_time is not None and
            time.time() - self.last_failure_time >= self.config.recovery_timeout
        )
    
    async def _on_success(self):
        """ì„±ê³µ ì‹œ ì²˜ë¦¬"""
        self.failure_count = 0
        
        if self.state == CircuitState.HALF_OPEN:
            self.success_count += 1
            if self.success_count >= self.config.success_threshold:
                self.state = CircuitState.CLOSED
                await self._notify_listeners('state_changed', CircuitState.CLOSED)
    
    async def _on_failure(self):
        """ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬"""
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.failure_count >= self.config.failure_threshold:
            if self.state != CircuitState.OPEN:
                self.state = CircuitState.OPEN
                await self._notify_listeners('state_changed', CircuitState.OPEN)
    
    def add_listener(self, listener: Callable):
        """ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ì¶”ê°€"""
        self.listeners.append(listener)
    
    async def _notify_listeners(self, event_type: str, data: Any):
        """ë¦¬ìŠ¤ë„ˆë“¤ì—ê²Œ ì´ë²¤íŠ¸ ì•Œë¦¼"""
        for listener in self.listeners:
            try:
                await listener(event_type, data)
            except Exception as e:
                logging.error(f"Circuit breaker listener error: {e}")

class CircuitOpenException(Exception):
    """ì„œí‚· ë¸Œë ˆì´ì»¤ ì—´ë¦¼ ì˜ˆì™¸"""
    pass

### ğŸ”„ **ë°±ì—… ì‹œìŠ¤í…œ ìë™ ì „í™˜**

```python
# automation/failover_manager.py
import asyncio
import aioredis
import asyncpg
from typing import Dict, List, Optional, Any
from enum import Enum
from dataclasses import dataclass
import logging
import time

class ServiceRole(Enum):
    """ì„œë¹„ìŠ¤ ì—­í• """
    PRIMARY = "primary"
    SECONDARY = "secondary"
    STANDBY = "standby"

class FailoverTrigger(Enum):
    """ì¥ì•  ì „í™˜ íŠ¸ë¦¬ê±°"""
    HEALTH_CHECK_FAILURE = "health_check_failure"
    MANUAL_TRIGGER = "manual_trigger"
    PERFORMANCE_DEGRADATION = "performance_degradation"
    RESOURCE_EXHAUSTION = "resource_exhaustion"

@dataclass
class ServiceEndpoint:
    """ì„œë¹„ìŠ¤ ì—”ë“œí¬ì¸íŠ¸"""
    name: str
    host: str
    port: int
    role: ServiceRole
    priority: int = 0  # ë‚®ì„ìˆ˜ë¡ ë†’ì€ ìš°ì„ ìˆœìœ„
    health_check_url: Optional[str] = None

class FailoverManager:
    """ì¥ì•  ì „í™˜ ê´€ë¦¬ì"""
    
    def __init__(self, notification_system):
        self.notification_system = notification_system
        self.service_groups: Dict[str, List[ServiceEndpoint]] = {}
        self.current_primary: Dict[str, ServiceEndpoint] = {}
        self.failover_history: List[Dict] = []
        self.is_monitoring = False
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def register_service_group(self, group_name: str, endpoints: List[ServiceEndpoint]):
        """ì„œë¹„ìŠ¤ ê·¸ë£¹ ë“±ë¡"""
        self.service_groups[group_name] = sorted(endpoints, key=lambda x: x.priority)
        
        # í˜„ì¬ í”„ë¼ì´ë¨¸ë¦¬ ì„ íƒ
        primary = next((ep for ep in endpoints if ep.role == ServiceRole.PRIMARY), None)
        if primary:
            self.current_primary[group_name] = primary
        else:
            # ìš°ì„ ìˆœìœ„ê°€ ê°€ì¥ ë†’ì€ ê²ƒì„ í”„ë¼ì´ë¨¸ë¦¬ë¡œ ì„¤ì •
            self.current_primary[group_name] = endpoints[0]
            endpoints[0].role = ServiceRole.PRIMARY
    
    async def trigger_failover(self, group_name: str, trigger: FailoverTrigger, 
                             reason: str = "", target_endpoint: Optional[str] = None) -> bool:
        """ì¥ì•  ì „í™˜ ì‹¤í–‰"""
        if group_name not in self.service_groups:
            self.logger.error(f"Service group not found: {group_name}")
            return False
        
        current_primary = self.current_primary.get(group_name)
        if not current_primary:
            self.logger.error(f"No primary service found for group: {group_name}")
            return False
        
        # ëŒ€ìƒ ì—”ë“œí¬ì¸íŠ¸ ì„ íƒ
        candidates = [ep for ep in self.service_groups[group_name] 
                     if ep.name != current_primary.name and ep.role != ServiceRole.STANDBY]
        
        if target_endpoint:
            target = next((ep for ep in candidates if ep.name == target_endpoint), None)
            if not target:
                self.logger.error(f"Target endpoint not found: {target_endpoint}")
                return False
            new_primary = target
        else:
            # ìš°ì„ ìˆœìœ„ê°€ ê°€ì¥ ë†’ì€ í›„ë³´ ì„ íƒ
            if not candidates:
                self.logger.error(f"No failover candidates for group: {group_name}")
                return False
            new_primary = min(candidates, key=lambda x: x.priority)
        
        # ìƒˆ í”„ë¼ì´ë¨¸ë¦¬ì˜ í—¬ìŠ¤ì²´í¬
        if not await self._verify_endpoint_health(new_primary):
            self.logger.error(f"New primary endpoint is not healthy: {new_primary.name}")
            return False
        
        # ì¥ì•  ì „í™˜ ì‹¤í–‰
        try:
            await self._execute_failover(group_name, current_primary, new_primary, trigger, reason)
            return True
            
        except Exception as e:
            self.logger.error(f"Failover execution failed: {e}")
            return False
    
    async def _execute_failover(self, group_name: str, old_primary: ServiceEndpoint, 
                              new_primary: ServiceEndpoint, trigger: FailoverTrigger, reason: str):
        """ì¥ì•  ì „í™˜ ì‹¤í–‰"""
        
        self.logger.info(f"Executing failover for {group_name}: {old_primary.name} -> {new_primary.name}")
        
        # 1. íŠ¸ë˜í”½ ë“œë ˆì´ë‹ (ì§„í–‰ì¤‘ì¸ ìš”ì²­ ì™„ë£Œ ëŒ€ê¸°)
        await self._drain_traffic(old_primary)
        
        # 2. ì—­í•  ë³€ê²½
        old_primary.role = ServiceRole.SECONDARY
        new_primary.role = ServiceRole.PRIMARY
        self.current_primary[group_name] = new_primary
        
        # 3. ì„¤ì • ì—…ë°ì´íŠ¸ (ë¡œë“œ ë°¸ëŸ°ì„œ, DNS ë“±)
        await self._update_service_configuration(group_name, new_primary)
        
        # 4. ìƒˆ í”„ë¼ì´ë¨¸ë¦¬ ê²€ì¦
        await self._verify_failover_success(group_name, new_primary)
        
        # 5. ê¸°ë¡ ë° ì•Œë¦¼
        failover_record = {
            'timestamp': time.time(),
            'group_name': group_name,
            'old_primary': old_primary.name,
            'new_primary': new_primary.name,
            'trigger': trigger.value,
            'reason': reason
        }
        
        self.failover_history.append(failover_record)
        
        # ì•Œë¦¼ ì „ì†¡
        await self._send_failover_notification(failover_record)
        
        self.logger.info(f"Failover completed for {group_name}")
    
    async def _drain_traffic(self, endpoint: ServiceEndpoint):
        """íŠ¸ë˜í”½ ë“œë ˆì´ë‹"""
        # ì§„í–‰ì¤‘ì¸ ì—°ê²° ì™„ë£Œ ëŒ€ê¸°
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë¡œë“œ ë°¸ëŸ°ì„œ API ì‚¬ìš©
        await asyncio.sleep(5)  # ì„ì‹œ ëŒ€ê¸°
    
    async def _update_service_configuration(self, group_name: str, new_primary: ServiceEndpoint):
        """ì„œë¹„ìŠ¤ ì„¤ì • ì—…ë°ì´íŠ¸"""
        # DNS ì—…ë°ì´íŠ¸, ë¡œë“œ ë°¸ëŸ°ì„œ ì„¤ì • ë“±
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì¸í”„ë¼ API ì‚¬ìš©
        pass
    
    async def _verify_endpoint_health(self, endpoint: ServiceEndpoint) -> bool:
        """ì—”ë“œí¬ì¸íŠ¸ í—¬ìŠ¤ì²´í¬"""
        if not endpoint.health_check_url:
            return True  # í—¬ìŠ¤ì²´í¬ URLì´ ì—†ìœ¼ë©´ í†µê³¼
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(endpoint.health_check_url, timeout=10) as resp:
                    return resp.status == 200
        except Exception as e:
            self.logger.error(f"Health check failed for {endpoint.name}: {e}")
            return False
    
    async def _verify_failover_success(self, group_name: str, new_primary: ServiceEndpoint):
        """ì¥ì•  ì „í™˜ ì„±ê³µ ê²€ì¦"""
        # ìƒˆ í”„ë¼ì´ë¨¸ë¦¬ê°€ ì •ìƒ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸
        max_attempts = 10
        for attempt in range(max_attempts):
            if await self._verify_endpoint_health(new_primary):
                self.logger.info(f"Failover verification successful for {group_name}")
                return True
            
            await asyncio.sleep(2)
        
        raise Exception(f"Failover verification failed for {group_name}")
    
    async def _send_failover_notification(self, record: Dict):
        """ì¥ì•  ì „í™˜ ì•Œë¦¼ ì „ì†¡"""
        title = f"ğŸ”„ ì¥ì•  ì „í™˜ ì™„ë£Œ: {record['group_name']}"
        message = f"""
ì„œë¹„ìŠ¤ ê·¸ë£¹: {record['group_name']}
ì´ì „ í”„ë¼ì´ë¨¸ë¦¬: {record['old_primary']}
ìƒˆ í”„ë¼ì´ë¨¸ë¦¬: {record['new_primary']}
íŠ¸ë¦¬ê±°: {record['trigger']}
ì‚¬ìœ : {record['reason']}
ì‹œê°„: {datetime.fromtimestamp(record['timestamp']).strftime('%Y-%m-%d %H:%M:%S')}
        """.strip()
        
        notification = Notification(
            title=title,
            message=message,
            priority=NotificationPriority.HIGH,
            tags=['failover', record['group_name']]
        )
        
        await self.notification_system.route_notification(notification)

### ğŸš« **ì¥ì•  ì „íŒŒ ë°©ì§€**

```python
# automation/failure_isolation.py
import asyncio
from typing import Dict, Set, List, Optional
from enum import Enum
from dataclasses import dataclass
import networkx as nx
import logging

class IsolationStrategy(Enum):
    """ê²©ë¦¬ ì „ëµ"""
    CIRCUIT_BREAKER = "circuit_breaker"
    BULKHEAD = "bulkhead"
    TIMEOUT = "timeout"
    RETRY_WITH_BACKOFF = "retry_with_backoff"
    GRACEFUL_DEGRADATION = "graceful_degradation"

class DependencyType(Enum):
    """ì˜ì¡´ì„± íƒ€ì…"""
    SYNCHRONOUS = "synchronous"
    ASYNCHRONOUS = "asynchronous"
    DATA = "data"
    CONFIGURATION = "configuration"

@dataclass
class ServiceDependency:
    """ì„œë¹„ìŠ¤ ì˜ì¡´ì„±"""
    source: str
    target: str
    dependency_type: DependencyType
    criticality: int  # 1-10, 10ì´ ê°€ì¥ ì¤‘ìš”
    isolation_strategies: List[IsolationStrategy]

class FailureIsolationManager:
    """ì¥ì•  ê²©ë¦¬ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.dependency_graph = nx.DiGraph()
        self.isolation_policies: Dict[str, Dict] = {}
        self.active_isolations: Dict[str, Set[str]] = {}
        self.circuit_breakers: Dict[str, CircuitBreaker] = {}
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def register_dependency(self, dependency: ServiceDependency):
        """ì„œë¹„ìŠ¤ ì˜ì¡´ì„± ë“±ë¡"""
        self.dependency_graph.add_edge(
            dependency.source, 
            dependency.target,
            **{
                'type': dependency.dependency_type,
                'criticality': dependency.criticality,
                'strategies': dependency.isolation_strategies
            }
        )
    
    def set_isolation_policy(self, service: str, policy: Dict):
        """ê²©ë¦¬ ì •ì±… ì„¤ì •"""
        self.isolation_policies[service] = policy
    
    async def handle_service_failure(self, failed_service: str, failure_details: Dict):
        """ì„œë¹„ìŠ¤ ì¥ì•  ì²˜ë¦¬"""
        self.logger.info(f"Handling failure for service: {failed_service}")
        
        # 1. ì§ì ‘ ì˜ì¡´í•˜ëŠ” ì„œë¹„ìŠ¤ë“¤ ì‹ë³„
        dependent_services = list(self.dependency_graph.predecessors(failed_service))
        
        # 2. ì¥ì•  ì „íŒŒ ìœ„í—˜ í‰ê°€
        propagation_risk = await self._assess_propagation_risk(failed_service, failure_details)
        
        # 3. ê²©ë¦¬ ì „ëµ ì ìš©
        isolation_actions = await self._apply_isolation_strategies(
            failed_service, dependent_services, propagation_risk
        )
        
        # 4. ë³µêµ¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        asyncio.create_task(self._monitor_recovery(failed_service, isolation_actions))
        
        return isolation_actions
    
    async def _assess_propagation_risk(self, failed_service: str, failure_details: Dict) -> int:
        """ì¥ì•  ì „íŒŒ ìœ„í—˜ í‰ê°€"""
        risk_score = 0
        
        # ì‹¤íŒ¨í•œ ì„œë¹„ìŠ¤ì˜ ì¤‘ìš”ë„
        service_criticality = failure_details.get('criticality', 5)
        risk_score += service_criticality
        
        # ì˜ì¡´ ì„œë¹„ìŠ¤ ìˆ˜
        dependent_count = len(list(self.dependency_graph.predecessors(failed_service)))
        risk_score += min(dependent_count * 2, 20)
        
        # ì‹¤íŒ¨ ìœ í˜•ë³„ ìœ„í—˜ë„
        failure_type = failure_details.get('type', 'unknown')
        if failure_type in ['resource_exhaustion', 'cascade_failure']:
            risk_score += 10
        elif failure_type in ['timeout', 'connection_error']:
            risk_score += 5
        
        return min(risk_score, 100)  # ìµœëŒ€ 100ì 
    
    async def _apply_isolation_strategies(self, failed_service: str, 
                                        dependent_services: List[str], 
                                        risk_score: int) -> List[Dict]:
        """ê²©ë¦¬ ì „ëµ ì ìš©"""
        actions = []
        
        for dependent in dependent_services:
            edge_data = self.dependency_graph.get_edge_data(dependent, failed_service)
            strategies = edge_data.get('strategies', [])
            
            for strategy in strategies:
                action = await self._apply_strategy(
                    strategy, dependent, failed_service, risk_score
                )
                if action:
                    actions.append(action)
        
        return actions
    
    async def _apply_strategy(self, strategy: IsolationStrategy, 
                            dependent: str, failed_service: str, risk_score: int) -> Optional[Dict]:
        """ê°œë³„ ê²©ë¦¬ ì „ëµ ì ìš©"""
        
        if strategy == IsolationStrategy.CIRCUIT_BREAKER:
            return await self._apply_circuit_breaker(dependent, failed_service)
        
        elif strategy == IsolationStrategy.BULKHEAD:
            return await self._apply_bulkhead_isolation(dependent, failed_service)
        
        elif strategy == IsolationStrategy.GRACEFUL_DEGRADATION:
            return await self._apply_graceful_degradation(dependent, failed_service, risk_score)
        
        elif strategy == IsolationStrategy.TIMEOUT:
            return await self._apply_timeout_reduction(dependent, failed_service)
        
        return None
    
    async def _apply_circuit_breaker(self, dependent: str, failed_service: str) -> Dict:
        """ì„œí‚· ë¸Œë ˆì´ì»¤ ì ìš©"""
        circuit_key = f"{dependent}->{failed_service}"
        
        if circuit_key not in self.circuit_breakers:
            config = CircuitBreakerConfig(
                failure_threshold=3,
                recovery_timeout=30,
                success_threshold=2
            )
            self.circuit_breakers[circuit_key] = CircuitBreaker(config)
        
        # ì„œí‚· ë¸Œë ˆì´ì»¤ ê°•ì œ ê°œë°©
        self.circuit_breakers[circuit_key].state = CircuitState.OPEN
        self.circuit_breakers[circuit_key].last_failure_time = time.time()
        
        self.logger.info(f"Circuit breaker opened: {circuit_key}")
        
        return {
            'strategy': 'circuit_breaker',
            'dependent': dependent,
            'target': failed_service,
            'status': 'applied'
        }
    
    async def _apply_bulkhead_isolation(self, dependent: str, failed_service: str) -> Dict:
        """ë²Œí¬í—¤ë“œ ê²©ë¦¬ ì ìš©"""
        # ì‹¤íŒ¨í•œ ì„œë¹„ìŠ¤ì™€ì˜ ì—°ê²°ì„ ë³„ë„ ìŠ¤ë ˆë“œ í’€ë¡œ ê²©ë¦¬
        if dependent not in self.active_isolations:
            self.active_isolations[dependent] = set()
        
        self.active_isolations[dependent].add(failed_service)
        
        self.logger.info(f"Bulkhead isolation applied: {dependent} -> {failed_service}")
        
        return {
            'strategy': 'bulkhead',
            'dependent': dependent,
            'target': failed_service,
            'status': 'applied'
        }
    
    async def _apply_graceful_degradation(self, dependent: str, failed_service: str, risk_score: int) -> Dict:
        """ìš°ì•„í•œ ì„±ëŠ¥ ì €í•˜ ì ìš©"""
        # ìœ„í—˜ë„ì— ë”°ë¥¸ ì„±ëŠ¥ ì €í•˜ ìˆ˜ì¤€ ê²°ì •
        if risk_score > 70:
            degradation_level = 'high'  # ê¸°ëŠ¥ ëŒ€í­ ì œí•œ
        elif risk_score > 40:
            degradation_level = 'medium'  # ì¼ë¶€ ê¸°ëŠ¥ ì œí•œ
        else:
            degradation_level = 'low'  # ìµœì†Œí•œì˜ ì œí•œ
        
        # ì„±ëŠ¥ ì €í•˜ ì •ì±… ì ìš© (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì„œë¹„ìŠ¤ë³„ ë¡œì§)
        policy = self.isolation_policies.get(dependent, {})
        degradation_config = policy.get('graceful_degradation', {})
        
        self.logger.info(f"Graceful degradation applied: {dependent}, level: {degradation_level}")
        
        return {
            'strategy': 'graceful_degradation',
            'dependent': dependent,
            'target': failed_service,
            'level': degradation_level,
            'status': 'applied'
        }
    
    async def _apply_timeout_reduction(self, dependent: str, failed_service: str) -> Dict:
        """íƒ€ì„ì•„ì›ƒ ë‹¨ì¶• ì ìš©"""
        # ê¸°ì¡´ íƒ€ì„ì•„ì›ƒì˜ 50%ë¡œ ë‹¨ì¶•
        original_timeout = 30  # ê¸°ë³¸ê°’
        reduced_timeout = max(original_timeout * 0.5, 5)  # ìµœì†Œ 5ì´ˆ
        
        self.logger.info(f"Timeout reduced: {dependent} -> {failed_service}, {original_timeout}s -> {reduced_timeout}s")
        
        return {
            'strategy': 'timeout_reduction',
            'dependent': dependent,
            'target': failed_service,
            'original_timeout': original_timeout,
            'reduced_timeout': reduced_timeout,
            'status': 'applied'
        }
    
    async def _monitor_recovery(self, failed_service: str, isolation_actions: List[Dict]):
        """ë³µêµ¬ ëª¨ë‹ˆí„°ë§"""
        recovery_attempts = 0
        max_recovery_attempts = 10
        
        while recovery_attempts < max_recovery_attempts:
            await asyncio.sleep(30)  # 30ì´ˆë§ˆë‹¤ ì²´í¬
            
            # ì„œë¹„ìŠ¤ ë³µêµ¬ ìƒíƒœ í™•ì¸
            if await self._check_service_recovery(failed_service):
                self.logger.info(f"Service recovery detected: {failed_service}")
                
                # ê²©ë¦¬ í•´ì œ
                await self._remove_isolations(failed_service, isolation_actions)
                break
            
            recovery_attempts += 1
        
        if recovery_attempts >= max_recovery_attempts:
            self.logger.warning(f"Service recovery timeout: {failed_service}")
    
    async def _check_service_recovery(self, service: str) -> bool:
        """ì„œë¹„ìŠ¤ ë³µêµ¬ ìƒíƒœ í™•ì¸"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” í—¬ìŠ¤ì²´í¬ ìˆ˜í–‰
        return False  # ì„ì‹œë¡œ False ë°˜í™˜
    
    async def _remove_isolations(self, recovered_service: str, isolation_actions: List[Dict]):
        """ê²©ë¦¬ í•´ì œ"""
        for action in isolation_actions:
            strategy = action['strategy']
            
            if strategy == 'circuit_breaker':
                circuit_key = f"{action['dependent']}->{recovered_service}"
                if circuit_key in self.circuit_breakers:
                    self.circuit_breakers[circuit_key].state = CircuitState.HALF_OPEN
            
            elif strategy == 'bulkhead':
                dependent = action['dependent']
                if dependent in self.active_isolations:
                    self.active_isolations[dependent].discard(recovered_service)
            
            self.logger.info(f"Isolation removed: {strategy} for {recovered_service}")

---

## âš¡ **ì„±ê³¼ ìµœì í™” ìë™í™”**

### ğŸ§  **ML ê¸°ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**

```python
# automation/hyperparameter_optimization.py
import optuna
import asyncio
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any, Callable
from datetime import datetime, timedelta
import logging
import json
from dataclasses import dataclass
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import joblib

@dataclass
class OptimizationTarget:
    """ìµœì í™” ëª©í‘œ"""
    name: str
    metric: str  # 'sharpe_ratio', 'total_return', 'max_drawdown', etc.
    direction: str  # 'maximize' or 'minimize'
    weight: float = 1.0

@dataclass
class ParameterSpace:
    """íŒŒë¼ë¯¸í„° ê³µê°„ ì •ì˜"""
    name: str
    param_type: str  # 'int', 'float', 'categorical'
    low: Optional[float] = None
    high: Optional[float] = None
    choices: Optional[List] = None
    step: Optional[float] = None

class HyperparameterOptimizer:
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self, freqtrade_manager, notification_system):
        self.freqtrade_manager = freqtrade_manager
        self.notification_system = notification_system
        self.optimization_history = []
        self.current_studies = {}
        self.surrogate_models = {}
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    async def optimize_strategy(self, strategy_name: str, parameter_spaces: List[ParameterSpace],
                              optimization_targets: List[OptimizationTarget],
                              n_trials: int = 100, timeout_hours: int = 24) -> Dict[str, Any]:
        """ì „ëµ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        
        study_name = f"{strategy_name}_optimization_{int(datetime.now().timestamp())}"
        
        # Optuna ìŠ¤í„°ë”” ìƒì„±
        study = optuna.create_study(
            study_name=study_name,
            directions=['maximize' if target.direction == 'maximize' else 'minimize' 
                       for target in optimization_targets],
            sampler=optuna.samplers.TPESampler(),
            pruner=optuna.pruners.MedianPruner()
        )
        
        self.current_studies[study_name] = study
        
        # ëª©ì  í•¨ìˆ˜ ì •ì˜
        async def objective(trial):
            return await self._evaluate_trial(trial, strategy_name, parameter_spaces, optimization_targets)
        
        try:
            # ìµœì í™” ì‹¤í–‰
            await self._run_optimization(study, objective, n_trials, timeout_hours)
            
            # ê²°ê³¼ ë¶„ì„ ë° ì €ì¥
            results = await self._analyze_optimization_results(study, strategy_name, optimization_targets)
            
            # ìµœì  íŒŒë¼ë¯¸í„° ì ìš©
            await self._apply_best_parameters(strategy_name, results['best_params'])
            
            # ê²°ê³¼ ì•Œë¦¼
            await self._send_optimization_notification(strategy_name, results)
            
            return results
            
        except Exception as e:
            self.logger.error(f"Optimization failed for {strategy_name}: {e}")
            raise e
        finally:
            if study_name in self.current_studies:
                del self.current_studies[study_name]
    
    async def _evaluate_trial(self, trial, strategy_name: str, parameter_spaces: List[ParameterSpace],
                             optimization_targets: List[OptimizationTarget]) -> List[float]:
        """ì‹œí–‰ í‰ê°€"""
        
        # íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
        params = {}
        for param_space in parameter_spaces:
            if param_space.param_type == 'int':
                params[param_space.name] = trial.suggest_int(
                    param_space.name, param_space.low, param_space.high, step=param_space.step
                )
            elif param_space.param_type == 'float':
                params[param_space.name] = trial.suggest_float(
                    param_space.name, param_space.low, param_space.high, step=param_space.step
                )
            elif param_space.param_type == 'categorical':
                params[param_space.name] = trial.suggest_categorical(
                    param_space.name, param_space.choices
                )
        
        # ë°±í…ŒìŠ¤íŒ… ì‹¤í–‰
        backtest_results = await self._run_backtest_with_params(strategy_name, params)
        
        # ëª©í‘œ ë©”íŠ¸ë¦­ ê³„ì‚°
        objective_values = []
        for target in optimization_targets:
            value = backtest_results.get(target.metric, 0)
            
            # ê°€ì¤‘ì¹˜ ì ìš©
            weighted_value = value * target.weight
            objective_values.append(weighted_value)
            
            # ì¤‘ê°„ ê²°ê³¼ ë³´ê³  (ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•œ)
            trial.report(weighted_value, step=trial.number)
            
            # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
            if trial.should_prune():
                raise optuna.TrialPruned()
        
        return objective_values
    
    async def _run_backtest_with_params(self, strategy_name: str, params: Dict[str, Any]) -> Dict[str, float]:
        """íŒŒë¼ë¯¸í„°ë¥¼ ì ìš©í•œ ë°±í…ŒìŠ¤íŒ… ì‹¤í–‰"""
        
        # ì„ì‹œ ì„¤ì • íŒŒì¼ ìƒì„±
        config = await self._create_temp_config(strategy_name, params)
        
        # ë°±í…ŒìŠ¤íŒ… ì‹¤í–‰
        backtest_command = [
            'freqtrade', 'backtesting',
            '--config', config['config_path'],
            '--strategy', strategy_name,
            '--timerange', '20241001-20241201',  # ìµœê·¼ 2ê°œì›”
            '--breakdown', 'day'
        ]
        
        result = await self.freqtrade_manager.run_command(backtest_command)
        
        # ê²°ê³¼ íŒŒì‹±
        return await self._parse_backtest_results(result)
    
    async def _create_temp_config(self, strategy_name: str, params: Dict[str, Any]) -> Dict[str, str]:
        """ì„ì‹œ ì„¤ì • íŒŒì¼ ìƒì„±"""
        
        base_config = await self.freqtrade_manager.get_base_config()
        
        # íŒŒë¼ë¯¸í„° ì ìš©
        strategy_config = base_config.get('strategy_parameters', {})
        strategy_config.update(params)
        base_config['strategy_parameters'] = strategy_config
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        temp_config_path = f"/tmp/config_{strategy_name}_{int(time.time())}.json"
        with open(temp_config_path, 'w') as f:
            json.dump(base_config, f, indent=2)
        
        return {'config_path': temp_config_path}
    
    async def _parse_backtest_results(self, backtest_output: str) -> Dict[str, float]:
        """ë°±í…ŒìŠ¤íŒ… ê²°ê³¼ íŒŒì‹±"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Freqtrade ê²°ê³¼ JSON íŒŒì‹±
        # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œ ë°ì´í„° ë°˜í™˜
        
        return {
            'total_return': np.random.uniform(0.05, 0.25),
            'sharpe_ratio': np.random.uniform(0.5, 2.5),
            'max_drawdown': np.random.uniform(0.05, 0.2),
            'win_rate': np.random.uniform(0.4, 0.7),
            'profit_factor': np.random.uniform(1.1, 2.0),
            'calmar_ratio': np.random.uniform(0.5, 1.5)
        }
    
    async def _run_optimization(self, study, objective_func: Callable, 
                               n_trials: int, timeout_hours: int):
        """ìµœì í™” ì‹¤í–‰"""
        
        start_time = time.time()
        timeout_seconds = timeout_hours * 3600
        
        for trial_num in range(n_trials):
            # íƒ€ì„ì•„ì›ƒ ì²´í¬
            if time.time() - start_time > timeout_seconds:
                self.logger.info(f"Optimization timeout reached after {timeout_hours} hours")
                break
            
            try:
                # ë¹„ë™ê¸° ëª©ì  í•¨ìˆ˜ ì‹¤í–‰ì„ ìœ„í•œ ë˜í¼
                def sync_objective(trial):
                    return asyncio.run(objective_func(trial))
                
                study.optimize(sync_objective, n_trials=1, timeout=3600)  # ì‹œí–‰ë‹¹ 1ì‹œê°„ ì œí•œ
                
                # ì§„í–‰ ìƒí™© ë¡œê¹…
                if trial_num % 10 == 0:
                    self.logger.info(f"Optimization progress: {trial_num}/{n_trials} trials completed")
                
            except Exception as e:
                self.logger.error(f"Trial {trial_num} failed: {e}")
                continue
    
    async def _analyze_optimization_results(self, study, strategy_name: str, 
                                          optimization_targets: List[OptimizationTarget]) -> Dict[str, Any]:
        """ìµœì í™” ê²°ê³¼ ë¶„ì„"""
        
        # ìµœì  ì‹œí–‰ ì„ íƒ (ë‹¤ëª©ì  ìµœì í™”ì˜ ê²½ìš° íŒŒë ˆí†  í”„ë¡ íŠ¸)
        if len(optimization_targets) == 1:
            best_trial = study.best_trial
            best_params = best_trial.params
            best_values = {optimization_targets[0].metric: best_trial.value}
        else:
            # ë‹¤ëª©ì  ìµœì í™” - íŒŒë ˆí†  ìµœì í•´ ì¤‘ í•˜ë‚˜ ì„ íƒ
            pareto_trials = study.best_trials
            if pareto_trials:
                # ê°€ì¤‘ í•©ê³„ê°€ ê°€ì¥ ë†’ì€ ì‹œí–‰ ì„ íƒ
                best_trial = max(pareto_trials, 
                               key=lambda t: sum(v * target.weight for v, target 
                                                in zip(t.values, optimization_targets)))
                best_params = best_trial.params
                best_values = {target.metric: value for target, value 
                             in zip(optimization_targets, best_trial.values)}
            else:
                best_params = {}
                best_values = {}
        
        # íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„ ë¶„ì„
        param_importance = {}
        for target in optimization_targets:
            importance = optuna.importance.get_param_importances(study)
            param_importance[target.metric] = importance
        
        # ìµœì í™” íˆìŠ¤í† ë¦¬
        optimization_record = {
            'strategy_name': strategy_name,
            'timestamp': datetime.now().isoformat(),
            'n_trials': len(study.trials),
            'best_params': best_params,
            'best_values': best_values,
            'param_importance': param_importance,
            'optimization_targets': [target.__dict__ for target in optimization_targets]
        }
        
        self.optimization_history.append(optimization_record)
        
        return {
            'best_params': best_params,
            'best_values': best_values,
            'param_importance': param_importance,
            'n_trials': len(study.trials),
            'study': study
        }
    
    async def _apply_best_parameters(self, strategy_name: str, best_params: Dict[str, Any]):
        """ìµœì  íŒŒë¼ë¯¸í„° ì ìš©"""
        
        # í”„ë¡œë•ì…˜ ì„¤ì •ì— íŒŒë¼ë¯¸í„° ì ìš©
        await self.freqtrade_manager.update_strategy_parameters(strategy_name, best_params)
        
        # ì „ëµ ì¬ì‹œì‘
        await self.freqtrade_manager.restart_strategy(strategy_name)
        
        self.logger.info(f"Applied optimized parameters to {strategy_name}: {best_params}")
    
    async def _send_optimization_notification(self, strategy_name: str, results: Dict[str, Any]):
        """ìµœì í™” ì™„ë£Œ ì•Œë¦¼"""
        
        title = f"ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì™„ë£Œ: {strategy_name}"
        
        message = f"""
ìµœì í™” ì™„ë£Œ ê²°ê³¼:
- ì‹œí–‰ íšŸìˆ˜: {results['n_trials']}
- ìµœì  íŒŒë¼ë¯¸í„°: {json.dumps(results['best_params'], indent=2)}
- ìµœì  ì„±ëŠ¥: {json.dumps(results['best_values'], indent=2)}
        """.strip()
        
        notification = Notification(
            title=title,
            message=message,
            priority=NotificationPriority.MEDIUM,
            tags=['optimization', strategy_name]
        )
        
        await self.notification_system.route_notification(notification)

class AutoML_StrategySelector:
    """ìë™ ê¸°ê³„í•™ìŠµ ê¸°ë°˜ ì „ëµ ì„ íƒê¸°"""
    
    def __init__(self, market_data_provider, performance_analyzer):
        self.market_data_provider = market_data_provider
        self.performance_analyzer = performance_analyzer
        self.feature_extractors = {}
        self.selection_models = {}
        self.strategy_performance_history = {}
        
    async def train_selection_model(self, historical_data_months: int = 12):
        """ì „ëµ ì„ íƒ ëª¨ë¸ í›ˆë ¨"""
        
        # 1. ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘
        end_date = datetime.now()
        start_date = end_date - timedelta(days=historical_data_months * 30)
        
        market_data = await self.market_data_provider.get_historical_data(
            start_date, end_date, ['BTCUSDT', 'ETHUSDT', 'ADAUSDT']
        )
        
        # 2. íŠ¹ì§• ì¶”ì¶œ
        features = await self._extract_market_features(market_data)
        
        # 3. ì „ëµë³„ ì„±ê³¼ ë ˆì´ë¸” ìƒì„±
        strategy_labels = await self._generate_strategy_labels(market_data)
        
        # 4. ëª¨ë¸ í›ˆë ¨
        for strategy_name, labels in strategy_labels.items():
            model = RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                random_state=42
            )
            
            model.fit(features, labels)
            self.selection_models[strategy_name] = model
            
            # ëª¨ë¸ ì €ì¥
            joblib.dump(model, f'models/strategy_selector_{strategy_name}.pkl')
        
        self.logger.info("Strategy selection models trained successfully")
    
    async def _extract_market_features(self, market_data: pd.DataFrame) -> np.ndarray:
        """ì‹œì¥ íŠ¹ì§• ì¶”ì¶œ"""
        
        features = []
        
        for symbol in ['BTCUSDT', 'ETHUSDT', 'ADAUSDT']:
            symbol_data = market_data[market_data['symbol'] == symbol]
            
            if len(symbol_data) == 0:
                continue
            
            # ê¸°ìˆ ì  ì§€í‘œ
            prices = symbol_data['close'].values
            
            # ì´ë™í‰ê· 
            ma_20 = pd.Series(prices).rolling(20).mean().iloc[-1]
            ma_50 = pd.Series(prices).rolling(50).mean().iloc[-1]
            
            # RSI
            delta = pd.Series(prices).diff()
            gain = (delta.where(delta > 0, 0)).rolling(14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
            rs = gain / loss
            rsi = 100 - (100 / (1 + rs)).iloc[-1]
            
            # ë³€ë™ì„±
            volatility = pd.Series(prices).pct_change().rolling(20).std().iloc[-1]
            
            # ê±°ë˜ëŸ‰ íŠ¹ì§•
            volume_ma = symbol_data['volume'].rolling(20).mean().iloc[-1]
            volume_ratio = symbol_data['volume'].iloc[-1] / volume_ma
            
            features.extend([ma_20, ma_50, rsi, volatility, volume_ratio])
        
        # ê±°ì‹œê²½ì œ íŠ¹ì§• (ì¶”ê°€ ê°€ëŠ¥)
        # ì˜ˆ: VIX, DXY, ê¸ˆë¦¬ ë“±
        
        return np.array(features).reshape(1, -1)
    
    async def _generate_strategy_labels(self, market_data: pd.DataFrame) -> Dict[str, np.ndarray]:
        """ì „ëµë³„ ì„±ê³¼ ë ˆì´ë¸” ìƒì„±"""
        
        strategy_labels = {}
        
        # ê° ì „ëµì˜ ê³¼ê±° ì„±ê³¼ ë°ì´í„° ìˆ˜ì§‘
        for strategy_name in ['FuturesAIRiskStrategy', 'RossCameronFuturesStrategy']:
            performance_data = await self.performance_analyzer.get_strategy_performance(
                strategy_name, market_data.index
            )
            
            # ì„±ê³¼ ì§€í‘œë¥¼ ë ˆì´ë¸”ë¡œ ì‚¬ìš© (ì˜ˆ: ì¼ì¼ ìˆ˜ìµë¥ )
            daily_returns = performance_data.get('daily_returns', [])
            strategy_labels[strategy_name] = np.array(daily_returns)
        
        return strategy_labels
    
    async def select_optimal_strategy(self, current_market_conditions: Dict[str, Any]) -> str:
        """í˜„ì¬ ì‹œì¥ ìƒí™©ì— ìµœì ì¸ ì „ëµ ì„ íƒ"""
        
        # í˜„ì¬ ì‹œì¥ íŠ¹ì§• ì¶”ì¶œ
        current_features = await self._extract_current_market_features(current_market_conditions)
        
        # ê° ì „ëµì˜ ì˜ˆìƒ ì„±ê³¼ ì˜ˆì¸¡
        strategy_predictions = {}
        
        for strategy_name, model in self.selection_models.items():
            predicted_performance = model.predict(current_features.reshape(1, -1))[0]
            strategy_predictions[strategy_name] = predicted_performance
        
        # ìµœê³  ì„±ê³¼ ì˜ˆìƒ ì „ëµ ì„ íƒ
        optimal_strategy = max(strategy_predictions, key=strategy_predictions.get)
        
        self.logger.info(f"Optimal strategy selected: {optimal_strategy}")
        self.logger.info(f"Strategy predictions: {strategy_predictions}")
        
        return optimal_strategy
    
    async def _extract_current_market_features(self, market_conditions: Dict[str, Any]) -> np.ndarray:
        """í˜„ì¬ ì‹œì¥ íŠ¹ì§• ì¶”ì¶œ"""
        # ì‹¤ì‹œê°„ ì‹œì¥ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ íŠ¹ì§• ì¶”ì¶œ
        # _extract_market_featuresì™€ ìœ ì‚¬í•˜ì§€ë§Œ ì‹¤ì‹œê°„ ë°ì´í„° ì‚¬ìš©
        pass

### ğŸ§ª **A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬**

```python
# automation/ab_testing.py
import asyncio
import random
import numpy as np
from typing import Dict, List, Optional, Any, Callable
from enum import Enum
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
from scipy import stats
import pandas as pd

class TestStatus(Enum):
    """í…ŒìŠ¤íŠ¸ ìƒíƒœ"""
    PLANNING = "planning"
    RUNNING = "running"
    PAUSED = "paused"
    COMPLETED = "completed"
    CANCELLED = "cancelled"

class SignificanceMethod(Enum):
    """ìœ ì˜ì„± ê²€ì • ë°©ë²•"""
    T_TEST = "t_test"
    MANN_WHITNEY = "mann_whitney"
    CHI_SQUARE = "chi_square"
    BAYESIAN = "bayesian"

@dataclass
class TestVariant:
    """í…ŒìŠ¤íŠ¸ ë³€í˜•"""
    name: str
    allocation_percent: float
    strategy_name: str
    parameters: Dict[str, Any]
    description: str = ""

@dataclass
class TestMetric:
    """í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­"""
    name: str
    metric_type: str  # 'ratio', 'mean', 'count'
    primary: bool = False
    direction: str = "increase"  # "increase" or "decrease"
    min_detectable_effect: float = 0.05  # ìµœì†Œ ê°ì§€ ê°€ëŠ¥í•œ íš¨ê³¼ í¬ê¸°

@dataclass
class ABTestConfig:
    """A/B í…ŒìŠ¤íŠ¸ ì„¤ì •"""
    test_id: str
    name: str
    description: str
    variants: List[TestVariant]
    metrics: List[TestMetric]
    duration_days: int
    confidence_level: float = 0.95
    power: float = 0.8
    significance_method: SignificanceMethod = SignificanceMethod.T_TEST
    early_stopping: bool = True
    minimum_sample_size: int = 100

class ABTestManager:
    """A/B í…ŒìŠ¤íŠ¸ ê´€ë¦¬ì"""
    
    def __init__(self, strategy_manager, performance_analyzer, notification_system):
        self.strategy_manager = strategy_manager
        self.performance_analyzer = performance_analyzer
        self.notification_system = notification_system
        self.active_tests: Dict[str, ABTestConfig] = {}
        self.test_data: Dict[str, Dict] = {}
        self.allocation_cache: Dict[str, str] = {}  # ì‚¬ìš©ì -> ë³€í˜• ë§¤í•‘
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    async def create_test(self, config: ABTestConfig) -> bool:
        """A/B í…ŒìŠ¤íŠ¸ ìƒì„±"""
        
        # ì„¤ì • ê²€ì¦
        if not await self._validate_test_config(config):
            return False
        
        # í‘œë³¸ í¬ê¸° ê³„ì‚°
        required_sample_size = await self._calculate_sample_size(config)
        if required_sample_size > config.minimum_sample_size * 10:
            self.logger.warning(f"Required sample size is very large: {required_sample_size}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì´ˆê¸°í™”
        self.test_data[config.test_id] = {
            'start_time': datetime.now(),
            'status': TestStatus.PLANNING,
            'variant_data': {variant.name: {'metrics': {}, 'samples': 0} 
                           for variant in config.variants},
            'required_sample_size': required_sample_size
        }
        
        self.active_tests[config.test_id] = config
        
        self.logger.info(f"A/B test created: {config.test_id}")
        return True
    
    async def start_test(self, test_id: str) -> bool:
        """A/B í…ŒìŠ¤íŠ¸ ì‹œì‘"""
        
        if test_id not in self.active_tests:
            self.logger.error(f"Test not found: {test_id}")
            return False
        
        config = self.active_tests[test_id]
        test_data = self.test_data[test_id]
        
        # ì „ëµ ë³€í˜•ë“¤ ë°°í¬
        for variant in config.variants:
            success = await self.strategy_manager.deploy_strategy_variant(
                variant.strategy_name, variant.parameters, f"{test_id}_{variant.name}"
            )
            if not success:
                self.logger.error(f"Failed to deploy variant: {variant.name}")
                return False
        
        # í…ŒìŠ¤íŠ¸ ìƒíƒœ ì—…ë°ì´íŠ¸
        test_data['status'] = TestStatus.RUNNING
        test_data['actual_start_time'] = datetime.now()
        
        # ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬ ì‹œì‘
        asyncio.create_task(self._monitor_test(test_id))
        
        # ì‹œì‘ ì•Œë¦¼
        await self._send_test_notification(test_id, "started")
        
        self.logger.info(f"A/B test started: {test_id}")
        return True
    
    async def allocate_user_to_variant(self, test_id: str, user_id: str) -> Optional[str]:
        """ì‚¬ìš©ìë¥¼ í…ŒìŠ¤íŠ¸ ë³€í˜•ì— í• ë‹¹"""
        
        if test_id not in self.active_tests:
            return None
        
        # ìºì‹œëœ í• ë‹¹ í™•ì¸
        cache_key = f"{test_id}_{user_id}"
        if cache_key in self.allocation_cache:
            return self.allocation_cache[cache_key]
        
        config = self.active_tests[test_id]
        test_data = self.test_data[test_id]
        
        if test_data['status'] != TestStatus.RUNNING:
            return None
        
        # í•´ì‹œ ê¸°ë°˜ ì¼ê´€ëœ í• ë‹¹
        hash_value = hash(f"{test_id}_{user_id}") % 100
        
        cumulative_percent = 0
        for variant in config.variants:
            cumulative_percent += variant.allocation_percent
            if hash_value < cumulative_percent:
                self.allocation_cache[cache_key] = variant.name
                return variant.name
        
        # ê¸°ë³¸ê°’ (ë§ˆì§€ë§‰ ë³€í˜•)
        default_variant = config.variants[-1].name
        self.allocation_cache[cache_key] = default_variant
        return default_variant
    
    async def record_metric(self, test_id: str, user_id: str, metric_name: str, value: float):
        """ë©”íŠ¸ë¦­ ê¸°ë¡"""
        
        if test_id not in self.active_tests:
            return
        
        variant = await self.allocate_user_to_variant(test_id, user_id)
        if not variant:
            return
        
        test_data = self.test_data[test_id]
        variant_data = test_data['variant_data'][variant]
        
        # ë©”íŠ¸ë¦­ ë°ì´í„° ì €ì¥
        if metric_name not in variant_data['metrics']:
            variant_data['metrics'][metric_name] = []
        
        variant_data['metrics'][metric_name].append({
            'value': value,
            'timestamp': datetime.now(),
            'user_id': user_id
        })
        
        variant_data['samples'] += 1
    
    async def _monitor_test(self, test_id: str):
        """í…ŒìŠ¤íŠ¸ ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        
        config = self.active_tests[test_id]
        test_data = self.test_data[test_id]
        
        end_time = test_data['actual_start_time'] + timedelta(days=config.duration_days)
        
        while test_data['status'] == TestStatus.RUNNING and datetime.now() < end_time:
            
            # í†µê³„ì  ìœ ì˜ì„± ê²€ì •
            if config.early_stopping:
                significance_result = await self._check_statistical_significance(test_id)
                
                if significance_result['significant']:
                    await self._stop_test(test_id, "statistical_significance")
                    break
            
            # í‘œë³¸ í¬ê¸° ì²´í¬
            await self._check_sample_size_adequacy(test_id)
            
            # 1ì‹œê°„ë§ˆë‹¤ ì²´í¬
            await asyncio.sleep(3600)
        
        # í…ŒìŠ¤íŠ¸ ì¢…ë£Œ ì‹œê°„ ë„ë‹¬
        if test_data['status'] == TestStatus.RUNNING:
            await self._stop_test(test_id, "duration_completed")
    
    async def _check_statistical_significance(self, test_id: str) -> Dict[str, Any]:
        """í†µê³„ì  ìœ ì˜ì„± ê²€ì •"""
        
        config = self.active_tests[test_id]
        test_data = self.test_data[test_id]
        
        results = {
            'significant': False,
            'winning_variant': None,
            'p_value': 1.0,
            'effect_size': 0.0,
            'confidence_interval': None
        }
        
        # ì£¼ìš” ë©”íŠ¸ë¦­ì— ëŒ€í•´ ê²€ì • ìˆ˜í–‰
        primary_metrics = [m for m in config.metrics if m.primary]
        
        for metric in primary_metrics:
            # ê° ë³€í˜•ì˜ ë°ì´í„° ìˆ˜ì§‘
            variant_data = {}
            for variant in config.variants:
                metrics_data = test_data['variant_data'][variant.name]['metrics']
                if metric.name in metrics_data:
                    values = [d['value'] for d in metrics_data[metric.name]]
                    variant_data[variant.name] = values
            
            if len(variant_data) < 2:
                continue
            
            # ë³€í˜•ë“¤ ê°„ ë¹„êµ (ì²« ë²ˆì§¸ë¥¼ ì»¨íŠ¸ë¡¤ë¡œ ê°€ì •)
            variants = list(variant_data.keys())
            control_data = variant_data[variants[0]]
            
            for i in range(1, len(variants)):
                treatment_data = variant_data[variants[i]]
                
                if len(control_data) < 30 or len(treatment_data) < 30:
                    continue  # í‘œë³¸ í¬ê¸° ë¶€ì¡±
                
                # í†µê³„ì  ê²€ì • ìˆ˜í–‰
                if config.significance_method == SignificanceMethod.T_TEST:
                    statistic, p_value = stats.ttest_ind(control_data, treatment_data)
                elif config.significance_method == SignificanceMethod.MANN_WHITNEY:
                    statistic, p_value = stats.mannwhitneyu(control_data, treatment_data)
                
                # íš¨ê³¼ í¬ê¸° ê³„ì‚° (Cohen's d)
                pooled_std = np.sqrt(((len(control_data) - 1) * np.var(control_data) + 
                                    (len(treatment_data) - 1) * np.var(treatment_data)) / 
                                   (len(control_data) + len(treatment_data) - 2))
                
                effect_size = (np.mean(treatment_data) - np.mean(control_data)) / pooled_std
                
                # ìœ ì˜ì„± íŒì •
                alpha = 1 - config.confidence_level
                if p_value < alpha and abs(effect_size) >= metric.min_detectable_effect:
                    results['significant'] = True
                    results['winning_variant'] = variants[i] if effect_size > 0 else variants[0]
                    results['p_value'] = p_value
                    results['effect_size'] = effect_size
                    
                    # ì‹ ë¢°êµ¬ê°„ ê³„ì‚°
                    se = pooled_std * np.sqrt(1/len(control_data) + 1/len(treatment_data))
                    margin_of_error = stats.t.ppf(1 - alpha/2, len(control_data) + len(treatment_data) - 2) * se
                    
                    mean_diff = np.mean(treatment_data) - np.mean(control_data)
                    results['confidence_interval'] = (
                        mean_diff - margin_of_error,
                        mean_diff + margin_of_error
                    )
                    
                    break
            
            if results['significant']:
                break
        
        return results
    
    async def _stop_test(self, test_id: str, reason: str):
        """í…ŒìŠ¤íŠ¸ ì¤‘ì§€"""
        
        config = self.active_tests[test_id]
        test_data = self.test_data[test_id]
        
        test_data['status'] = TestStatus.COMPLETED
        test_data['end_time'] = datetime.now()
        test_data['stop_reason'] = reason
        
        # ìµœì¢… ê²°ê³¼ ë¶„ì„
        final_results = await self._analyze_final_results(test_id)
        test_data['final_results'] = final_results
        
        # ìŠ¹ë¦¬ ë³€í˜• ë°°í¬ (í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ê²½ìš°)
        if final_results['significant']:
            await self._deploy_winning_variant(test_id, final_results['winning_variant'])
        
        # í…ŒìŠ¤íŠ¸ ë³€í˜•ë“¤ ì •ë¦¬
        await self._cleanup_test_variants(test_id)
        
        # ì™„ë£Œ ì•Œë¦¼
        await self._send_test_notification(test_id, "completed", final_results)
        
        self.logger.info(f"A/B test completed: {test_id}, reason: {reason}")
    
    async def _analyze_final_results(self, test_id: str) -> Dict[str, Any]:
        """ìµœì¢… ê²°ê³¼ ë¶„ì„"""
        
        # í†µê³„ì  ìœ ì˜ì„± ì¬í™•ì¸
        significance_result = await self._check_statistical_significance(test_id)
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ê³„ì‚°
        config = self.active_tests[test_id]
        test_data = self.test_data[test_id]
        
        variant_summaries = {}
        
        for variant in config.variants:
            variant_data = test_data['variant_data'][variant.name]
            summary = {
                'sample_size': variant_data['samples'],
                'metrics': {}
            }
            
            for metric in config.metrics:
                if metric.name in variant_data['metrics']:
                    values = [d['value'] for d in variant_data['metrics'][metric.name]]
                    summary['metrics'][metric.name] = {
                        'mean': np.mean(values),
                        'std': np.std(values),
                        'count': len(values),
                        'confidence_interval': self._calculate_confidence_interval(values, config.confidence_level)
                    }
            
            variant_summaries[variant.name] = summary
        
        return {
            **significance_result,
            'variant_summaries': variant_summaries,
            'test_duration_hours': (test_data['end_time'] - test_data['actual_start_time']).total_seconds() / 3600
        }
    
    def _calculate_confidence_interval(self, values: List[float], confidence_level: float) -> tuple:
        """ì‹ ë¢°êµ¬ê°„ ê³„ì‚°"""
        if len(values) < 2:
            return (0, 0)
        
        alpha = 1 - confidence_level
        mean = np.mean(values)
        se = stats.sem(values)
        margin_of_error = stats.t.ppf(1 - alpha/2, len(values) - 1) * se
        
        return (mean - margin_of_error, mean + margin_of_error)
    
    async def _deploy_winning_variant(self, test_id: str, winning_variant: str):
        """ìŠ¹ë¦¬ ë³€í˜• ë°°í¬"""
        
        config = self.active_tests[test_id]
        
        # ìŠ¹ë¦¬ ë³€í˜• ì°¾ê¸°
        winner = next((v for v in config.variants if v.name == winning_variant), None)
        if not winner:
            return
        
        # í”„ë¡œë•ì…˜ì— ë°°í¬
        await self.strategy_manager.deploy_strategy_to_production(
            winner.strategy_name, winner.parameters
        )
        
        self.logger.info(f"Deployed winning variant: {winning_variant} for test {test_id}")
    
    async def _cleanup_test_variants(self, test_id: str):
        """í…ŒìŠ¤íŠ¸ ë³€í˜•ë“¤ ì •ë¦¬"""
        
        config = self.active_tests[test_id]
        
        for variant in config.variants:
            variant_id = f"{test_id}_{variant.name}"
            await self.strategy_manager.remove_strategy_variant(variant_id)
        
        # í• ë‹¹ ìºì‹œ ì •ë¦¬
        cache_keys_to_remove = [k for k in self.allocation_cache.keys() if k.startswith(f"{test_id}_")]
        for key in cache_keys_to_remove:
            del self.allocation_cache[key]
    
    async def _send_test_notification(self, test_id: str, event_type: str, results: Dict = None):
        """í…ŒìŠ¤íŠ¸ ì´ë²¤íŠ¸ ì•Œë¦¼"""
        
        config = self.active_tests[test_id]
        
        if event_type == "started":
            title = f"ğŸ§ª A/B í…ŒìŠ¤íŠ¸ ì‹œì‘: {config.name}"
            message = f"í…ŒìŠ¤íŠ¸ ID: {test_id}\nê¸°ê°„: {config.duration_days}ì¼\në³€í˜• ìˆ˜: {len(config.variants)}"
            priority = NotificationPriority.LOW
            
        elif event_type == "completed":
            title = f"âœ… A/B í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {config.name}"
            
            if results and results.get('significant'):
                message = f"""
í…ŒìŠ¤íŠ¸ ID: {test_id}
ê²°ê³¼: í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•¨ (p={results['p_value']:.4f})
ìŠ¹ë¦¬ ë³€í˜•: {results['winning_variant']}
íš¨ê³¼ í¬ê¸°: {results['effect_size']:.4f}
                """.strip()
                priority = NotificationPriority.MEDIUM
            else:
                message = f"""
í…ŒìŠ¤íŠ¸ ID: {test_id}
ê²°ê³¼: í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ì§€ ì•ŠìŒ
ê²°ë¡ : ë³€í˜•ë“¤ ê°„ ì˜ë¯¸ìˆëŠ” ì°¨ì´ ì—†ìŒ
                """.strip()
                priority = NotificationPriority.LOW
        
        notification = Notification(
            title=title,
            message=message,
            priority=priority,
            tags=['ab_test', test_id]
        )
        
        await self.notification_system.route_notification(notification)

---

## ğŸ” **ë³´ì•ˆ ë° ì»´í”Œë¼ì´ì–¸ìŠ¤**

### ğŸ” **API í‚¤ ê´€ë¦¬ (HashiCorp Vault)**

```python
# automation/secrets_management.py
import hvac
import asyncio
import aiohttp
from typing import Dict, Optional, Any
import logging
import time
from datetime import datetime, timedelta
import json
import os

class VaultSecretManager:
    """HashiCorp Vault ê¸°ë°˜ ë¹„ë°€ ê´€ë¦¬"""
    
    def __init__(self, vault_url: str, vault_token: str = None):
        self.vault_url = vault_url
        self.client = hvac.Client(url=vault_url, token=vault_token)
        self.secret_cache = {}
        self.cache_ttl = 300  # 5ë¶„
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    async def authenticate_with_kubernetes(self, role: str, jwt_path: str = "/var/run/secrets/kubernetes.io/serviceaccount/token"):
        """Kubernetes ì„œë¹„ìŠ¤ ì–´ì¹´ìš´íŠ¸ë¥¼ í†µí•œ ì¸ì¦"""
        try:
            with open(jwt_path, 'r') as f:
                jwt_token = f.read()
            
            auth_response = self.client.auth.kubernetes.login(
                role=role,
                jwt=jwt_token
            )
            
            self.client.token = auth_response['auth']['client_token']
            self.logger.info("Successfully authenticated with Vault using Kubernetes")
            
            return True
            
        except Exception as e:
            self.logger.error(f"Kubernetes authentication failed: {e}")
            return False
    
    async def get_secret(self, path: str, force_refresh: bool = False) -> Optional[Dict[str, Any]]:
        """ë¹„ë°€ ì •ë³´ ì¡°íšŒ"""
        
        cache_key = f"secret_{path}"
        
        # ìºì‹œ í™•ì¸ (ê°•ì œ ìƒˆë¡œê³ ì¹¨ì´ ì•„ë‹Œ ê²½ìš°)
        if not force_refresh and cache_key in self.secret_cache:
            cached_data = self.secret_cache[cache_key]
            if time.time() - cached_data['timestamp'] < self.cache_ttl:
                return cached_data['data']
        
        try:
            # Vaultì—ì„œ ë¹„ë°€ ì •ë³´ ì¡°íšŒ
            response = self.client.secrets.kv.v2.read_secret_version(path=path)
            secret_data = response['data']['data']
            
            # ìºì‹œì— ì €ì¥
            self.secret_cache[cache_key] = {
                'data': secret_data,
                'timestamp': time.time()
            }
            
            self.logger.info(f"Retrieved secret from path: {path}")
            return secret_data
            
        except Exception as e:
            self.logger.error(f"Failed to retrieve secret from {path}: {e}")
            return None
    
    async def store_secret(self, path: str, secret_data: Dict[str, Any]) -> bool:
        """ë¹„ë°€ ì •ë³´ ì €ì¥"""
        try:
            self.client.secrets.kv.v2.create_or_update_secret(
                path=path,
                secret=secret_data
            )
            
            # ìºì‹œ ì—…ë°ì´íŠ¸
            cache_key = f"secret_{path}"
            self.secret_cache[cache_key] = {
                'data': secret_data,
                'timestamp': time.time()
            }
            
            self.logger.info(f"Stored secret at path: {path}")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to store secret at {path}: {e}")
            return False
    
    async def rotate_secret(self, path: str, rotation_function) -> bool:
        """ë¹„ë°€ ì •ë³´ ìˆœí™˜"""
        try:
            # í˜„ì¬ ë¹„ë°€ ì •ë³´ ì¡°íšŒ
            current_secret = await self.get_secret(path)
            if not current_secret:
                self.logger.error(f"Cannot rotate secret - current secret not found at {path}")
                return False
            
            # ìƒˆ ë¹„ë°€ ì •ë³´ ìƒì„±
            new_secret = await rotation_function(current_secret)
            
            # ìƒˆ ë¹„ë°€ ì •ë³´ ì €ì¥
            success = await self.store_secret(path, new_secret)
            
            if success:
                self.logger.info(f"Successfully rotated secret at {path}")
            
            return success
            
        except Exception as e:
            self.logger.error(f"Secret rotation failed for {path}: {e}")
            return False
    
    async def delete_secret(self, path: str) -> bool:
        """ë¹„ë°€ ì •ë³´ ì‚­ì œ"""
        try:
            self.client.secrets.kv.v2.delete_metadata_and_all_versions(path=path)
            
            # ìºì‹œì—ì„œë„ ì œê±°
            cache_key = f"secret_{path}"
            if cache_key in self.secret_cache:
                del self.secret_cache[cache_key]
            
            self.logger.info(f"Deleted secret at path: {path}")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to delete secret at {path}: {e}")
            return False

class APIKeyManager:
    """API í‚¤ ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, vault_manager: VaultSecretManager):
        self.vault_manager = vault_manager
        self.key_rotation_schedule = {}
        self.key_usage_tracking = {}
        
    async def get_binance_credentials(self, environment: str = "production") -> Optional[Dict[str, str]]:
        """Binance API ìê²©ì¦ëª… ì¡°íšŒ"""
        path = f"freqtrade/{environment}/binance"
        
        credentials = await self.vault_manager.get_secret(path)
        if not credentials:
            return None
        
        # ì‚¬ìš©ëŸ‰ ì¶”ì 
        self._track_key_usage(path)
        
        return {
            'api_key': credentials.get('api_key'),
            'api_secret': credentials.get('api_secret'),
            'testnet': credentials.get('testnet', False)
        }
    
    async def rotate_binance_keys(self, environment: str = "production") -> bool:
        """Binance API í‚¤ ìˆœí™˜"""
        
        def rotation_function(current_credentials):
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Binance APIë¥¼ í†µí•´ ìƒˆ í‚¤ ìƒì„±
            # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œ ë¡œì§
            return {
                'api_key': f"new_api_key_{int(time.time())}",
                'api_secret': f"new_api_secret_{int(time.time())}",
                'testnet': current_credentials.get('testnet', False),
                'created_at': datetime.now().isoformat(),
                'previous_key': current_credentials.get('api_key')
            }
        
        path = f"freqtrade/{environment}/binance"
        return await self.vault_manager.rotate_secret(path, rotation_function)
    
    async def schedule_key_rotation(self, key_path: str, rotation_interval_days: int):
        """í‚¤ ìˆœí™˜ ìŠ¤ì¼€ì¤„ë§"""
        next_rotation = datetime.now() + timedelta(days=rotation_interval_days)
        
        self.key_rotation_schedule[key_path] = {
            'next_rotation': next_rotation,
            'interval_days': rotation_interval_days
        }
        
        # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ ìˆœí™˜ ëª¨ë‹ˆí„°ë§
        asyncio.create_task(self._monitor_key_rotation(key_path))
    
    async def _monitor_key_rotation(self, key_path: str):
        """í‚¤ ìˆœí™˜ ëª¨ë‹ˆí„°ë§"""
        while key_path in self.key_rotation_schedule:
            schedule_info = self.key_rotation_schedule[key_path]
            
            if datetime.now() >= schedule_info['next_rotation']:
                # í‚¤ ìˆœí™˜ ì‹¤í–‰
                if "binance" in key_path:
                    environment = key_path.split('/')[1]
                    await self.rotate_binance_keys(environment)
                
                # ë‹¤ìŒ ìˆœí™˜ ì‹œê°„ ì—…ë°ì´íŠ¸
                schedule_info['next_rotation'] = datetime.now() + timedelta(
                    days=schedule_info['interval_days']
                )
            
            # 1ì‹œê°„ë§ˆë‹¤ ì²´í¬
            await asyncio.sleep(3600)
    
    def _track_key_usage(self, key_path: str):
        """í‚¤ ì‚¬ìš©ëŸ‰ ì¶”ì """
        if key_path not in self.key_usage_tracking:
            self.key_usage_tracking[key_path] = {
                'usage_count': 0,
                'last_used': None,
                'first_used': datetime.now()
            }
        
        tracking_info = self.key_usage_tracking[key_path]
        tracking_info['usage_count'] += 1
        tracking_info['last_used'] = datetime.now()
    
    async def get_key_usage_stats(self) -> Dict[str, Dict]:
        """í‚¤ ì‚¬ìš©ëŸ‰ í†µê³„ ì¡°íšŒ"""
        return self.key_usage_tracking.copy()

### ğŸŒ **ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ ì„¤ì •**

```python
# automation/network_security.py
import asyncio
import subprocess
import ipaddress
from typing import List, Dict, Optional, Set
import logging
import json
from dataclasses import dataclass
from datetime import datetime, timedelta

@dataclass
class FirewallRule:
    """ë°©í™”ë²½ ê·œì¹™"""
    name: str
    source_ip: str
    destination_port: int
    protocol: str = "tcp"
    action: str = "allow"  # allow, deny
    priority: int = 100

@dataclass
class RateLimitRule:
    """ì†ë„ ì œí•œ ê·œì¹™"""
    endpoint: str
    requests_per_minute: int
    burst_size: int = 10
    whitelist_ips: List[str] = None

class NetworkSecurityManager:
    """ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ ê´€ë¦¬ì"""
    
    def __init__(self, notification_system):
        self.notification_system = notification_system
        self.firewall_rules: List[FirewallRule] = []
        self.rate_limit_rules: List[RateLimitRule] = []
        self.blocked_ips: Set[str] = set()
        self.connection_tracking: Dict[str, Dict] = {}
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    async def setup_firewall_rules(self):
        """ê¸°ë³¸ ë°©í™”ë²½ ê·œì¹™ ì„¤ì •"""
        
        # ê¸°ë³¸ ê·œì¹™ë“¤
        default_rules = [
            FirewallRule("ssh_access", "0.0.0.0/0", 22, "tcp", "allow", 50),
            FirewallRule("http_access", "0.0.0.0/0", 80, "tcp", "allow", 100),
            FirewallRule("https_access", "0.0.0.0/0", 443, "tcp", "allow", 100),
            FirewallRule("freqtrade_api", "10.0.0.0/8", 8080, "tcp", "allow", 200),
            FirewallRule("database_access", "10.0.0.0/8", 5432, "tcp", "allow", 300),
            FirewallRule("redis_access", "10.0.0.0/8", 6379, "tcp", "allow", 300),
            FirewallRule("deny_all", "0.0.0.0/0", 0, "tcp", "deny", 1000)
        ]
        
        for rule in default_rules:
            await self.add_firewall_rule(rule)
    
    async def add_firewall_rule(self, rule: FirewallRule) -> bool:
        """ë°©í™”ë²½ ê·œì¹™ ì¶”ê°€"""
        try:
            # iptables ê·œì¹™ ìƒì„±
            if rule.action == "allow":
                action = "ACCEPT"
            else:
                action = "DROP"
            
            # ê·œì¹™ ì¶”ê°€ ëª…ë ¹
            if rule.destination_port > 0:
                cmd = [
                    "iptables", "-A", "INPUT",
                    "-s", rule.source_ip,
                    "-p", rule.protocol,
                    "--dport", str(rule.destination_port),
                    "-j", action,
                    "-m", "comment", "--comment", rule.name
                ]
            else:
                cmd = [
                    "iptables", "-A", "INPUT",
                    "-s", rule.source_ip,
                    "-j", action,
                    "-m", "comment", "--comment", rule.name
                ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                self.firewall_rules.append(rule)
                self.logger.info(f"Added firewall rule: {rule.name}")
                return True
            else:
                self.logger.error(f"Failed to add firewall rule: {result.stderr}")
                return False
                
        except Exception as e:
            self.logger.error(f"Error adding firewall rule: {e}")
            return False
    
    async def block_ip_address(self, ip_address: str, duration_minutes: int = 60, reason: str = ""):
        """IP ì£¼ì†Œ ì°¨ë‹¨"""
        try:
            # IP ì£¼ì†Œ ìœ íš¨ì„± ê²€ì¦
            ipaddress.ip_address(ip_address)
            
            # iptables ê·œì¹™ ì¶”ê°€
            cmd = [
                "iptables", "-A", "INPUT",
                "-s", ip_address,
                "-j", "DROP",
                "-m", "comment", "--comment", f"blocked_{reason}"
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                self.blocked_ips.add(ip_address)
                
                # ìë™ í•´ì œ ìŠ¤ì¼€ì¤„ë§
                if duration_minutes > 0:
                    asyncio.create_task(
                        self._schedule_ip_unblock(ip_address, duration_minutes)
                    )
                
                # ì°¨ë‹¨ ì•Œë¦¼
                await self._send_security_alert(
                    "IP_BLOCKED",
                    f"IP ì£¼ì†Œ ì°¨ë‹¨: {ip_address}",
                    f"ì‚¬ìœ : {reason}\nì§€ì†ì‹œê°„: {duration_minutes}ë¶„"
                )
                
                self.logger.info(f"Blocked IP address: {ip_address} for {duration_minutes} minutes")
                return True
            else:
                self.logger.error(f"Failed to block IP: {result.stderr}")
                return False
                
        except Exception as e:
            self.logger.error(f"Error blocking IP {ip_address}: {e}")
            return False
    
    async def _schedule_ip_unblock(self, ip_address: str, duration_minutes: int):
        """IP ì°¨ë‹¨ í•´ì œ ìŠ¤ì¼€ì¤„ë§"""
        await asyncio.sleep(duration_minutes * 60)
        await self.unblock_ip_address(ip_address)
    
    async def unblock_ip_address(self, ip_address: str) -> bool:
        """IP ì£¼ì†Œ ì°¨ë‹¨ í•´ì œ"""
        try:
            # iptables ê·œì¹™ ì œê±°
            cmd = [
                "iptables", "-D", "INPUT",
                "-s", ip_address,
                "-j", "DROP"
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                self.blocked_ips.discard(ip_address)
                self.logger.info(f"Unblocked IP address: {ip_address}")
                return True
            else:
                self.logger.warning(f"Failed to unblock IP (may not be blocked): {result.stderr}")
                return False
                
        except Exception as e:
            self.logger.error(f"Error unblocking IP {ip_address}: {e}")
            return False
    
    async def setup_rate_limiting(self):
        """ì†ë„ ì œí•œ ì„¤ì •"""
        
        # Nginx ì„¤ì •ì„ í†µí•œ ì†ë„ ì œí•œ
        nginx_config = """
# Rate limiting zones
limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/m;
limit_req_zone $binary_remote_addr zone=login_limit:10m rate=5r/m;
limit_req_zone $binary_remote_addr zone=general_limit:10m rate=60r/m;

server {
    listen 80;
    server_name freqtrade-api.local;
    
    # API ì—”ë“œí¬ì¸íŠ¸ ì†ë„ ì œí•œ
    location /api/ {
        limit_req zone=api_limit burst=5 nodelay;
        limit_req_status 429;
        proxy_pass http://freqtrade-backend;
    }
    
    # ë¡œê·¸ì¸ ì—”ë“œí¬ì¸íŠ¸ ì†ë„ ì œí•œ
    location /api/v1/token/login {
        limit_req zone=login_limit burst=2 nodelay;
        limit_req_status 429;
        proxy_pass http://freqtrade-backend;
    }
    
    # ì¼ë°˜ ìš”ì²­ ì†ë„ ì œí•œ
    location / {
        limit_req zone=general_limit burst=20 nodelay;
        limit_req_status 429;
        proxy_pass http://freqtrade-backend;
    }
}
        """
        
        # ì„¤ì • íŒŒì¼ ì €ì¥
        with open('/etc/nginx/sites-available/freqtrade-rate-limit', 'w') as f:
            f.write(nginx_config)
        
        # Nginx ì¬ë¡œë“œ
        subprocess.run(['nginx', '-s', 'reload'])
        
        self.logger.info("Rate limiting configured")
    
    async def monitor_suspicious_activity(self):
        """ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í™œë™ ëª¨ë‹ˆí„°ë§"""
        while True:
            try:
                # ë¡œê·¸ íŒŒì¼ ë¶„ì„
                await self._analyze_access_logs()
                
                # ì—°ê²° ì¶”ì  ë¶„ì„
                await self._analyze_connection_patterns()
                
                # ë¹„ì •ìƒì ì¸ API ì‚¬ìš© íŒ¨í„´ ê°ì§€
                await self._detect_api_abuse()
                
                await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬
                
            except Exception as e:
                self.logger.error(f"Security monitoring error: {e}")
                await asyncio.sleep(300)  # ì˜¤ë¥˜ ì‹œ 5ë¶„ ëŒ€ê¸°
    
    async def _analyze_access_logs(self):
        """ì ‘ê·¼ ë¡œê·¸ ë¶„ì„"""
        try:
            # Nginx ì ‘ê·¼ ë¡œê·¸ ë¶„ì„
            with open('/var/log/nginx/access.log', 'r') as f:
                lines = f.readlines()[-1000:]  # ìµœê·¼ 1000ì¤„
            
            suspicious_patterns = [
                'sql injection', 'script>', '<script', 'union select',
                '../../', 'cmd.exe', '/bin/bash', 'wget', 'curl'
            ]
            
            for line in lines:
                for pattern in suspicious_patterns:
                    if pattern.lower() in line.lower():
                        # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ìš”ì²­ ê°ì§€
                        ip_address = line.split()[0]
                        await self._handle_suspicious_activity(
                            ip_address, f"Suspicious pattern detected: {pattern}", line
                        )
                        break
                        
        except Exception as e:
            self.logger.error(f"Log analysis error: {e}")
    
    async def _analyze_connection_patterns(self):
        """ì—°ê²° íŒ¨í„´ ë¶„ì„"""
        try:
            # netstatì„ í†µí•œ ì—°ê²° ë¶„ì„
            result = subprocess.run(
                ['netstat', '-tn'], 
                capture_output=True, text=True
            )
            
            if result.returncode != 0:
                return
            
            connections = {}
            for line in result.stdout.split('\n'):
                if 'ESTABLISHED' in line:
                    parts = line.split()
                    if len(parts) >= 5:
                        foreign_address = parts[4].split(':')[0]
                        if foreign_address not in connections:
                            connections[foreign_address] = 0
                        connections[foreign_address] += 1
            
            # ë¹„ì •ìƒì ìœ¼ë¡œ ë§ì€ ì—°ê²°ì„ ê°€ì§„ IP ê°ì§€
            for ip, count in connections.items():
                if count > 50:  # 50ê°œ ì´ìƒ ì—°ê²°
                    await self._handle_suspicious_activity(
                        ip, f"Too many connections: {count}", ""
                    )
                    
        except Exception as e:
            self.logger.error(f"Connection pattern analysis error: {e}")
    
    async def _detect_api_abuse(self):
        """API ë‚¨ìš© ê°ì§€"""
        # API ì‚¬ìš©ëŸ‰ ë¡œê·¸ ë¶„ì„
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Freqtrade API ë¡œê·¸ë‚˜ ë©”íŠ¸ë¦­ ë¶„ì„
        pass
    
    async def _handle_suspicious_activity(self, ip_address: str, reason: str, details: str):
        """ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í™œë™ ì²˜ë¦¬"""
        
        # IP ì£¼ì†Œê°€ ì´ë¯¸ ì°¨ë‹¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if ip_address in self.blocked_ips:
            return
        
        # ë‚´ë¶€ IPëŠ” ìŠ¤í‚µ
        try:
            ip_obj = ipaddress.ip_address(ip_address)
            if ip_obj.is_private or ip_obj.is_loopback:
                return
        except:
            return
        
        # IP ì°¨ë‹¨
        await self.block_ip_address(ip_address, duration_minutes=60, reason=reason)
        
        # ë³´ì•ˆ ì•Œë¦¼ ì „ì†¡
        await self._send_security_alert(
            "SUSPICIOUS_ACTIVITY",
            f"ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í™œë™ ê°ì§€: {ip_address}",
            f"ì‚¬ìœ : {reason}\nìƒì„¸: {details[:500]}"
        )
    
    async def _send_security_alert(self, alert_type: str, title: str, message: str):
        """ë³´ì•ˆ ì•Œë¦¼ ì „ì†¡"""
        notification = Notification(
            title=f"ğŸš¨ {title}",
            message=f"ìœ í˜•: {alert_type}\n\n{message}",
            priority=NotificationPriority.HIGH,
            tags=['security', alert_type.lower()]
        )
        
        await self.notification_system.route_notification(notification)

### ğŸ“‹ **ê°ì‚¬ ë¡œê·¸ ì‹œìŠ¤í…œ**

```python
# automation/audit_logging.py
import asyncio
import json
import hashlib
import time
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
from enum import Enum
import logging
import os
from dataclasses import dataclass, asdict

class AuditEventType(Enum):
    """ê°ì‚¬ ì´ë²¤íŠ¸ íƒ€ì…"""
    USER_LOGIN = "user_login"
    USER_LOGOUT = "user_logout"
    API_KEY_ACCESS = "api_key_access"
    STRATEGY_CHANGE = "strategy_change"
    TRADE_EXECUTION = "trade_execution"
    CONFIGURATION_CHANGE = "configuration_change"
    SYSTEM_START = "system_start"
    SYSTEM_STOP = "system_stop"
    SECURITY_ALERT = "security_alert"
    DATA_ACCESS = "data_access"
    BACKUP_OPERATION = "backup_operation"
    ADMIN_ACTION = "admin_action"

class AuditLevel(Enum):
    """ê°ì‚¬ ë ˆë²¨"""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"

@dataclass
class AuditEvent:
    """ê°ì‚¬ ì´ë²¤íŠ¸"""
    event_id: str
    timestamp: datetime
    event_type: AuditEventType
    level: AuditLevel
    user_id: Optional[str]
    session_id: Optional[str]
    source_ip: Optional[str]
    user_agent: Optional[str]
    resource: str
    action: str
    details: Dict[str, Any]
    result: str  # success, failure, partial
    risk_score: int = 0  # 0-100
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        data = asdict(self)
        data['timestamp'] = self.timestamp.isoformat()
        data['event_type'] = self.event_type.value
        data['level'] = self.level.value
        return data
    
    def calculate_hash(self) -> str:
        """ì´ë²¤íŠ¸ ë¬´ê²°ì„± í•´ì‹œ ê³„ì‚°"""
        content = json.dumps(self.to_dict(), sort_keys=True)
        return hashlib.sha256(content.encode()).hexdigest()

class AuditLogger:
    """ê°ì‚¬ ë¡œê¹… ì‹œìŠ¤í…œ"""
    
    def __init__(self, log_directory: str = "/var/log/freqtrade/audit"):
        self.log_directory = log_directory
        self.current_log_file = None
        self.log_rotation_size = 100 * 1024 * 1024  # 100MB
        self.retention_days = 365  # 1ë…„
        self.event_buffer = []
        self.buffer_size = 100
        self.encryption_key = self._load_or_generate_encryption_key()
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(log_directory, exist_ok=True)
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì‹œì‘
        asyncio.create_task(self._background_tasks())
    
    def _load_or_generate_encryption_key(self) -> bytes:
        """ì•”í˜¸í™” í‚¤ ë¡œë“œ ë˜ëŠ” ìƒì„±"""
        key_file = os.path.join(self.log_directory, '.audit_key')
        
        if os.path.exists(key_file):
            with open(key_file, 'rb') as f:
                return f.read()
        else:
            # ìƒˆ í‚¤ ìƒì„±
            key = os.urandom(32)  # 256-bit key
            with open(key_file, 'wb') as f:
                f.write(key)
            os.chmod(key_file, 0o600)  # ì†Œìœ ìë§Œ ì½ê¸° ê°€ëŠ¥
            return key
    
    async def log_event(self, event: AuditEvent):
        """ê°ì‚¬ ì´ë²¤íŠ¸ ë¡œê¹…"""
        
        # ì´ë²¤íŠ¸ ID ìƒì„± (ì—†ëŠ” ê²½ìš°)
        if not event.event_id:
            event.event_id = self._generate_event_id()
        
        # ë²„í¼ì— ì¶”ê°€
        self.event_buffer.append(event)
        
        # ë²„í¼ê°€ ê°€ë“ ì°¬ ê²½ìš° ì¦‰ì‹œ í”ŒëŸ¬ì‹œ
        if len(self.event_buffer) >= self.buffer_size:
            await self._flush_buffer()
        
        # ì¤‘ìš”í•œ ì´ë²¤íŠ¸ëŠ” ì¦‰ì‹œ ê¸°ë¡
        if event.level in [AuditLevel.ERROR, AuditLevel.CRITICAL] or event.risk_score > 70:
            await self._flush_buffer()
    
    def _generate_event_id(self) -> str:
        """ì´ë²¤íŠ¸ ID ìƒì„±"""
        timestamp = int(time.time() * 1000000)  # ë§ˆì´í¬ë¡œì´ˆ
        return f"audit_{timestamp}_{os.urandom(4).hex()}"
    
    async def _flush_buffer(self):
        """ë²„í¼ í”ŒëŸ¬ì‹œ"""
        if not self.event_buffer:
            return
        
        try:
            current_file = await self._get_current_log_file()
            
            with open(current_file, 'a', encoding='utf-8') as f:
                for event in self.event_buffer:
                    # ì´ë²¤íŠ¸ ë¬´ê²°ì„± í•´ì‹œ ì¶”ê°€
                    event_data = event.to_dict()
                    event_data['integrity_hash'] = event.calculate_hash()
                    
                    # JSON ë¼ì¸ìœ¼ë¡œ ê¸°ë¡
                    f.write(json.dumps(event_data) + '\n')
            
            self.logger.info(f"Flushed {len(self.event_buffer)} audit events")
            self.event_buffer.clear()
            
        except Exception as e:
            self.logger.error(f"Failed to flush audit buffer: {e}")
    
    async def _get_current_log_file(self) -> str:
        """í˜„ì¬ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        today = datetime.now().strftime('%Y-%m-%d')
        log_file = os.path.join(self.log_directory, f"audit_{today}.jsonl")
        
        # íŒŒì¼ í¬ê¸° ì²´í¬ ë° ë¡œí…Œì´ì…˜
        if os.path.exists(log_file) and os.path.getsize(log_file) > self.log_rotation_size:
            # ë¡œí…Œì´ì…˜ ì‹¤í–‰
            timestamp = datetime.now().strftime('%H%M%S')
            rotated_file = os.path.join(self.log_directory, f"audit_{today}_{timestamp}.jsonl")
            os.rename(log_file, rotated_file)
        
        return log_file
    
    async def _background_tasks(self):
        """ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬"""
        while True:
            try:
                # ì£¼ê¸°ì  ë²„í¼ í”ŒëŸ¬ì‹œ (30ì´ˆë§ˆë‹¤)
                await asyncio.sleep(30)
                await self._flush_buffer()
                
                # ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì •ë¦¬ (1ì‹œê°„ë§ˆë‹¤)
                if int(time.time()) % 3600 < 30:
                    await self._cleanup_old_logs()
                
            except Exception as e:
                self.logger.error(f"Audit background task error: {e}")
                await asyncio.sleep(60)
    
    async def _cleanup_old_logs(self):
        """ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì •ë¦¬"""
        try:
            cutoff_date = datetime.now() - timedelta(days=self.retention_days)
            
            for filename in os.listdir(self.log_directory):
                if filename.startswith('audit_') and filename.endswith('.jsonl'):
                    file_path = os.path.join(self.log_directory, filename)
                    file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                    
                    if file_mtime < cutoff_date:
                        os.remove(file_path)
                        self.logger.info(f"Removed old audit log: {filename}")
                        
        except Exception as e:
            self.logger.error(f"Log cleanup error: {e}")
    
    async def search_events(self, 
                          start_time: datetime, 
                          end_time: datetime,
                          event_types: List[AuditEventType] = None,
                          user_id: str = None,
                          source_ip: str = None,
                          resource: str = None) -> List[AuditEvent]:
        """ê°ì‚¬ ì´ë²¤íŠ¸ ê²€ìƒ‰"""
        
        events = []
        
        # ê²€ìƒ‰ ëŒ€ìƒ ë‚ ì§œ ë²”ìœ„ì˜ ë¡œê·¸ íŒŒì¼ë“¤
        current_date = start_time.date()
        end_date = end_time.date()
        
        while current_date <= end_date:
            date_str = current_date.strftime('%Y-%m-%d')
            pattern = f"audit_{date_str}*.jsonl"
            
            for filename in os.listdir(self.log_directory):
                if filename.startswith(f"audit_{date_str}") and filename.endswith('.jsonl'):
                    file_path = os.path.join(self.log_directory, filename)
                    
                    with open(file_path, 'r', encoding='utf-8') as f:
                        for line in f:
                            try:
                                event_data = json.loads(line.strip())
                                event_time = datetime.fromisoformat(event_data['timestamp'])
                                
                                # ì‹œê°„ ë²”ìœ„ ì²´í¬
                                if not (start_time <= event_time <= end_time):
                                    continue
                                
                                # í•„í„° ì ìš©
                                if event_types and event_data['event_type'] not in [et.value for et in event_types]:
                                    continue
                                
                                if user_id and event_data.get('user_id') != user_id:
                                    continue
                                
                                if source_ip and event_data.get('source_ip') != source_ip:
                                    continue
                                
                                if resource and resource not in event_data.get('resource', ''):
                                    continue
                                
                                # AuditEvent ê°ì²´ë¡œ ë³€í™˜
                                event = AuditEvent(
                                    event_id=event_data['event_id'],
                                    timestamp=event_time,
                                    event_type=AuditEventType(event_data['event_type']),
                                    level=AuditLevel(event_data['level']),
                                    user_id=event_data.get('user_id'),
                                    session_id=event_data.get('session_id'),
                                    source_ip=event_data.get('source_ip'),
                                    user_agent=event_data.get('user_agent'),
                                    resource=event_data['resource'],
                                    action=event_data['action'],
                                    details=event_data.get('details', {}),
                                    result=event_data['result'],
                                    risk_score=event_data.get('risk_score', 0)
                                )
                                
                                events.append(event)
                                
                            except Exception as e:
                                self.logger.error(f"Error parsing audit event: {e}")
                                continue
            
            current_date += timedelta(days=1)
        
        # ì‹œê°„ìˆœ ì •ë ¬
        events.sort(key=lambda x: x.timestamp)
        
        return events
    
    async def verify_integrity(self, event: AuditEvent) -> bool:
        """ì´ë²¤íŠ¸ ë¬´ê²°ì„± ê²€ì¦"""
        calculated_hash = event.calculate_hash()
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì €ì¥ëœ í•´ì‹œì™€ ë¹„êµ
        return True  # ì„ì‹œë¡œ True ë°˜í™˜

class ComplianceReporter:
    """ì»´í”Œë¼ì´ì–¸ìŠ¤ ë³´ê³ ì„œ ìƒì„±ê¸°"""
    
    def __init__(self, audit_logger: AuditLogger):
        self.audit_logger = audit_logger
    
    async def generate_security_report(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """ë³´ì•ˆ ë³´ê³ ì„œ ìƒì„±"""
        
        # ë³´ì•ˆ ê´€ë ¨ ì´ë²¤íŠ¸ ì¡°íšŒ
        security_events = await self.audit_logger.search_events(
            start_date, end_date,
            event_types=[
                AuditEventType.USER_LOGIN,
                AuditEventType.API_KEY_ACCESS,
                AuditEventType.SECURITY_ALERT,
                AuditEventType.ADMIN_ACTION
            ]
        )
        
        # í†µê³„ ê³„ì‚°
        total_events = len(security_events)
        failed_logins = len([e for e in security_events 
                           if e.event_type == AuditEventType.USER_LOGIN and e.result == 'failure'])
        
        unique_ips = len(set(e.source_ip for e in security_events if e.source_ip))
        high_risk_events = len([e for e in security_events if e.risk_score > 70])
        
        # ì‹œê°„ë³„ ë¶„í¬
        hourly_distribution = {}
        for event in security_events:
            hour = event.timestamp.hour
            hourly_distribution[hour] = hourly_distribution.get(hour, 0) + 1
        
        return {
            'report_period': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat()
            },
            'summary': {
                'total_security_events': total_events,
                'failed_login_attempts': failed_logins,
                'unique_source_ips': unique_ips,
                'high_risk_events': high_risk_events
            },
            'hourly_distribution': hourly_distribution,
            'top_source_ips': self._get_top_source_ips(security_events),
            'recommendations': self._generate_security_recommendations(security_events)
        }
    
    def _get_top_source_ips(self, events: List[AuditEvent]) -> List[Dict[str, Any]]:
        """ìƒìœ„ ì†ŒìŠ¤ IP ë¶„ì„"""
        ip_counts = {}
        
        for event in events:
            if event.source_ip:
                if event.source_ip not in ip_counts:
                    ip_counts[event.source_ip] = {'count': 0, 'risk_events': 0}
                
                ip_counts[event.source_ip]['count'] += 1
                if event.risk_score > 50:
                    ip_counts[event.source_ip]['risk_events'] += 1
        
        # ìƒìœ„ 10ê°œ IP
        top_ips = sorted(ip_counts.items(), key=lambda x: x[1]['count'], reverse=True)[:10]
        
        return [{'ip': ip, 'count': data['count'], 'risk_events': data['risk_events']} 
                for ip, data in top_ips]
    
    def _generate_security_recommendations(self, events: List[AuditEvent]) -> List[str]:
        """ë³´ì•ˆ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        failed_logins = [e for e in events 
                        if e.event_type == AuditEventType.USER_LOGIN and e.result == 'failure']
        
        if len(failed_logins) > 50:
            recommendations.append("ë¡œê·¸ì¸ ì‹¤íŒ¨ íšŸìˆ˜ê°€ ë§ìŠµë‹ˆë‹¤. ê³„ì • ì ê¸ˆ ì •ì±…ì„ ê°•í™”í•˜ì„¸ìš”.")
        
        high_risk_events = [e for e in events if e.risk_score > 80]
        if len(high_risk_events) > 10:
            recommendations.append("ê³ ìœ„í—˜ ë³´ì•ˆ ì´ë²¤íŠ¸ê°€ ë‹¤ìˆ˜ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë³´ì•ˆ ëª¨ë‹ˆí„°ë§ì„ ê°•í™”í•˜ì„¸ìš”.")
        
        # ì•¼ê°„ í™œë™ ì²´í¬
        night_events = [e for e in events if 0 <= e.timestamp.hour <= 6]
        if len(night_events) > len(events) * 0.3:
            recommendations.append("ì•¼ê°„ ì‹œê°„ëŒ€ í™œë™ì´ ë§ìŠµë‹ˆë‹¤. ë¹„ì •ìƒì ì¸ ì ‘ê·¼ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
        
        return recommendations

---

## ğŸš€ **ìš´ì˜ ë° ìœ ì§€ë³´ìˆ˜ ìë™í™”**

### ğŸ’¾ **ìë™ ë°±ì—… ë° ë³µêµ¬**

```python
# automation/backup_automation.py
import asyncio
import os
import shutil
import tarfile
import gzip
import boto3
import subprocess
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
import logging
import json
from dataclasses import dataclass
from pathlib import Path

@dataclass
class BackupConfig:
    """ë°±ì—… ì„¤ì •"""
    name: str
    source_paths: List[str]
    destination: str
    schedule_cron: str
    retention_days: int
    compression: bool = True
    encryption: bool = True
    cloud_sync: bool = False
    verification: bool = True

class AutomatedBackupManager:
    """ìë™í™”ëœ ë°±ì—… ê´€ë¦¬ì"""
    
    def __init__(self, notification_system):
        self.notification_system = notification_system
        self.backup_configs: Dict[str, BackupConfig] = {}
        self.active_backups: Dict[str, asyncio.Task] = {}
        self.backup_history: List[Dict] = []
        
        # AWS S3 í´ë¼ì´ì–¸íŠ¸ (í´ë¼ìš°ë“œ ë°±ì—…ìš©)
        self.s3_client = None
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def setup_s3_client(self, aws_access_key: str, aws_secret_key: str, region: str = 'us-east-1'):
        """S3 í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"""
        self.s3_client = boto3.client(
            's3',
            aws_access_key_id=aws_access_key,
            aws_secret_access_key=aws_secret_key,
            region_name=region
        )
    
    async def register_backup_job(self, config: BackupConfig):
        """ë°±ì—… ì‘ì—… ë“±ë¡"""
        self.backup_configs[config.name] = config
        
        # ìŠ¤ì¼€ì¤„ëœ ë°±ì—… íƒœìŠ¤í¬ ì‹œì‘
        if config.name not in self.active_backups:
            task = asyncio.create_task(self._scheduled_backup_loop(config))
            self.active_backups[config.name] = task
        
        self.logger.info(f"Registered backup job: {config.name}")
    
    async def _scheduled_backup_loop(self, config: BackupConfig):
        """ìŠ¤ì¼€ì¤„ëœ ë°±ì—… ë£¨í”„"""
        while True:
            try:
                # ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ê³„ì‚° (ê°„ë‹¨í•œ êµ¬í˜„ - ì‹¤ì œë¡œëŠ” croniter ì‚¬ìš©)
                next_run = self._calculate_next_run_time(config.schedule_cron)
                
                # ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ê¹Œì§€ ëŒ€ê¸°
                wait_seconds = (next_run - datetime.now()).total_seconds()
                if wait_seconds > 0:
                    await asyncio.sleep(wait_seconds)
                
                # ë°±ì—… ì‹¤í–‰
                await self.execute_backup(config.name)
                
            except Exception as e:
                self.logger.error(f"Scheduled backup error for {config.name}: {e}")
                await asyncio.sleep(3600)  # ì˜¤ë¥˜ ì‹œ 1ì‹œê°„ ëŒ€ê¸°
    
    def _calculate_next_run_time(self, cron_expression: str) -> datetime:
        """ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ê³„ì‚° (ê°„ë‹¨í•œ êµ¬í˜„)"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” croniter ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
        # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ 1ì‹œê°„ í›„ ë°˜í™˜
        return datetime.now() + timedelta(hours=1)
    
    async def execute_backup(self, backup_name: str, force: bool = False) -> bool:
        """ë°±ì—… ì‹¤í–‰"""
        if backup_name not in self.backup_configs:
            self.logger.error(f"Backup config not found: {backup_name}")
            return False
        
        config = self.backup_configs[backup_name]
        backup_id = f"{backup_name}_{int(datetime.now().timestamp())}"
        
        try:
            self.logger.info(f"Starting backup: {backup_name}")
            
            # ë°±ì—… ì‹œì‘ ì•Œë¦¼
            await self._send_backup_notification(backup_name, "started", backup_id)
            
            # 1. ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
            backup_dir = await self._create_backup_directory(config, backup_id)
            
            # 2. ë°ì´í„° ë°±ì—…
            backup_files = await self._backup_data(config, backup_dir)
            
            # 3. ì••ì¶• ë° ì•”í˜¸í™”
            if config.compression or config.encryption:
                backup_archive = await self._compress_and_encrypt(backup_dir, config)
                
                # ì›ë³¸ ë””ë ‰í† ë¦¬ ì œê±°
                shutil.rmtree(backup_dir)
                backup_files = [backup_archive]
            
            # 4. ê²€ì¦
            if config.verification:
                verification_result = await self._verify_backup(backup_files)
                if not verification_result:
                    raise Exception("Backup verification failed")
            
            # 5. í´ë¼ìš°ë“œ ë™ê¸°í™”
            if config.cloud_sync and self.s3_client:
                await self._sync_to_cloud(backup_files, config)
            
            # 6. ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬
            await self._cleanup_old_backups(config)
            
            # ë°±ì—… ì™„ë£Œ ê¸°ë¡
            backup_record = {
                'backup_id': backup_id,
                'backup_name': backup_name,
                'timestamp': datetime.now().isoformat(),
                'status': 'completed',
                'files': backup_files,
                'size_mb': sum(os.path.getsize(f) for f in backup_files) / (1024 * 1024)
            }
            
            self.backup_history.append(backup_record)
            
            # ì™„ë£Œ ì•Œë¦¼
            await self._send_backup_notification(backup_name, "completed", backup_id, backup_record)
            
            self.logger.info(f"Backup completed successfully: {backup_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"Backup failed for {backup_name}: {e}")
            
            # ì‹¤íŒ¨ ì•Œë¦¼
            await self._send_backup_notification(backup_name, "failed", backup_id, {'error': str(e)})
            
            return False
    
    async def _create_backup_directory(self, config: BackupConfig, backup_id: str) -> str:
        """ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_dir = os.path.join(config.destination, f"{backup_id}_{timestamp}")
        
        os.makedirs(backup_dir, exist_ok=True)
        return backup_dir
    
    async def _backup_data(self, config: BackupConfig, backup_dir: str) -> List[str]:
        """ë°ì´í„° ë°±ì—…"""
        backup_files = []
        
        for source_path in config.source_paths:
            if not os.path.exists(source_path):
                self.logger.warning(f"Source path not found: {source_path}")
                continue
            
            # ì†ŒìŠ¤ ê²½ë¡œì˜ ë² ì´ìŠ¤ë„¤ì„ ì–»ê¸°
            basename = os.path.basename(source_path)
            dest_path = os.path.join(backup_dir, basename)
            
            if os.path.isfile(source_path):
                # íŒŒì¼ ë³µì‚¬
                shutil.copy2(source_path, dest_path)
                backup_files.append(dest_path)
                
            elif os.path.isdir(source_path):
                # ë””ë ‰í† ë¦¬ ë³µì‚¬
                shutil.copytree(source_path, dest_path)
                backup_files.append(dest_path)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… (PostgreSQL)
        if "database" in config.name.lower():
            await self._backup_database(backup_dir)
        
        return backup_files
    
    async def _backup_database(self, backup_dir: str):
        """ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            dump_file = os.path.join(backup_dir, f"database_dump_{timestamp}.sql")
            
            # PostgreSQL ë¤í”„
            cmd = [
                'pg_dump',
                '-h', 'localhost',
                '-U', 'freqtrade',
                '-d', 'freqtrade',
                '-f', dump_file,
                '--no-password'
            ]
            
            # í™˜ê²½ ë³€ìˆ˜ë¡œ ë¹„ë°€ë²ˆí˜¸ ì„¤ì •
            env = os.environ.copy()
            env['PGPASSWORD'] = os.getenv('DB_PASSWORD', '')
            
            result = subprocess.run(cmd, env=env, capture_output=True, text=True)
            
            if result.returncode == 0:
                self.logger.info(f"Database backup completed: {dump_file}")
            else:
                self.logger.error(f"Database backup failed: {result.stderr}")
                
        except Exception as e:
            self.logger.error(f"Database backup error: {e}")
    
    async def _compress_and_encrypt(self, backup_dir: str, config: BackupConfig) -> str:
        """ë°±ì—… ì••ì¶• ë° ì•”í˜¸í™”"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        archive_name = f"{config.name}_{timestamp}.tar.gz"
        archive_path = os.path.join(config.destination, archive_name)
        
        # tar.gz ì••ì¶•
        with tarfile.open(archive_path, 'w:gz') as tar:
            tar.add(backup_dir, arcname=os.path.basename(backup_dir))
        
        # ì•”í˜¸í™” (GPG ì‚¬ìš©)
        if config.encryption:
            encrypted_path = f"{archive_path}.gpg"
            
            cmd = [
                'gpg', '--cipher-algo', 'AES256',
                '--compress-algo', '1',
                '--symmetric',
                '--batch', '--yes',
                '--passphrase', os.getenv('BACKUP_PASSPHRASE', 'default_passphrase'),
                '--output', encrypted_path,
                archive_path
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                os.remove(archive_path)  # ì›ë³¸ ì••ì¶• íŒŒì¼ ì‚­ì œ
                archive_path = encrypted_path
            else:
                self.logger.error(f"Encryption failed: {result.stderr}")
        
        return archive_path
    
    async def _verify_backup(self, backup_files: List[str]) -> bool:
        """ë°±ì—… ê²€ì¦"""
        try:
            for backup_file in backup_files:
                if not os.path.exists(backup_file):
                    self.logger.error(f"Backup file not found: {backup_file}")
                    return False
                
                # íŒŒì¼ í¬ê¸° í™•ì¸
                file_size = os.path.getsize(backup_file)
                if file_size == 0:
                    self.logger.error(f"Backup file is empty: {backup_file}")
                    return False
                
                # ì••ì¶• íŒŒì¼ ë¬´ê²°ì„± ê²€ì‚¬
                if backup_file.endswith('.tar.gz'):
                    try:
                        with tarfile.open(backup_file, 'r:gz') as tar:
                            tar.getmembers()  # ì••ì¶• íŒŒì¼ êµ¬ì¡° í™•ì¸
                    except Exception as e:
                        self.logger.error(f"Backup file integrity check failed: {e}")
                        return False
            
            self.logger.info("Backup verification completed successfully")
            return True
            
        except Exception as e:
            self.logger.error(f"Backup verification error: {e}")
            return False
    
    async def _sync_to_cloud(self, backup_files: List[str], config: BackupConfig):
        """í´ë¼ìš°ë“œ ë™ê¸°í™”"""
        if not self.s3_client:
            self.logger.warning("S3 client not configured, skipping cloud sync")
            return
        
        bucket_name = os.getenv('BACKUP_S3_BUCKET', 'freqtrade-backups')
        
        for backup_file in backup_files:
            try:
                filename = os.path.basename(backup_file)
                s3_key = f"freqtrade/{config.name}/{filename}"
                
                self.s3_client.upload_file(backup_file, bucket_name, s3_key)
                
                self.logger.info(f"Uploaded to S3: {s3_key}")
                
            except Exception as e:
                self.logger.error(f"S3 upload failed for {backup_file}: {e}")
    
    async def _cleanup_old_backups(self, config: BackupConfig):
        """ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬"""
        try:
            cutoff_date = datetime.now() - timedelta(days=config.retention_days)
            
            # ë¡œì»¬ ë°±ì—… ì •ë¦¬
            for filename in os.listdir(config.destination):
                if filename.startswith(config.name):
                    file_path = os.path.join(config.destination, filename)
                    file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                    
                    if file_mtime < cutoff_date:
                        os.remove(file_path)
                        self.logger.info(f"Removed old backup: {filename}")
            
            # S3 ë°±ì—… ì •ë¦¬ (ìˆëŠ” ê²½ìš°)
            if config.cloud_sync and self.s3_client:
                await self._cleanup_s3_backups(config, cutoff_date)
                
        except Exception as e:
            self.logger.error(f"Backup cleanup error: {e}")
    
    async def _cleanup_s3_backups(self, config: BackupConfig, cutoff_date: datetime):
        """S3 ë°±ì—… ì •ë¦¬"""
        try:
            bucket_name = os.getenv('BACKUP_S3_BUCKET', 'freqtrade-backups')
            prefix = f"freqtrade/{config.name}/"
            
            response = self.s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)
            
            if 'Contents' in response:
                for obj in response['Contents']:
                    if obj['LastModified'].replace(tzinfo=None) < cutoff_date:
                        self.s3_client.delete_object(Bucket=bucket_name, Key=obj['Key'])
                        self.logger.info(f"Removed old S3 backup: {obj['Key']}")
                        
        except Exception as e:
            self.logger.error(f"S3 cleanup error: {e}")
    
    async def _send_backup_notification(self, backup_name: str, status: str, 
                                      backup_id: str, details: Dict = None):
        """ë°±ì—… ì•Œë¦¼ ì „ì†¡"""
        
        if status == "started":
            title = f"ğŸ’¾ ë°±ì—… ì‹œì‘: {backup_name}"
            message = f"ë°±ì—… ID: {backup_id}\nì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            priority = NotificationPriority.LOW
            
        elif status == "completed":
            title = f"âœ… ë°±ì—… ì™„ë£Œ: {backup_name}"
            message = f"ë°±ì—… ID: {backup_id}\n"
            if details:
                message += f"í¬ê¸°: {details.get('size_mb', 0):.2f} MB\n"
                message += f"íŒŒì¼ ìˆ˜: {len(details.get('files', []))}"
            priority = NotificationPriority.LOW
            
        elif status == "failed":
            title = f"âŒ ë°±ì—… ì‹¤íŒ¨: {backup_name}"
            message = f"ë°±ì—… ID: {backup_id}\n"
            if details and 'error' in details:
                message += f"ì˜¤ë¥˜: {details['error']}"
            priority = NotificationPriority.HIGH
        
        notification = Notification(
            title=title,
            message=message,
            priority=priority,
            tags=['backup', backup_name, status]
        )
        
        await self.notification_system.route_notification(notification)
    
    async def restore_from_backup(self, backup_id: str, restore_path: str) -> bool:
        """ë°±ì—…ì—ì„œ ë³µì›"""
        
        # ë°±ì—… ê¸°ë¡ì—ì„œ ì°¾ê¸°
        backup_record = next((r for r in self.backup_history if r['backup_id'] == backup_id), None)
        
        if not backup_record:
            self.logger.error(f"Backup record not found: {backup_id}")
            return False
        
        try:
            self.logger.info(f"Starting restore from backup: {backup_id}")
            
            backup_files = backup_record['files']
            
            for backup_file in backup_files:
                if backup_file.endswith('.gpg'):
                    # ë³µí˜¸í™”
                    decrypted_file = await self._decrypt_backup(backup_file)
                    backup_file = decrypted_file
                
                if backup_file.endswith('.tar.gz'):
                    # ì••ì¶• í•´ì œ
                    with tarfile.open(backup_file, 'r:gz') as tar:
                        tar.extractall(restore_path)
                else:
                    # ë‹¨ìˆœ íŒŒì¼ ë³µì‚¬
                    shutil.copy2(backup_file, restore_path)
            
            self.logger.info(f"Restore completed successfully to: {restore_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"Restore failed: {e}")
            return False
    
    async def _decrypt_backup(self, encrypted_file: str) -> str:
        """ë°±ì—… ë³µí˜¸í™”"""
        decrypted_file = encrypted_file[:-4]  # .gpg ì œê±°
        
        cmd = [
            'gpg', '--batch', '--yes',
            '--passphrase', os.getenv('BACKUP_PASSPHRASE', 'default_passphrase'),
            '--decrypt',
            '--output', decrypted_file,
            encrypted_file
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            return decrypted_file
        else:
            raise Exception(f"Decryption failed: {result.stderr}")

### ğŸ”„ **ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ìë™í™”**

```python
# automation/system_updates.py
import asyncio
import subprocess
import os
import json
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
import logging
from dataclasses import dataclass
from enum import Enum

class UpdateType(Enum):
    """ì—…ë°ì´íŠ¸ íƒ€ì…"""
    SECURITY = "security"
    SYSTEM = "system"
    APPLICATION = "application"
    DOCKER = "docker"
    STRATEGY = "strategy"

class UpdateStatus(Enum):
    """ì—…ë°ì´íŠ¸ ìƒíƒœ"""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"
    ROLLED_BACK = "rolled_back"

@dataclass
class UpdatePackage:
    """ì—…ë°ì´íŠ¸ íŒ¨í‚¤ì§€"""
    name: str
    current_version: str
    new_version: str
    update_type: UpdateType
    security_update: bool = False
    auto_install: bool = False
    requires_restart: bool = False

class SystemUpdateManager:
    """ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ê´€ë¦¬ì"""
    
    def __init__(self, notification_system, backup_manager):
        self.notification_system = notification_system
        self.backup_manager = backup_manager
        self.update_schedule = {}
        self.update_history = []
        self.maintenance_windows = []
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    async def check_available_updates(self) -> Dict[UpdateType, List[UpdatePackage]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì—…ë°ì´íŠ¸ í™•ì¸"""
        
        updates = {
            UpdateType.SECURITY: [],
            UpdateType.SYSTEM: [],
            UpdateType.APPLICATION: [],
            UpdateType.DOCKER: [],
            UpdateType.STRATEGY: []
        }
        
        # ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸ í™•ì¸
        system_updates = await self._check_system_updates()
        updates[UpdateType.SYSTEM].extend(system_updates)
        
        # ë³´ì•ˆ ì—…ë°ì´íŠ¸ í™•ì¸
        security_updates = await self._check_security_updates()
        updates[UpdateType.SECURITY].extend(security_updates)
        
        # Docker ì´ë¯¸ì§€ ì—…ë°ì´íŠ¸ í™•ì¸
        docker_updates = await self._check_docker_updates()
        updates[UpdateType.DOCKER].extend(docker_updates)
        
        # Freqtrade ì—…ë°ì´íŠ¸ í™•ì¸
        app_updates = await self._check_freqtrade_updates()
        updates[UpdateType.APPLICATION].extend(app_updates)
        
        # ì „ëµ ì—…ë°ì´íŠ¸ í™•ì¸
        strategy_updates = await self._check_strategy_updates()
        updates[UpdateType.STRATEGY].extend(strategy_updates)
        
        return updates
    
    async def _check_system_updates(self) -> List[UpdatePackage]:
        """ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸ í™•ì¸"""
        try:
            # apt list --upgradable ì‹¤í–‰
            result = subprocess.run(
                ['apt', 'list', '--upgradable'],
                capture_output=True, text=True
            )
            
            updates = []
            lines = result.stdout.strip().split('\n')[1:]  # í—¤ë” ì œì™¸
            
            for line in lines:
                if '/' in line:
                    parts = line.split()
                    if len(parts) >= 2:
                        package_name = parts[0].split('/')[0]
                        version_info = parts[1]
                        
                        # í˜„ì¬ ë²„ì „ê³¼ ìƒˆ ë²„ì „ ì¶”ì¶œ
                        if '[upgradable from:' in line:
                            current_version = line.split('[upgradable from: ')[1].split(']')[0]
                            new_version = version_info
                        else:
                            current_version = "unknown"
                            new_version = version_info
                        
                        update = UpdatePackage(
                            name=package_name,
                            current_version=current_version,
                            new_version=new_version,
                            update_type=UpdateType.SYSTEM,
                            auto_install=not self._is_critical_package(package_name)
                        )
                        
                        updates.append(update)
            
            return updates
            
        except Exception as e:
            self.logger.error(f"System update check failed: {e}")
            return []
    
    async def _check_security_updates(self) -> List[UpdatePackage]:
        """ë³´ì•ˆ ì—…ë°ì´íŠ¸ í™•ì¸"""
        try:
            # unattended-upgradesë¥¼ í†µí•œ ë³´ì•ˆ ì—…ë°ì´íŠ¸ í™•ì¸
            result = subprocess.run(
                ['unattended-upgrade', '--dry-run'],
                capture_output=True, text=True
            )
            
            updates = []
            # ê²°ê³¼ íŒŒì‹±í•˜ì—¬ ë³´ì•ˆ ì—…ë°ì´íŠ¸ ì¶”ì¶œ
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ íŒŒì‹± í•„ìš”
            
            return updates
            
        except Exception as e:
            self.logger.error(f"Security update check failed: {e}")
            return []
    
    async def _check_docker_updates(self) -> List[UpdatePackage]:
        """Docker ì´ë¯¸ì§€ ì—…ë°ì´íŠ¸ í™•ì¸"""
        try:
            updates = []
            
            # ì‚¬ìš© ì¤‘ì¸ ì´ë¯¸ì§€ ëª©ë¡
            images = [
                'freqtradeorg/freqtrade:stable',
                'postgres:14',
                'redis:alpine',
                'nginx:alpine'
            ]
            
            for image in images:
                try: